{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trajectories.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmountain_lite\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m fd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrajectories.pickle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m D \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fd)\n\u001b[0;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trajectories.pickle'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys  \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gymnasium\n",
    "import mountain_lite\n",
    "fd = open('trajectories.pickle','rb')\n",
    "D = pickle.load(fd)\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "env = gymnasium.make('mountain_lite/GridWorld-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(D[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((15, 0), 0, -0.5058824, (14, 0)),\n",
       " ((14, 0), 6, -0.5058824, (14, 0)),\n",
       " ((14, 0), 5, -0.5058824, (15, 1)),\n",
       " ((15, 1), 3, -0.5058824, (16, 0)),\n",
       " ((16, 0), 3, -0.5058824, (17, 0)),\n",
       " ((17, 0), 6, -0.5058824, (17, 0)),\n",
       " ((17, 0), 0, -0.5058824, (16, 0)),\n",
       " ((16, 0), 5, -0.5058824, (17, 0)),\n",
       " ((17, 0), 1, -0.5058824, (16, 0))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[0][0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((21, 7), 6, -0.2, (21, 6)),\n",
       " ((21, 6), 1, -0.5058824, (20, 7)),\n",
       " ((20, 7), 6, -0.41960785, (20, 6)),\n",
       " ((20, 6), 0, -0.5058824, (19, 5)),\n",
       " ((19, 5), 6, -0.5882353, (19, 4)),\n",
       " ((19, 4), 6, -0.6509804, (19, 3)),\n",
       " ((19, 3), 6, -0.627451, (19, 2)),\n",
       " ((19, 2), 5, -0.5411765, (20, 3)),\n",
       " ((20, 3), 3, -0.5058824, (21, 2))]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D[0][100:109]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Q, state, epsilon):\n",
    "    if np.random.randint() < epsilon:\n",
    "        return np.random.randint(Q.shape[2])\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "\n",
    "def update_SARSA_n(Q, state_action_pairs, rewards, alpha, gamma, n):\n",
    "    # Calculate cumulative reward with decay\n",
    "    G = 0\n",
    "    for reward in rewards:\n",
    "        G += reward*gamma\n",
    "        gamma *= gamma\n",
    "    # G = sum([gamma ** i * rewards[i] for i in range(len(rewards))])\n",
    "    # Calculate the final state and action\n",
    "    final_state, final_action = state_action_pairs[-1]\n",
    "    # Update Q value for the starting state and action\n",
    "    start_state, start_action = state_action_pairs[0]\n",
    "    Q[start_state][start_action] += alpha * (G + (gamma ** n) * Q[final_state][final_action] - Q[start_state][start_action])\n",
    "    return Q\n",
    "\n",
    "\n",
    "def SARSA_n(env, episodes, alpha, gamma, epsilon, n, random_start=True):\n",
    "    shape = (env.height, env.width, 8)  \n",
    "    Q = np.zeros(shape)\n",
    "\n",
    "    for episode in tqdm(range(episodes)):\n",
    "        epsilon *= 0.999\n",
    "        # Initialization for each episode\n",
    "        state_action_pairs = []\n",
    "        rewards = []\n",
    "        if random_start and episode < 9 * episodes // 10:\n",
    "            start_col = int(env.width-1-episode*env.width//episodes)\n",
    "            env.set_start_position((np.random.randint(env.height),start_col))\n",
    "        else:\n",
    "            env.set_start_position((15, 0))\n",
    "        # Reset environment and get initial state and action\n",
    "        observation, info = env.reset(seed=episode)\n",
    "        state = tuple(observation[\"agent\"][\"pos\"])\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "        state_action_pairs.append((state, action))\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            observation, reward, terminated, truncated, info = env.step(action)\n",
    "            next_state = tuple(observation[\"agent\"][\"pos\"])\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon)\n",
    "\n",
    "            # Store state, action, and reward\n",
    "            state_action_pairs.append((next_state, next_action))\n",
    "            rewards.append(reward)\n",
    "\n",
    "            if len(rewards) >= n or terminated or truncated:\n",
    "                Q = update_SARSA_n(Q, state_action_pairs, rewards, alpha, gamma, n)\n",
    "                # Reset the state_action_pairs and rewards for the next update\n",
    "                state_action_pairs = state_action_pairs[-1:]\n",
    "                rewards = []\n",
    "\n",
    "            state, action = next_state, next_action\n",
    "            done = terminated or truncated\n",
    "\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def epsilon_greedy_policy(Q, state, epsilon):\n",
    "    # Correct the probability check and action selection in the epsilon-greedy policy\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(Q.shape[2])\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "def sarsa(env, episodes, alpha, gamma, epsilon):\n",
    "    # Initialize Q-values based on the environment's dimensions for dynamic compatibility\n",
    "    shape = (env.height, env.width, 8)  # Assuming 8 possible actions\n",
    "    Q = np.zeros(shape)  # Adjusted to use the 'shape' variable\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]['agent']['pos']  # Start a new episode and get the initial state\n",
    "        state = tuple(state)\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "      \n",
    "        for _ in range(500):  # Limit the number of steps in each episode\n",
    "            next_state, reward, done, _, _ = env.step(action)  # Take action\n",
    "            next_state = tuple(next_state['agent']['pos'])\n",
    "            \n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon)  # Choose next action\n",
    "            \n",
    "            # SARSA(0) update rule\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])\n",
    "            \n",
    "            state, action = next_state, next_action  # Move to the next state and action\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    # Derive policy from Q-values\n",
    "    policy = np.argmax(Q, axis=2)\n",
    "    return Q, policy\n",
    "\n",
    "# Parameters (example, adjust as necessary)\n",
    "env = None  # Placeholder for your environment\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "episodes = 1000  # Number of episodes\n",
    "\n",
    "# Assuming 'env' is properly defined elsewhere\n",
    "# Q, policy = sarsa(env, episodes, alpha, gamma, epsilon)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf368",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
