{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import cookiedisaster\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "\n",
    "SEED=2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "env = gym.make('cookiedisaster/GridWorld-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'agent': {'pos': 4, 'vel': 0.0},\n",
       "  'cookie': {'pos': 0.8451834911475694, 'time': 5}},\n",
       " {'distance': 3.1548165088524307, 'steps': 0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(seed=SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "# env.render()\n",
    "DUMMY_STATE=env.step(0)\n",
    "MAX_TIME=5*25 # 5 cookies elapse time\n",
    "PUNISHMENT=-1 # may have to be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(value, min_value, max_value, scale_min=-1, scale_max=1):\n",
    "    return ((value - min_value) / (max_value - min_value)) * (scale_max - scale_min) + scale_min\n",
    "\n",
    "def preprocess_state(state):\n",
    "    # Assuming state is a dictionary like:\n",
    "    # {'robot': {'pos': x, 'vel': y}, 'cookie': {'pos': z, 'time': w}}\n",
    "    robot_pos = normalize(state[0]['agent']['pos'], 0, 10)\n",
    "    robot_vel = normalize(state[0]['agent']['vel'], -7, 7)\n",
    "    cookie_pos = normalize(state[0]['cookie']['pos'], 0, 10)\n",
    "    cookie_time = normalize(state[0]['cookie']['time'], 0, 5)\n",
    "    distance = robot_pos - cookie_pos\n",
    "    direction = 1 if distance > 0 else -1\n",
    "    \n",
    "    # Return the normalized state as a numpy array\n",
    "    return np.array([robot_pos, robot_vel, cookie_pos, cookie_time,distance, direction])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor - critic model - from chat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 6  # From preprocess_state function\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "actor = Actor(input_dim, output_dim)\n",
    "critic = Critic(input_dim)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "gamma = 0.99  # Discount factor for future rewards\n",
    "\n",
    "\n",
    "def train_actor_critic(env, num_episodes=10, gamma=0.99, actor_lr=0.001, critic_lr=0.001):\n",
    "    # Environment and model parameters\n",
    "    input_dim = 6  # From preprocess_state function\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    # Initialize models\n",
    "    actor = Actor(input_dim, output_dim)\n",
    "    critic = Critic(input_dim)\n",
    "\n",
    "    # Initialize optimizers\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    # Training loop\n",
    "    episode_rewards = []  # Track rewards per episode\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        count = 0\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = actor(state_tensor)\n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            \n",
    "            next_state= env.step(action.item())\n",
    "            reward = next_state[1]\n",
    "            done = next_state[2]\n",
    "            \n",
    "            if next_state[1]>0:\n",
    "                break\n",
    "            # missed 5 cookies (MAX_TIME), minus points and break\n",
    "            if count>MAX_TIME:\n",
    "                # env._cumulative_reward+=PUNISHMENT\n",
    "                # total_reward+=PUNISHMENT\n",
    "                break\n",
    "            count+=1\n",
    "            # if count>100:\n",
    "            #     print('count',count)\n",
    "            #     break\n",
    "            \n",
    "            next_state = preprocess_state(next_state)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            # Critic update\n",
    "            value = critic(state_tensor)\n",
    "            next_value = critic(torch.FloatTensor(next_state).unsqueeze(0))\n",
    "            td_error = reward + gamma * next_value * (1 - int(done)) - value\n",
    "            critic_loss = td_error.pow(2)\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor update\n",
    "            actor_loss = -distribution.log_prob(action) * td_error.detach()\n",
    "            \n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    return actor, critic, episode_rewards\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(env, actor, num_episodes=100):\n",
    "    total_rewards = 0.0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        count = 0\n",
    "\n",
    "        while not done:\n",
    "            if count > MAX_TIME:  # Break if max time exceeded without positive reward\n",
    "                break\n",
    "            \n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = actor(state_tensor)\n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            \n",
    "            next_state= env.step(action.item())\n",
    "            reward = next_state[1]\n",
    "            done = next_state[2]\n",
    "            \n",
    "            if next_state[1]>0:\n",
    "                break\n",
    "            \n",
    "            if count>MAX_TIME:\n",
    "                \n",
    "                break\n",
    "            count+=1\n",
    "          \n",
    "            \n",
    "            next_state = preprocess_state(next_state)\n",
    "            \n",
    "            total_reward += reward\n",
    "\n",
    "       \n",
    "\n",
    "    average_reward = total_rewards / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_policy(env, initial_actor_lr=0.001, initial_critic_lr=0.001, gamma=0.99, threshold_reward=None, max_iterations=10, eval_episodes=10):\n",
    "    actor_lr = initial_actor_lr\n",
    "    critic_lr = initial_critic_lr\n",
    "    \n",
    "    best_average_reward = float('-inf')\n",
    "    best_actor = None\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{max_iterations}: Training Started\")\n",
    "        actor, critic, _ = train_actor_critic(env, num_episodes=10, gamma=gamma, actor_lr=actor_lr, critic_lr=critic_lr)\n",
    "        average_reward = evaluate_policy(env, actor, num_episodes=eval_episodes)\n",
    "        print(f\"Iteration {iteration + 1}: Average Reward = {average_reward}\")\n",
    "        \n",
    "        # Check if the current model is the best one\n",
    "        if average_reward > best_average_reward:\n",
    "            best_average_reward = average_reward\n",
    "            best_actor = actor\n",
    "            print(\"New best model found!\")\n",
    "            \n",
    "            # Optional: Adjust learning rates based on performance, implement your strategy here\n",
    "            # actor_lr *= learning_rate_decay\n",
    "            # critic_lr *= learning_rate_decay\n",
    "            \n",
    "        # If a threshold is defined and met, stop training\n",
    "        if threshold_reward is not None and average_reward >= threshold_reward:\n",
    "            print(f\"Desired threshold reward of {threshold_reward} achieved.\")\n",
    "            break\n",
    "        \n",
    "        # Optional: Implement additional logic to adjust training parameters or terminate early\n",
    "    \n",
    "    return best_actor, best_average_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/10: Training Started\n",
      "Episode 0, Total Reward: -5.784163567670691\n",
      "Average Reward over 10 episodes: 0.0\n",
      "Iteration 1: Average Reward = 0.0\n",
      "New best model found!\n",
      "Desired threshold reward of -2.5 achieved.\n",
      "Best average reward achieved: 0.0\n"
     ]
    }
   ],
   "source": [
    "best_actor, best_reward = find_optimal_policy(env, initial_actor_lr=0.001, initial_critic_lr=0.001, threshold_reward=-2.5, max_iterations=10)\n",
    "print(f\"Best average reward achieved: {best_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_actor_critic(env, actor, episodes=100):\n",
    "    total_rewards = []\n",
    "    state = env.reset()\n",
    "\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = preprocess_state(state)  # Adjust based on your state preprocessing\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        count = 0\n",
    "        \n",
    "        \n",
    "      \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = actor(state_tensor)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        next_state= env.step(action)\n",
    "        reward = next_state[1]\n",
    "        done = next_state[2]    \n",
    "        # next_state = preprocess_state(next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gymnasium\\spaces\\box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 episodes: -0.32913523367071407\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('cookiedisaster/GridWorld-v0',render_mode='human')\n",
    "test_actor_critic(env, best_actor, episodes=100)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import cookiedisaster\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "\n",
    "SEED=2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "env = gym.make('cookiedisaster/GridWorld-v0')\n",
    "\n",
    "env.reset(seed=SEED) \n",
    "# env.render()\n",
    "DUMMY_STATE=env.step(0)\n",
    "MAX_TIME=5*25 # 5 cookies elapse time\n",
    "PUNISHMENT=-1 # may have to be adjusted\n",
    "\n",
    "def normalize(value, min_value, max_value, scale_min=-1, scale_max=1):\n",
    "    return ((value - min_value) / (max_value - min_value)) * (scale_max - scale_min) + scale_min\n",
    "\n",
    "def preprocess_state(state):\n",
    "    # Assuming state is a dictionary like:\n",
    "    # {'robot': {'pos': x, 'vel': y}, 'cookie': {'pos': z, 'time': w}}\n",
    "    robot_pos = normalize(state[0]['agent']['pos'], 0, 10)\n",
    "    robot_vel = normalize(state[0]['agent']['vel'], -7, 7)\n",
    "    cookie_pos = normalize(state[0]['cookie']['pos'], 0, 10)\n",
    "    cookie_time = normalize(state[0]['cookie']['time'], 0, 5)\n",
    "    distance = robot_pos - cookie_pos\n",
    "    direction = 1 if distance > 0 else -1\n",
    "    \n",
    "    # Return the normalized state as a numpy array\n",
    "    return np.array([robot_pos, robot_vel, cookie_pos, cookie_time,distance, direction])\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "input_dim = 6  # From preprocess_state function\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "actor = Actor(input_dim, output_dim)\n",
    "critic = Critic(input_dim)\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
    "\n",
    "num_episodes = 10\n",
    "gamma = 0.99  # Discount factor for future rewards\n",
    "\n",
    "\n",
    "def train_actor_critic(env, num_episodes=10, gamma=0.99, actor_lr=0.001, critic_lr=0.001):\n",
    "    # Environment and model parameters\n",
    "    input_dim = 6  # From preprocess_state function\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    # Initialize models\n",
    "    actor = Actor(input_dim, output_dim)\n",
    "    critic = Critic(input_dim)\n",
    "\n",
    "    # Initialize optimizers\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    # Training loop\n",
    "    episode_rewards = []  # Track rewards per episode\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        count = 0\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = actor(state_tensor)\n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            \n",
    "            next_state= env.step(action.item())\n",
    "            reward = next_state[1]\n",
    "            done = next_state[2]\n",
    "            \n",
    "            if next_state[1]>0:\n",
    "                break\n",
    "            # missed 5 cookies (MAX_TIME), minus points and break\n",
    "            if count>MAX_TIME:\n",
    "                # env._cumulative_reward+=PUNISHMENT\n",
    "                # total_reward+=PUNISHMENT\n",
    "                break\n",
    "            count+=1\n",
    "            # if count>100:\n",
    "            #     print('count',count)\n",
    "            #     break\n",
    "            \n",
    "            next_state = preprocess_state(next_state)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            # Critic update\n",
    "            value = critic(state_tensor)\n",
    "            next_value = critic(torch.FloatTensor(next_state).unsqueeze(0))\n",
    "            td_error = reward + gamma * next_value * (1 - int(done)) - value\n",
    "            critic_loss = td_error.pow(2)\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor update\n",
    "            actor_loss = -distribution.log_prob(action) * td_error.detach()\n",
    "            \n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    return actor, critic, episode_rewards\n",
    "\n",
    "def evaluate_policy(env, actor, num_episodes=100):\n",
    "    total_rewards = 0.0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        count = 0\n",
    "\n",
    "        while not done:\n",
    "            if count > MAX_TIME:  # Break if max time exceeded without positive reward\n",
    "                break\n",
    "            \n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = actor(state_tensor)\n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            \n",
    "            next_state= env.step(action.item())\n",
    "            reward = next_state[1]\n",
    "            done = next_state[2]\n",
    "            \n",
    "            if next_state[1]>0:\n",
    "                break\n",
    "            \n",
    "            if count>MAX_TIME:\n",
    "                \n",
    "                break\n",
    "            count+=1\n",
    "          \n",
    "            \n",
    "            next_state = preprocess_state(next_state)\n",
    "            \n",
    "            total_reward += reward\n",
    "\n",
    "       \n",
    "\n",
    "    average_reward = total_rewards / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward\n",
    "\n",
    "def find_optimal_policy(env, initial_actor_lr=0.001, initial_critic_lr=0.001, gamma=0.99, threshold_reward=None, max_iterations=10, eval_episodes=10):\n",
    "    actor_lr = initial_actor_lr\n",
    "    critic_lr = initial_critic_lr\n",
    "    \n",
    "    best_average_reward = float('-inf')\n",
    "    best_actor = None\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{max_iterations}: Training Started\")\n",
    "        actor, critic, _ = train_actor_critic(env, num_episodes=10, gamma=gamma, actor_lr=actor_lr, critic_lr=critic_lr)\n",
    "        average_reward = evaluate_policy(env, actor, num_episodes=eval_episodes)\n",
    "        print(f\"Iteration {iteration + 1}: Average Reward = {average_reward}\")\n",
    "        \n",
    "        # Check if the current model is the best one\n",
    "        if average_reward > best_average_reward:\n",
    "            best_average_reward = average_reward\n",
    "            best_actor = actor\n",
    "            print(\"New best model found!\")\n",
    "            \n",
    "            # Optional: Adjust learning rates based on performance, implement your strategy here\n",
    "            # actor_lr *= learning_rate_decay\n",
    "            # critic_lr *= learning_rate_decay\n",
    "            \n",
    "        # If a threshold is defined and met, stop training\n",
    "        if threshold_reward is not None and average_reward >= threshold_reward:\n",
    "            print(f\"Desired threshold reward of {threshold_reward} achieved.\")\n",
    "            break\n",
    "        \n",
    "        # Optional: Implement additional logic to adjust training parameters or terminate early\n",
    "    \n",
    "    return best_actor, best_average_reward\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
