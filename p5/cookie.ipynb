{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import cookiedisaster\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "\n",
    "SEED=2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "env = gym.make('cookiedisaster/GridWorld-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### seed vs not seed, doens't matter, still get random each time??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\spaces\\box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'agent': {'pos': 4, 'vel': 0.0},\n",
       "  'cookie': {'pos': 0.10111663773625759, 'time': 5}},\n",
       " {'distance': 3.898883362263742, 'steps': 0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(seed=SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the observation:\n",
    "- the agents pos and vel\n",
    "- the cookies pos and time remaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent': {'pos': 4, 'vel': 0.0},\n",
       " 'cookie': {'pos': 0.9194711828695544, 'time': 5}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(seed=SEED)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'agent': {'pos': 4.2, 'vel': 1.0},\n",
       "  'cookie': {'pos': 0.9194711828695544, 'time': 5}},\n",
       " 0,\n",
       " False,\n",
       " False,\n",
       " {'distance': 3.280528817130446, 'steps': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.render()\n",
    "DUMMY_STATE=env.step(0)\n",
    "MAX_TIME=5*25 # 5 cookies elapse time\n",
    "PUNISHMENT=-1 # may have to be adjusted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat lets gooo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -5.316062190712567\n",
      "Episode 100, Total Reward: 0.5\n",
      "Episode 200, Total Reward: 0.3327504983225351\n",
      "Episode 300, Total Reward: 1\n",
      "Episode 400, Total Reward: -0.9974756767772084\n",
      "Episode 500, Total Reward: 1\n",
      "Episode 600, Total Reward: -1.089508730677816\n",
      "Episode 700, Total Reward: -1.0\n",
      "Episode 800, Total Reward: 0.0\n",
      "Episode 900, Total Reward: -1.0\n",
      "Episode 1000, Total Reward: 0.5\n",
      "Episode 1100, Total Reward: 1\n",
      "Episode 1200, Total Reward: 0.5\n",
      "Episode 1300, Total Reward: 0.5\n",
      "Episode 1400, Total Reward: 1\n",
      "Episode 1500, Total Reward: 1\n",
      "Episode 1600, Total Reward: -2.5\n",
      "Episode 1700, Total Reward: -2.5\n",
      "Episode 1800, Total Reward: 0.5\n",
      "Episode 1900, Total Reward: 1\n",
      "Episode 2000, Total Reward: 1\n",
      "Episode 2100, Total Reward: 1\n",
      "Episode 2200, Total Reward: 0.5\n",
      "Episode 2300, Total Reward: 1\n",
      "Episode 2400, Total Reward: 1\n",
      "Episode 2500, Total Reward: 1\n",
      "Episode 2600, Total Reward: -2.0773491004469835\n",
      "Episode 2700, Total Reward: 1\n",
      "Episode 2800, Total Reward: 0.5\n",
      "Episode 2900, Total Reward: -1.8222967886927366\n",
      "Episode 3000, Total Reward: -6.469979706617418\n",
      "Episode 3100, Total Reward: 0.5\n",
      "Episode 3200, Total Reward: -0.75814831744518\n",
      "Episode 3300, Total Reward: 1\n",
      "Episode 3400, Total Reward: 0.0\n",
      "Episode 3500, Total Reward: 1\n",
      "Episode 3600, Total Reward: 1\n",
      "Episode 3700, Total Reward: -0.27876947068458713\n",
      "Episode 3800, Total Reward: 0.5\n",
      "Episode 3900, Total Reward: 1\n",
      "Episode 4000, Total Reward: -3.239986189233315\n",
      "Episode 4100, Total Reward: 1\n",
      "Episode 4200, Total Reward: 1\n",
      "Episode 4300, Total Reward: 1\n",
      "Episode 4400, Total Reward: -1.9312161933075536\n",
      "Episode 4500, Total Reward: 1\n",
      "Episode 4600, Total Reward: 1\n",
      "Episode 4700, Total Reward: -4.120578807010958\n",
      "Episode 4800, Total Reward: -7.902469257667038\n",
      "Episode 4900, Total Reward: 0.5\n",
      "Episode 5000, Total Reward: 1\n",
      "Episode 5100, Total Reward: -4.007443637619162\n",
      "Episode 5200, Total Reward: -3.218680133534614\n",
      "Episode 5300, Total Reward: 0.30534845632927377\n",
      "Episode 5400, Total Reward: 0.5\n",
      "Episode 5500, Total Reward: 0.5\n",
      "Episode 5600, Total Reward: -2.563708695835779\n",
      "Episode 5700, Total Reward: 1\n",
      "Episode 5800, Total Reward: 1\n",
      "Episode 5900, Total Reward: 1\n",
      "Episode 6000, Total Reward: -2.032120789684789\n",
      "Episode 6100, Total Reward: 1\n",
      "Episode 6200, Total Reward: 1\n",
      "Episode 6300, Total Reward: 1\n",
      "Episode 6400, Total Reward: 1\n",
      "Episode 6500, Total Reward: -3.44870435156377\n",
      "Episode 6600, Total Reward: 1\n",
      "Episode 6700, Total Reward: 1\n",
      "Episode 6800, Total Reward: -3.983255020703451\n",
      "Episode 6900, Total Reward: -2.5\n",
      "Episode 7000, Total Reward: 0.5\n",
      "Episode 7100, Total Reward: 0.5\n",
      "Episode 7200, Total Reward: 1\n",
      "Episode 7300, Total Reward: 0.44389085212075485\n",
      "Episode 7400, Total Reward: 0.7467867805800044\n",
      "Episode 7500, Total Reward: 1\n",
      "Episode 7600, Total Reward: 0.5\n",
      "Episode 7700, Total Reward: 1\n",
      "Episode 7800, Total Reward: 0.27827769897386334\n",
      "Episode 7900, Total Reward: -5.8170549532320415\n",
      "Episode 8000, Total Reward: -0.4197880895415369\n",
      "Episode 8100, Total Reward: 1\n",
      "Episode 8200, Total Reward: 0.0\n",
      "Episode 8300, Total Reward: 1\n",
      "Episode 8400, Total Reward: 0.2871675792295516\n",
      "Episode 8500, Total Reward: 1\n",
      "Episode 8600, Total Reward: 0.0\n",
      "Episode 8700, Total Reward: -2.7741121867864917\n",
      "Episode 8800, Total Reward: 0.0\n",
      "Episode 8900, Total Reward: 0.5\n",
      "Episode 9000, Total Reward: 1\n",
      "Episode 9100, Total Reward: -1.7053313287329472\n",
      "Episode 9200, Total Reward: 0.5\n",
      "Episode 9300, Total Reward: 1\n",
      "Episode 9400, Total Reward: -0.5\n",
      "Episode 9500, Total Reward: 0.5\n",
      "Episode 9600, Total Reward: 0.3845922684585379\n",
      "Episode 9700, Total Reward: 1\n",
      "Episode 9800, Total Reward: -1.2285105447259688\n",
      "Episode 9900, Total Reward: 1\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "def normalize(value, min_value, max_value, scale_min=-1, scale_max=1):\n",
    "    return ((value - min_value) / (max_value - min_value)) * (scale_max - scale_min) + scale_min\n",
    "\n",
    "def preprocess_state(state):\n",
    "    # Assuming state is a dictionary like:\n",
    "    # {'robot': {'pos': x, 'vel': y}, 'cookie': {'pos': z, 'time': w}}\n",
    "    robot_pos = normalize(state[0]['agent']['pos'], 0, 10)\n",
    "    robot_vel = normalize(state[0]['agent']['vel'], -7, 7)\n",
    "    cookie_pos = normalize(state[0]['cookie']['pos'], 0, 10)\n",
    "    cookie_time = normalize(state[0]['cookie']['time'], 0, 5)\n",
    "    distance = robot_pos - cookie_pos\n",
    "    direction = 1 if distance > 0 else -1\n",
    "    \n",
    "    # Return the normalized state as a numpy array\n",
    "    return np.array([robot_pos, robot_vel, cookie_pos, cookie_time,distance, direction])\n",
    "\n",
    "\n",
    "# def preprocess_state(state):\n",
    "#     # Assuming state is a dictionary with the structure you've shown\n",
    "#     agent_pos = state[0]['agent']['pos']\n",
    "#     agent_vel = state[0]['agent']['vel']\n",
    "#     cookie_pos = state[0]['cookie']['pos']\n",
    "#     cookie_time = state[0]['cookie']['time']\n",
    "#     distance = state[-1]['distance']\n",
    "#     steps = state[-1]['steps']\n",
    "    \n",
    "#     # Combine these into a flat array of features\n",
    "#     processed_state = np.array([agent_pos, agent_vel, cookie_pos, cookie_time, distance, steps])\n",
    "\n",
    "#     return processed_state\n",
    "\n",
    "def choose_action(state, weights, epsilon, num_actions):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(num_actions)\n",
    "    else:\n",
    "        # print('state',state)\n",
    "        # print('weights',weights)\n",
    "        q_values = np.dot(state, weights)\n",
    "        # print('q_values',q_values)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "def update_weights(state, action, reward, next_state, weights, learning_rate, discount_factor):\n",
    "    next_action = choose_action(next_state, weights, 0, len(weights[0]))\n",
    "    td_target = reward + discount_factor * np.dot(next_state, weights[:, next_action])\n",
    "    td_error = td_target - np.dot(state, weights[:, action])\n",
    "    weights[:, action] += learning_rate * td_error * state\n",
    "    return weights\n",
    "\n",
    "\n",
    "def train(env, num_episodes=10000, learning_rate=0.1, discount_factor=0.999, epsilon=0.3):\n",
    "    num_actions = env.action_space.n\n",
    "    num_features = env.observation_space.shape[0]  # Assuming Box space\n",
    "    num_features = len(preprocess_state(DUMMY_STATE))  # Use an example or dummy initial state\n",
    "    weights = np.zeros((num_features, num_actions))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        count=0\n",
    "        while not done:\n",
    "            processed_state = preprocess_state(state)  # Process state\n",
    "            action = choose_action(processed_state, weights, epsilon, num_actions)\n",
    "            next_state= env.step(action)\n",
    "            total_reward += next_state[1]\n",
    "\n",
    "            processed_next_state = preprocess_state(next_state)  # Process next state\n",
    "            weights = update_weights(processed_state, action, next_state[1], processed_next_state, weights, learning_rate, discount_factor)\n",
    "\n",
    "            state = next_state\n",
    "            # found cookie break\n",
    "            if next_state[1]>0:\n",
    "                break\n",
    "            # missed 5 cookies (MAX_TIME), minus points and break\n",
    "            if count>MAX_TIME:\n",
    "                # env._cumulative_reward+=PUNISHMENT\n",
    "                # total_reward+=PUNISHMENT\n",
    "                break\n",
    "            count+=1\n",
    "            # if count>100:\n",
    "            #     print('count',count)\n",
    "            #     break\n",
    "            \n",
    "            if done:\n",
    "                print(f\"DONE! Episode {episode}, count {count}, Total Reward: {total_reward}\")\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "    print(\"Training finished.\")\n",
    "    return weights\n",
    "\n",
    "def test(env, weights, num_episodes=100):\n",
    "    state = env.reset()\n",
    "\n",
    "    total_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        # while not done:\n",
    "        processed_state = preprocess_state(state)  # Process state\n",
    "        action = choose_action(processed_state, weights, 0, len(weights[0]))\n",
    "        state = env.step(action)\n",
    "        total_reward += state[1]\n",
    "            # if state[1]>0:\n",
    "            #     # print(f'Episoned {episode}, count {count}, reward {next_state[1]}')\n",
    "            #     break\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average reward over {num_episodes} episodes: {avg_reward}\")\n",
    "\n",
    "env = gym.make('cookiedisaster/GridWorld-v0')\n",
    "\n",
    "# Train the agent\n",
    "trained_weights = train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 100 episodes: -0.06888766166276471\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make('cookiedisaster/GridWorld-v0',render_mode='human')\n",
    "\n",
    "# Test the agent\n",
    "test(env, trained_weights)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15253852, -0.24189452, -0.23474692],\n",
       "       [ 0.19079199, -0.3134597 , -0.70424752],\n",
       "       [-0.16657725, -0.0258768 , -0.20355156],\n",
       "       [ 0.43376501,  0.28852999,  0.19494037],\n",
       "       [ 0.01403873, -0.21601772, -0.03119535],\n",
       "       [ 0.41139576,  0.36041718, -0.20901164]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat Neural network !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden_size=64):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('cookiedisaster/GridWorld-v0')\n",
    "state_size = len(preprocess_state(DUMMY_STATE))  # Adjust this based on your preprocessing\n",
    "action_size = env.action_space.n\n",
    "q_network = QNetwork(state_size, action_size)\n",
    "optimizer = optim.Adam(q_network.parameters(), lr=0.1)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_q_network(env, q_network, episodes=1000, gamma=0.999):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state)  # Make sure to implement this\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        count=0\n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = q_network(state_tensor)\n",
    "            action = torch.argmax(q_values).item()\n",
    "            \n",
    "            next_state = env.step(action)\n",
    "            done=next_state[2]\n",
    "            reward = next_state[1]\n",
    "            \n",
    "            # found cookie break\n",
    "            if next_state[1]>0:\n",
    "                break\n",
    "            # missed 5 cookies (MAX_TIME), minus points and break\n",
    "            if count>MAX_TIME:\n",
    "                # env._cumulative_reward+=\n",
    "                break\n",
    "            count+=1\n",
    "\n",
    "            next_state = preprocess_state(next_state)\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "            if not done:\n",
    "                max_next_q = torch.max(q_network(next_state_tensor))\n",
    "                target_q = reward + gamma * max_next_q\n",
    "            else:\n",
    "                target_q = torch.tensor([reward])\n",
    "\n",
    "            target_q_values = q_values.clone()\n",
    "            target_q_values[0][action] = target_q\n",
    "\n",
    "            loss = loss_fn(q_values, target_q_values)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        if episode % 100 == 0:\n",
    "            print(f'Episode: {episode}, Total Reward: {total_reward}')\n",
    "    return q_network\n",
    "\n",
    "\n",
    "def test_q_network(env, q_network, episodes=100):\n",
    "    state = env.reset()\n",
    "    total_rewards = []\n",
    "    for _ in range(episodes):\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        \n",
    "        state = preprocess_state(state)\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = q_network(state_tensor)\n",
    "        action = torch.argmax(q_values).item()\n",
    "\n",
    "        next_state= env.step(action)\n",
    "        total_reward += next_state[1]\n",
    "        state = next_state\n",
    "\n",
    "        total_rewards.append(total_reward)\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    return avg_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\spaces\\box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Total Reward: 0\n",
      "Episode: 100, Total Reward: -18.933618826467114\n",
      "Episode: 200, Total Reward: -2.5\n",
      "Episode: 300, Total Reward: -2.5\n",
      "Episode: 400, Total Reward: -17.51606219071256\n",
      "Episode: 500, Total Reward: -18.933618826467114\n",
      "Episode: 600, Total Reward: -2.5\n",
      "Episode: 700, Total Reward: -2.5\n",
      "Episode: 800, Total Reward: -2.5\n",
      "Episode: 900, Total Reward: -2.5\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('cookiedisaster/GridWorld-v0')\n",
    "trained_network=train_q_network(env, q_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('cookiedisaster/GridWorld-v0',render_mode='human')\n",
    "test_q_network(env, trained_network)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf368",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
