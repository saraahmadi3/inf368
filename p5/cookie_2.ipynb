{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import cookiedisaster\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "\n",
    "SEED=2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "env = gym.make('cookiedisaster/GridWorld-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(state, action):\n",
    "    \"\"\"\n",
    "    Feature engineering function to convert state and action into a feature vector.\n",
    "\n",
    "    Parameters:\n",
    "    - state: Raw state information from the environment.\n",
    "    - action: Chosen action by the agent.\n",
    "\n",
    "    Returns:\n",
    "    - features: Feature vector representing the state-action pair.\n",
    "    \"\"\"\n",
    "    # Extract relevant information from the state\n",
    "    agent_pos = state['agent']['pos']\n",
    "    agent_vel = state['agent']['vel']\n",
    "    cookie_pos = state['cookie']['pos']\n",
    "    cookie_time = state['cookie']['time']\n",
    "\n",
    "    # Initialize feature vector\n",
    "    features = []\n",
    "\n",
    "    # Relative position of the agent and the cookie\n",
    "    relative_pos = cookie_pos - agent_pos\n",
    "    features.append(relative_pos)\n",
    "\n",
    "    # Direction of the cookie\n",
    "    direction = 1 if relative_pos > 0 else -1\n",
    "    features.append(direction)\n",
    "\n",
    "    # Agent velocity\n",
    "    features.append(agent_vel)\n",
    "\n",
    "    # Time remaining for the cookie to disappear\n",
    "    features.append(cookie_time)\n",
    "\n",
    "    # Cookie position\n",
    "    features.append(cookie_pos)\n",
    "\n",
    "    # Agent position\n",
    "    features.append(agent_pos)\n",
    "\n",
    "    # # Feature 4: Squared terms for position difference and velocity\n",
    "    # features.append(relative_pos ** 2)\n",
    "    # features.append(agent_vel ** 2)\n",
    "\n",
    "    # # Feature 5: Interaction term between velocity and relative position\n",
    "    # features.append(agent_vel * relative_pos)\n",
    "\n",
    "    # Optionally, include features related to the action\n",
    "    # These could be one-hot encoded or numerical values representing the chosen action\n",
    "    features.append(action)\n",
    "\n",
    "    # proximity to the wall\n",
    "    features.append(10-agent_pos)\n",
    "\n",
    "    # Convert the feature vector to numpy array for compatibility with agent's update function\n",
    "    features=np.array(features)\n",
    "    # print(\"features\",features)\n",
    "    # Normalize features to have mean=0 and variance=1\n",
    "    features = (features - np.mean(features)) / np.std(features)\n",
    "    # print(\"features\",features)\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearQAgent:\n",
    "    def __init__(self, num_features, learning_rate=0.05, gamma=0.99,l2_reg=0.01):\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.learning_rate = learning_rate # Step size\n",
    "        self.gamma = gamma # Discount factor\n",
    "        self.l2_reg = l2_reg # L2 regularization strength\n",
    "\n",
    "    def predict_q(self, features):\n",
    "        return np.dot(self.weights, features)\n",
    "\n",
    "    def update(self, features, reward, next_features, done):\n",
    "        q_current = self.predict_q(features)\n",
    "        q_next_max = 0 if done else np.max([self.predict_q(next_features) for a in range(3)])  # Assuming 3 actions\n",
    "        target = reward + self.gamma * q_next_max\n",
    "        error = target - q_current\n",
    "        self.weights += self.learning_rate * (error * features- self.l2_reg * self.weights)\n",
    "        # print(self.weights)\n",
    "\n",
    "    def choose_action(self, state, epsilon=0.1):\n",
    "        if np.random.rand() < epsilon:  # Exploration\n",
    "            return np.random.choice([0, 1, 2])  # Assuming actions are 0, 1, 2\n",
    "        else:  # Exploitation\n",
    "            q_values = [self.predict_q(feature_engineering(state, a)) for a in range(3)]\n",
    "            return np.argmax(q_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, feature_engineering, number_of_episodes=1000, episode_length=1000, epsilon_start=1.0, epsilon_decay=0.995, epsilon_min=0.1):\n",
    "    \"\"\"\n",
    "    Trains the agent in the given environment.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment instance.\n",
    "    - agent: The agent instance.\n",
    "    - feature_engineering: Function to convert state and action into features.\n",
    "    - number_of_episodes: Total number of episodes for training.\n",
    "    - episode_length: The maximum length of an episode.\n",
    "    - epsilon_start: Starting value of epsilon for Îµ-greedy policy.\n",
    "    - epsilon_decay: The decay rate of epsilon after each episode.\n",
    "    - epsilon_min: The minimum value of epsilon.\n",
    "    \"\"\"\n",
    "    epsilon = epsilon_start\n",
    "    state = env.reset()[0]\n",
    "    for episode in range(number_of_episodes):\n",
    "        \n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(episode_length):\n",
    "            action = agent.choose_action(state, epsilon)\n",
    "            next_state, reward, _, _, _ = env.step(action)\n",
    "            \n",
    "            # Feature Engineering for current and next state\n",
    "            features = feature_engineering(state, action)\n",
    "            next_features = feature_engineering(next_state, action)  # For simplicity\n",
    "            \n",
    "            # Update the agent\n",
    "            agent.update(features, reward, next_features, False)  # Force 'done' to False since the episode doesn't naturally end\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "        # Logging\n",
    "        if (episode + 1) % 10 == 0:  # Log every 100 episodes\n",
    "            print(f\"Episode {episode + 1}: Total Reward = {total_reward}, Epsilon = {epsilon}\")\n",
    "    # return total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, agent, feature_engineering, num_episodes=3, max_steps_per_episode=200):\n",
    "    \"\"\"\n",
    "    Test the agent in the given environment with a maximum number of steps and render its performance.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The environment instance.\n",
    "    - agent: The trained agent instance.\n",
    "    - feature_engineering: Function to convert state and action into features.\n",
    "    - num_episodes: Number of episodes to run the agent.\n",
    "    - max_steps_per_episode: Maximum number of steps to execute per episode.\n",
    "    \"\"\"\n",
    "    total_rewards = []  # To keep track of the rewards for each episode\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()[0]\n",
    "        episode_rewards = 0\n",
    "\n",
    "        for _ in range(max_steps_per_episode):\n",
    "            action = agent.choose_action(state, epsilon=0)  # Use epsilon=0 for no exploration\n",
    "            next_state, reward, _, _, _ = env.step(action)\n",
    "            state = next_state  # Update to the new state\n",
    "\n",
    "            episode_rewards += reward\n",
    "\n",
    "            # env.render()  # Render the current state of the environment\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Total Reward = {episode_rewards}\")\n",
    "        total_rewards.append(episode_rewards)\n",
    "\n",
    "    env.close()  # Close the environment window if it's open\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {np.mean(total_rewards)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\spaces\\box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10: Total Reward = -26.23848695251783, Epsilon = 0.9704017769489168\n",
      "Episode 20: Total Reward = -33.842660954903955, Epsilon = 0.9416796087056153\n",
      "Episode 30: Total Reward = -38.79817381682741, Epsilon = 0.9138075656044898\n",
      "Episode 40: Total Reward = -37.11587655474579, Epsilon = 0.8867604854519608\n",
      "Episode 50: Total Reward = -37.915960976613626, Epsilon = 0.860513950810667\n",
      "Episode 60: Total Reward = -60.365976470913985, Epsilon = 0.835044266956004\n",
      "Episode 70: Total Reward = -68.57606120033631, Epsilon = 0.8103284404851119\n",
      "Episode 80: Total Reward = -51.086459031910486, Epsilon = 0.7863441585589971\n",
      "Episode 90: Total Reward = -52.15513715647097, Epsilon = 0.7630697687590515\n",
      "Episode 100: Total Reward = -57.61295101170633, Epsilon = 0.7404842595397826\n",
      "Episode 110: Total Reward = -64.45690607747026, Epsilon = 0.7185672412601078\n",
      "Episode 120: Total Reward = -63.55428624762215, Epsilon = 0.6972989277760897\n",
      "Episode 130: Total Reward = -55.5513931205547, Epsilon = 0.676660118578492\n",
      "Episode 140: Total Reward = -65.60284343910558, Epsilon = 0.6566321814590335\n",
      "Episode 150: Total Reward = -65.28758427237486, Epsilon = 0.6371970356896897\n",
      "Episode 160: Total Reward = -75.0456289269048, Epsilon = 0.6183371356998573\n",
      "Episode 170: Total Reward = -75.06436313013752, Epsilon = 0.6000354552366449\n",
      "Episode 180: Total Reward = -50.0189088354509, Epsilon = 0.5822754719939924\n",
      "Episode 190: Total Reward = -80.56388854597739, Epsilon = 0.5650411526967395\n",
      "Episode 200: Total Reward = -79.5839852159934, Epsilon = 0.5483169386261801\n",
      "Episode 210: Total Reward = -80.93218542099744, Epsilon = 0.5320877315740353\n",
      "Episode 220: Total Reward = -91.32033096128879, Epsilon = 0.5163388802121622\n",
      "Episode 230: Total Reward = -82.30209132316418, Epsilon = 0.5010561668656963\n",
      "Episode 240: Total Reward = -77.70986484960102, Epsilon = 0.48622579467768473\n",
      "Episode 250: Total Reward = -80.17198047611367, Epsilon = 0.4718343751536244\n",
      "Episode 260: Total Reward = -85.12222347074069, Epsilon = 0.457868916074659\n",
      "Episode 270: Total Reward = -87.33778234913427, Epsilon = 0.44431680976852356\n",
      "Episode 280: Total Reward = -78.47293633643527, Epsilon = 0.43116582172764906\n",
      "Episode 290: Total Reward = -97.11392750927656, Epsilon = 0.4184040795641506\n",
      "Episode 300: Total Reward = -92.90758919548942, Epsilon = 0.4060200622917276\n",
      "Episode 310: Total Reward = -92.30392462723432, Epsilon = 0.3940025899248023\n",
      "Episode 320: Total Reward = -92.3935357102273, Epsilon = 0.3823408133855036\n",
      "Episode 330: Total Reward = -89.51721853612005, Epsilon = 0.3710242047093869\n",
      "Episode 340: Total Reward = -91.56641014679178, Epsilon = 0.3600425475410476\n",
      "Episode 350: Total Reward = -85.11894758622707, Epsilon = 0.3493859279110474\n",
      "Episode 360: Total Reward = -87.69512049736855, Epsilon = 0.3390447252858265\n",
      "Episode 370: Total Reward = -98.47070196944665, Epsilon = 0.32900960388252337\n",
      "Episode 380: Total Reward = -91.58270482103305, Epsilon = 0.31927150424085987\n",
      "Episode 390: Total Reward = -99.01911888493599, Epsilon = 0.30982163504448407\n",
      "Episode 400: Total Reward = -97.21676788176342, Epsilon = 0.30065146518438607\n",
      "Episode 410: Total Reward = -92.62968367524438, Epsilon = 0.2917527160572235\n",
      "Episode 420: Total Reward = -73.00683779009987, Epsilon = 0.2831173540916025\n",
      "Episode 430: Total Reward = -97.1726457829476, Epsilon = 0.27473758349556665\n",
      "Episode 440: Total Reward = -96.2963166927976, Epsilon = 0.2666058392187492\n",
      "Episode 450: Total Reward = -95.44242885260799, Epsilon = 0.25871478012283144\n",
      "Episode 460: Total Reward = -87.2770176057092, Epsilon = 0.251057282354144\n",
      "Episode 470: Total Reward = -101.59337459246926, Epsilon = 0.24362643291242728\n",
      "Episode 480: Total Reward = -90.98243707171807, Epsilon = 0.23641552340994548\n",
      "Episode 490: Total Reward = -98.6342020805735, Epsilon = 0.22941804401531934\n",
      "Episode 500: Total Reward = -100.08887100687546, Epsilon = 0.2226276775766107\n",
      "[ 1.95111962  3.2679856   1.19104769  0.27183511 -3.26608458 -0.55581416\n",
      "  0.4036742  -3.26376348]\n"
     ]
    }
   ],
   "source": [
    "agent=LinearQAgent(num_features=8, learning_rate=0.01,l2_reg=0.001)\n",
    "env = gym.make('cookiedisaster/GridWorld-v0')\n",
    "train_agent(env, agent, feature_engineering, number_of_episodes=500, episode_length=1000, epsilon_start=1.0, epsilon_decay=0.997, epsilon_min=0.15)\n",
    "print(agent.weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = -29.371980511599027\n",
      "Average Reward over 1 episodes: -29.371980511599027\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('cookiedisaster/GridWorld-v0',render_mode='human')\n",
    "test_agent(env, agent, feature_engineering, num_episodes=1, max_steps_per_episode=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=1.0\n",
    "# for i in range(1000):\n",
    "#     a*=0.997\n",
    "#     print('i',i,'a',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf368",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
