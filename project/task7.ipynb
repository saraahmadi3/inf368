{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from torch.distributions import Normal\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "\n",
    "import pickle\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward = 4.0\n",
      "Episode 2, Total Reward = 13.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 13.0\n",
      "Episode 5, Total Reward = 10.0\n",
      "Episode 6, Total Reward = 8.0\n",
      "Episode 7, Total Reward = 4.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 20.0\n",
      "Episode 12, Total Reward = 11.0\n",
      "Episode 13, Total Reward = 13.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 5.0\n",
      "Episode 16, Total Reward = 10.0\n",
      "Episode 17, Total Reward = 8.0\n",
      "Episode 18, Total Reward = 7.0\n",
      "Episode 19, Total Reward = 16.0\n",
      "Episode 20, Total Reward = 5.0\n",
      "Episode 21, Total Reward = 5.0\n",
      "Episode 22, Total Reward = 7.0\n",
      "Episode 23, Total Reward = 12.0\n",
      "Episode 24, Total Reward = 10.0\n",
      "Episode 25, Total Reward = 9.0\n",
      "Episode 26, Total Reward = 10.0\n",
      "Episode 27, Total Reward = 5.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 19.0\n",
      "Episode 30, Total Reward = 4.0\n",
      "Episode 31, Total Reward = 5.0\n",
      "Episode 32, Total Reward = 18.0\n",
      "Episode 33, Total Reward = 25.0\n",
      "Episode 34, Total Reward = 7.0\n",
      "Episode 35, Total Reward = 5.0\n",
      "Episode 36, Total Reward = 8.0\n",
      "Episode 37, Total Reward = 7.0\n",
      "Episode 38, Total Reward = 3.0\n",
      "Episode 39, Total Reward = 4.0\n",
      "Episode 40, Total Reward = 13.0\n",
      "Episode 41, Total Reward = 8.0\n",
      "Episode 42, Total Reward = 19.0\n",
      "Episode 43, Total Reward = 28.0\n",
      "Episode 44, Total Reward = 7.0\n",
      "Episode 45, Total Reward = 5.0\n",
      "Episode 46, Total Reward = 6.0\n",
      "Episode 47, Total Reward = 7.0\n",
      "Episode 48, Total Reward = 24.0\n",
      "Episode 49, Total Reward = 18.0\n",
      "Episode 50, Total Reward = 15.0\n",
      "Episode 51, Total Reward = 8.0\n",
      "Episode 52, Total Reward = 8.0\n",
      "Episode 53, Total Reward = 7.0\n",
      "Episode 54, Total Reward = 10.0\n",
      "Episode 55, Total Reward = 15.0\n",
      "Episode 56, Total Reward = 20.0\n",
      "Episode 57, Total Reward = 27.0\n",
      "Episode 58, Total Reward = 29.0\n",
      "Episode 59, Total Reward = 21.0\n",
      "Episode 60, Total Reward = 47.0\n",
      "Episode 61, Total Reward = 27.0\n",
      "Episode 62, Total Reward = 18.0\n",
      "Episode 63, Total Reward = 28.0\n",
      "Episode 64, Total Reward = 70.0\n",
      "Episode 65, Total Reward = 22.0\n",
      "Episode 66, Total Reward = 6.0\n",
      "Episode 67, Total Reward = 17.0\n",
      "Episode 68, Total Reward = 47.0\n",
      "Episode 69, Total Reward = 34.0\n",
      "Episode 70, Total Reward = 19.0\n",
      "Episode 71, Total Reward = 8.0\n",
      "Episode 72, Total Reward = 10.0\n",
      "Episode 73, Total Reward = 18.0\n",
      "Episode 74, Total Reward = 29.0\n",
      "Episode 75, Total Reward = 53.0\n",
      "Episode 76, Total Reward = 99.0\n",
      "Episode 77, Total Reward = 24.0\n",
      "Episode 78, Total Reward = 51.0\n",
      "Episode 79, Total Reward = 42.0\n",
      "Episode 80, Total Reward = 56.0\n",
      "Episode 81, Total Reward = 51.0\n",
      "Episode 82, Total Reward = 109.0\n",
      "Episode 83, Total Reward = 46.0\n",
      "Episode 84, Total Reward = 114.0\n",
      "Episode 85, Total Reward = 40.0\n",
      "Episode 86, Total Reward = 29.0\n",
      "Episode 87, Total Reward = 33.0\n",
      "Episode 88, Total Reward = 45.0\n",
      "Episode 89, Total Reward = 48.0\n",
      "Episode 90, Total Reward = 48.0\n",
      "Episode 91, Total Reward = 82.0\n",
      "Episode 92, Total Reward = 138.0\n",
      "Episode 93, Total Reward = 35.0\n",
      "Episode 94, Total Reward = 38.0\n",
      "Episode 95, Total Reward = 68.0\n",
      "Episode 96, Total Reward = 62.0\n",
      "Episode 97, Total Reward = 8.0\n",
      "Episode 98, Total Reward = 29.0\n",
      "Episode 99, Total Reward = 90.0\n",
      "Episode 100, Total Reward = 68.0\n",
      "Episode 101, Total Reward = 22.0\n",
      "Episode 102, Total Reward = 40.0\n",
      "Episode 103, Total Reward = 18.0\n",
      "Episode 104, Total Reward = 59.0\n",
      "Episode 105, Total Reward = 50.0\n",
      "Episode 106, Total Reward = 85.0\n",
      "Episode 107, Total Reward = 78.0\n",
      "Episode 108, Total Reward = 71.0\n",
      "Episode 109, Total Reward = 121.0\n",
      "Episode 110, Total Reward = 70.0\n",
      "Episode 111, Total Reward = 65.0\n",
      "Episode 112, Total Reward = 31.0\n",
      "Episode 113, Total Reward = 74.0\n",
      "Episode 114, Total Reward = 146.0\n",
      "Episode 115, Total Reward = 109.0\n",
      "Episode 116, Total Reward = 209.0\n",
      "Episode 117, Total Reward = 144.0\n",
      "Episode 118, Total Reward = 314.0\n",
      "Episode 119, Total Reward = 72.0\n",
      "Episode 120, Total Reward = 279.0\n",
      "Episode 121, Total Reward = 50.0\n",
      "Episode 122, Total Reward = 100.0\n",
      "Episode 123, Total Reward = 126.0\n",
      "Episode 124, Total Reward = 50.0\n",
      "Episode 125, Total Reward = 94.0\n",
      "Episode 126, Total Reward = 66.0\n",
      "Episode 127, Total Reward = 104.0\n",
      "Episode 128, Total Reward = 121.0\n",
      "Episode 129, Total Reward = 69.0\n",
      "Episode 130, Total Reward = 123.0\n",
      "Episode 131, Total Reward = 170.0\n",
      "Episode 132, Total Reward = 280.0\n",
      "Episode 133, Total Reward = 71.0\n",
      "Episode 134, Total Reward = 24.0\n",
      "Episode 135, Total Reward = 75.0\n",
      "Episode 136, Total Reward = 102.0\n",
      "Episode 137, Total Reward = 487.0\n",
      "Episode 138, Total Reward = 237.0\n",
      "Episode 139, Total Reward = 129.0\n",
      "Episode 140, Total Reward = 107.0\n",
      "Episode 141, Total Reward = 40.0\n",
      "Episode 142, Total Reward = 108.0\n",
      "Episode 143, Total Reward = 59.0\n",
      "Episode 144, Total Reward = 67.0\n",
      "Episode 145, Total Reward = 49.0\n",
      "Episode 146, Total Reward = 51.0\n",
      "Episode 147, Total Reward = 61.0\n",
      "Episode 148, Total Reward = 129.0\n",
      "Episode 149, Total Reward = 93.0\n",
      "Episode 150, Total Reward = 191.0\n",
      "Episode 151, Total Reward = 173.0\n",
      "Episode 152, Total Reward = 133.0\n",
      "Episode 153, Total Reward = 762.0\n",
      "Episode 154, Total Reward = 47.0\n",
      "Episode 155, Total Reward = 215.0\n",
      "Episode 156, Total Reward = 27.0\n",
      "Episode 157, Total Reward = 20.0\n",
      "Episode 158, Total Reward = 142.0\n",
      "Episode 159, Total Reward = 398.0\n",
      "Episode 160, Total Reward = 295.0\n",
      "Episode 161, Total Reward = 104.0\n",
      "Episode 162, Total Reward = 71.0\n",
      "Episode 163, Total Reward = 167.0\n",
      "Episode 164, Total Reward = 1000.0\n",
      "Episode 165, Total Reward = 511.0\n",
      "Episode 166, Total Reward = 228.0\n",
      "Episode 167, Total Reward = 1000.0\n",
      "Episode 168, Total Reward = 474.0\n",
      "Episode 169, Total Reward = 185.0\n",
      "Episode 170, Total Reward = 122.0\n",
      "Episode 171, Total Reward = 103.0\n",
      "Episode 172, Total Reward = 221.0\n",
      "Episode 173, Total Reward = 17.0\n",
      "Episode 174, Total Reward = 311.0\n",
      "Episode 175, Total Reward = 67.0\n",
      "Episode 176, Total Reward = 958.0\n",
      "Episode 177, Total Reward = 1000.0\n",
      "Episode 178, Total Reward = 1000.0\n",
      "Episode 179, Total Reward = 1000.0\n",
      "Episode 180, Total Reward = 1000.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 171\u001b[0m\n\u001b[0;32m    169\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPOAgent(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    170\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvertedPendulum-v4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 171\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhorizon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_experiments\u001b[39m(env, param_grid):\n\u001b[0;32m    174\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[10], line 106\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[1;34m(self, env, total_episodes, horizon, batch_size)\u001b[0m\n\u001b[0;32m    103\u001b[0m episode_rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m--> 106\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_critic(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat())[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    108\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[1;32mIn[10], line 40\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m     39\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m---> 40\u001b[0m     mu, std, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     distribution \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mNormal(mu, std)\n\u001b[0;32m     42\u001b[0m     action \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 22\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     21\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(x)\n\u001b[1;32m---> 22\u001b[0m     std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_std\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mexpand_as(mu)\n\u001b[0;32m     24\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic(x)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mu, std, value\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        \n",
    "        value = self.critic(x)\n",
    "        return mu, std, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, k_epochs=10, minibatch_size=64, gae_lambda=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.actor_critic = ActorCritic(input_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        mu, std, _ = self.actor_critic(state)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            indices = np.arange(states.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, states.shape[0], self.minibatch_size):\n",
    "                end = start + self.minibatch_size\n",
    "                minibatch_indices = indices[start:end]\n",
    "                \n",
    "                minibatch_states = states[minibatch_indices]\n",
    "                minibatch_actions = actions[minibatch_indices]\n",
    "                minibatch_log_probs_old = log_probs_old[minibatch_indices]\n",
    "                minibatch_returns = returns[minibatch_indices]\n",
    "                minibatch_advantages = advantages[minibatch_indices]\n",
    "\n",
    "                mu, std, values = self.actor_critic(minibatch_states)\n",
    "                dist = torch.distributions.Normal(mu, std)\n",
    "                log_probs_new = dist.log_prob(minibatch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratios = torch.exp(log_probs_new - minibatch_log_probs_old)\n",
    "                surr1 = ratios * minibatch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * minibatch_advantages\n",
    "\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy\n",
    "                critic_loss = self.mse_loss(values, minibatch_returns)\n",
    "\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train(self, env, total_episodes=1000000, horizon=2048, batch_size=64):\n",
    "        all_rewards = []\n",
    "\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            state = env.reset()[0]\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "                \n",
    "\n",
    "                if len(states) % batch_size == 0:\n",
    "                    next_value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {episode + 1}, Total Reward = {episode_rewards}\")\n",
    "\n",
    "\n",
    "        return all_rewards\n",
    "    \n",
    "    def evaluate(self, env, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state = state[0] if isinstance(state, tuple) else state\n",
    "                \n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state array to tensor\n",
    "               \n",
    "                dist, value = self.actor_critic(state_tensor)\n",
    "                action = dist.sample()\n",
    "               \n",
    "                # Ensure action remains a 1-dimensional array with a single element\n",
    "                action_numpy = action.squeeze().detach().numpy()  # Squeeze to potentially reduce dimensions\n",
    "                if action_numpy.ndim == 0:  # If the result is a scalar, convert it back to an array\n",
    "                    action_numpy = np.array([action_numpy])\n",
    "                \n",
    "                next_state, reward, done, _, _ = env.step(action_numpy)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "# Initialize PPOAgent and train\n",
    "ppo = PPOAgent(4, 1)\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "rewards = ppo.train(env, total_episodes=1000000, horizon=2048, batch_size=64)\n",
    "\n",
    "def run_experiments(env, param_grid):\n",
    "    results = []\n",
    "\n",
    "    for params in param_grid:\n",
    "        lr, epsilon, k_epochs, batch_size = params\n",
    "        print(f\"Running experiment with lr={lr}, epsilon={epsilon}, k_epochs={k_epochs}, batch_size={batch_size}\")\n",
    "\n",
    "        ppo = PPOAgent(input_dim=4, action_dim=1, lr=lr, epsilon=epsilon, k_epochs=k_epochs, minibatch_size=batch_size)\n",
    "        rewards = ppo.train(env, total_episodes=100)  # Reduce the number of episodes for quicker experimentation\n",
    "        avg_reward = ppo.evaluate(env)\n",
    "        \n",
    "        result = {\n",
    "            'lr': lr,\n",
    "            'epsilon': epsilon,\n",
    "            'k_epochs': k_epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'average_reward': avg_reward,\n",
    "            'rewards': rewards\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        with open('experiment_results_trial_1.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = list(product(\n",
    "    [1e-4, 3e-4, 1e-3],  # Learning rates\n",
    "    [0.1, 0.2, 0.3],     # Epsilon values\n",
    "    [5, 10, 20],         # Epochs\n",
    "    [32, 64, 128]        # Batch sizes\n",
    "))\n",
    "\n",
    "#results = run_experiments(env, param_grid)\n",
    "#print(\"Experiments completed and results saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My code so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward = 14.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 8.0\n",
      "Episode 6, Total Reward = 5.0\n",
      "Episode 7, Total Reward = 7.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 18.0\n",
      "Episode 10, Total Reward = 11.0\n",
      "Episode 11, Total Reward = 4.0\n",
      "Episode 12, Total Reward = 10.0\n",
      "Episode 13, Total Reward = 7.0\n",
      "Episode 14, Total Reward = 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shani\\AppData\\Local\\Temp\\ipykernel_29212\\1155589828.py:60: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  actions = torch.tensor(actions).detach()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15, Total Reward = 22.0\n",
      "Episode 16, Total Reward = 5.0\n",
      "Episode 17, Total Reward = 5.0\n",
      "Episode 18, Total Reward = 7.0\n",
      "Episode 19, Total Reward = 6.0\n",
      "Episode 20, Total Reward = 15.0\n",
      "Episode 21, Total Reward = 15.0\n",
      "Episode 22, Total Reward = 5.0\n",
      "Episode 23, Total Reward = 11.0\n",
      "Episode 24, Total Reward = 9.0\n",
      "Episode 25, Total Reward = 6.0\n",
      "Episode 26, Total Reward = 34.0\n",
      "Episode 27, Total Reward = 6.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 10.0\n",
      "Episode 30, Total Reward = 9.0\n",
      "Episode 31, Total Reward = 4.0\n",
      "Episode 32, Total Reward = 12.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 24.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 6.0\n",
      "Episode 37, Total Reward = 8.0\n",
      "Episode 38, Total Reward = 14.0\n",
      "Episode 39, Total Reward = 13.0\n",
      "Episode 40, Total Reward = 11.0\n",
      "Episode 41, Total Reward = 29.0\n",
      "Episode 42, Total Reward = 21.0\n",
      "Episode 43, Total Reward = 11.0\n",
      "Episode 44, Total Reward = 8.0\n",
      "Episode 45, Total Reward = 19.0\n",
      "Episode 46, Total Reward = 23.0\n",
      "Episode 47, Total Reward = 5.0\n",
      "Episode 48, Total Reward = 36.0\n",
      "Episode 49, Total Reward = 22.0\n",
      "Episode 50, Total Reward = 7.0\n",
      "Episode 51, Total Reward = 61.0\n",
      "Episode 52, Total Reward = 35.0\n",
      "Episode 53, Total Reward = 9.0\n",
      "Episode 54, Total Reward = 16.0\n",
      "Episode 55, Total Reward = 17.0\n",
      "Episode 56, Total Reward = 35.0\n",
      "Episode 57, Total Reward = 10.0\n",
      "Episode 58, Total Reward = 22.0\n",
      "Episode 59, Total Reward = 14.0\n",
      "Episode 60, Total Reward = 16.0\n",
      "Episode 61, Total Reward = 8.0\n",
      "Episode 62, Total Reward = 13.0\n",
      "Episode 63, Total Reward = 12.0\n",
      "Episode 64, Total Reward = 6.0\n",
      "Episode 65, Total Reward = 24.0\n",
      "Episode 66, Total Reward = 25.0\n",
      "Episode 67, Total Reward = 18.0\n",
      "Episode 68, Total Reward = 10.0\n",
      "Episode 69, Total Reward = 74.0\n",
      "Episode 70, Total Reward = 17.0\n",
      "Episode 71, Total Reward = 11.0\n",
      "Episode 72, Total Reward = 45.0\n",
      "Episode 73, Total Reward = 162.0\n",
      "Episode 74, Total Reward = 99.0\n",
      "Episode 75, Total Reward = 109.0\n",
      "Episode 76, Total Reward = 22.0\n",
      "Episode 77, Total Reward = 47.0\n",
      "Episode 78, Total Reward = 19.0\n",
      "Episode 79, Total Reward = 47.0\n",
      "Episode 80, Total Reward = 12.0\n",
      "Episode 81, Total Reward = 49.0\n",
      "Episode 82, Total Reward = 14.0\n",
      "Episode 83, Total Reward = 30.0\n",
      "Episode 84, Total Reward = 64.0\n",
      "Episode 85, Total Reward = 33.0\n",
      "Episode 86, Total Reward = 41.0\n",
      "Episode 87, Total Reward = 30.0\n",
      "Episode 88, Total Reward = 16.0\n",
      "Episode 89, Total Reward = 55.0\n",
      "Episode 90, Total Reward = 21.0\n",
      "Episode 91, Total Reward = 53.0\n",
      "Episode 92, Total Reward = 99.0\n",
      "Episode 93, Total Reward = 65.0\n",
      "Episode 94, Total Reward = 52.0\n",
      "Episode 95, Total Reward = 69.0\n",
      "Episode 96, Total Reward = 59.0\n",
      "Episode 97, Total Reward = 138.0\n",
      "Episode 98, Total Reward = 91.0\n",
      "Episode 99, Total Reward = 293.0\n",
      "Episode 100, Total Reward = 125.0\n",
      "Episode 101, Total Reward = 92.0\n",
      "Episode 102, Total Reward = 114.0\n",
      "Episode 103, Total Reward = 75.0\n",
      "Episode 104, Total Reward = 60.0\n",
      "Episode 105, Total Reward = 82.0\n",
      "Episode 106, Total Reward = 67.0\n",
      "Episode 107, Total Reward = 63.0\n",
      "Episode 108, Total Reward = 45.0\n",
      "Episode 109, Total Reward = 51.0\n",
      "Episode 110, Total Reward = 53.0\n",
      "Episode 111, Total Reward = 41.0\n",
      "Episode 112, Total Reward = 50.0\n",
      "Episode 113, Total Reward = 53.0\n",
      "Episode 114, Total Reward = 56.0\n",
      "Episode 115, Total Reward = 52.0\n",
      "Episode 116, Total Reward = 11.0\n",
      "Episode 117, Total Reward = 72.0\n",
      "Episode 118, Total Reward = 85.0\n",
      "Episode 119, Total Reward = 85.0\n",
      "Episode 120, Total Reward = 146.0\n",
      "Episode 121, Total Reward = 53.0\n",
      "Episode 122, Total Reward = 88.0\n",
      "Episode 123, Total Reward = 46.0\n",
      "Episode 124, Total Reward = 71.0\n",
      "Episode 125, Total Reward = 54.0\n",
      "Episode 126, Total Reward = 73.0\n",
      "Episode 127, Total Reward = 65.0\n",
      "Episode 128, Total Reward = 51.0\n",
      "Episode 129, Total Reward = 61.0\n",
      "Episode 130, Total Reward = 71.0\n",
      "Episode 131, Total Reward = 150.0\n",
      "Episode 132, Total Reward = 101.0\n",
      "Episode 133, Total Reward = 106.0\n",
      "Episode 134, Total Reward = 68.0\n",
      "Episode 135, Total Reward = 102.0\n",
      "Episode 136, Total Reward = 109.0\n",
      "Episode 137, Total Reward = 245.0\n",
      "Episode 138, Total Reward = 60.0\n",
      "Episode 139, Total Reward = 99.0\n",
      "Episode 140, Total Reward = 72.0\n",
      "Episode 141, Total Reward = 122.0\n",
      "Episode 142, Total Reward = 149.0\n",
      "Episode 143, Total Reward = 6.0\n",
      "Episode 144, Total Reward = 180.0\n",
      "Episode 145, Total Reward = 43.0\n",
      "Episode 146, Total Reward = 295.0\n",
      "Episode 147, Total Reward = 200.0\n",
      "Episode 148, Total Reward = 149.0\n",
      "Episode 149, Total Reward = 265.0\n",
      "Episode 150, Total Reward = 306.0\n",
      "Episode 151, Total Reward = 353.0\n",
      "Episode 152, Total Reward = 1000.0\n",
      "Episode 153, Total Reward = 205.0\n",
      "Episode 154, Total Reward = 208.0\n",
      "Episode 155, Total Reward = 728.0\n",
      "Episode 156, Total Reward = 248.0\n",
      "Episode 157, Total Reward = 463.0\n",
      "Episode 158, Total Reward = 293.0\n",
      "Episode 159, Total Reward = 1000.0\n",
      "Episode 160, Total Reward = 1000.0\n",
      "Episode 161, Total Reward = 1000.0\n",
      "Episode 162, Total Reward = 1000.0\n",
      "Episode 163, Total Reward = 351.0\n",
      "Episode 164, Total Reward = 1000.0\n",
      "Episode 165, Total Reward = 1000.0\n",
      "Episode 166, Total Reward = 1000.0\n",
      "Episode 167, Total Reward = 1000.0\n",
      "Episode 168, Total Reward = 1000.0\n",
      "Episode 169, Total Reward = 1000.0\n",
      "Episode 170, Total Reward = 1000.0\n",
      "Episode 171, Total Reward = 1000.0\n",
      "Episode 172, Total Reward = 6.0\n",
      "Episode 173, Total Reward = 1000.0\n",
      "Episode 174, Total Reward = 381.0\n",
      "Episode 175, Total Reward = 3.0\n",
      "Episode 176, Total Reward = 1000.0\n",
      "Episode 177, Total Reward = 1000.0\n",
      "Episode 178, Total Reward = 1000.0\n",
      "Episode 179, Total Reward = 1000.0\n",
      "Episode 180, Total Reward = 1000.0\n",
      "Episode 181, Total Reward = 1000.0\n",
      "Episode 182, Total Reward = 1000.0\n",
      "Episode 183, Total Reward = 1000.0\n",
      "Episode 184, Total Reward = 1000.0\n",
      "Episode 185, Total Reward = 1000.0\n",
      "Episode 186, Total Reward = 1000.0\n",
      "Episode 187, Total Reward = 1000.0\n",
      "Episode 188, Total Reward = 1000.0\n",
      "Episode 189, Total Reward = 1000.0\n",
      "Episode 190, Total Reward = 1000.0\n",
      "Episode 191, Total Reward = 1000.0\n",
      "Episode 192, Total Reward = 1000.0\n",
      "Episode 193, Total Reward = 1000.0\n",
      "Episode 194, Total Reward = 1000.0\n",
      "Episode 195, Total Reward = 1000.0\n",
      "Episode 196, Total Reward = 1000.0\n",
      "Episode 197, Total Reward = 1000.0\n",
      "Episode 198, Total Reward = 962.0\n",
      "Episode 199, Total Reward = 1000.0\n",
      "Episode 200, Total Reward = 1000.0\n",
      "Episode 201, Total Reward = 1000.0\n",
      "Episode 202, Total Reward = 1000.0\n",
      "Episode 203, Total Reward = 1000.0\n",
      "Episode 204, Total Reward = 1000.0\n",
      "Episode 205, Total Reward = 1000.0\n",
      "Episode 206, Total Reward = 1000.0\n",
      "Episode 207, Total Reward = 1000.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 173\u001b[0m\n\u001b[0;32m    171\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPOAgent(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    172\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvertedPendulum-v4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 173\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m average_reward \u001b[38;5;241m=\u001b[39m ppo\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maverage_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 125\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[1;34m(self, env, total_episodes, horizon, batch_size)\u001b[0m\n\u001b[0;32m    123\u001b[0m     advantages \u001b[38;5;241m=\u001b[39m [ret \u001b[38;5;241m-\u001b[39m val \u001b[38;5;28;01mfor\u001b[39;00m ret, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(returns, values)]\n\u001b[0;32m    124\u001b[0m     trajectory \u001b[38;5;241m=\u001b[39m (states, actions, log_probs, returns, advantages)\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrajectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     states, actions, rewards, log_probs, values, masks \u001b[38;5;241m=\u001b[39m [], [], [], [], [], []\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "Cell \u001b[1;32mIn[2], line 92\u001b[0m, in \u001b[0;36mPPOAgent.update\u001b[1;34m(self, trajectory)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     91\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 92\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\optim\\adam.py:423\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    421\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom)\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 423\u001b[0m     step \u001b[38;5;241m=\u001b[39m \u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_t\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n\u001b[0;32m    426\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m step\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\optim\\optimizer.py:89\u001b[0m, in \u001b[0;36m_get_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = Normal(mu, std)\n",
    "        value = self.critic(x)\n",
    "        return dist, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, k_epochs=10, minibatch_size=64, gae_lambda=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.actor_critic = ActorCritic(input_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        dist, _ = self.actor_critic(state)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            indices = np.arange(states.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, states.shape[0], self.minibatch_size):\n",
    "                end = start + self.minibatch_size\n",
    "                minibatch_indices = indices[start:end]\n",
    "                \n",
    "                minibatch_states = states[minibatch_indices]\n",
    "                minibatch_actions = actions[minibatch_indices]\n",
    "                minibatch_log_probs_old = log_probs_old[minibatch_indices]\n",
    "                minibatch_returns = returns[minibatch_indices]\n",
    "                minibatch_advantages = advantages[minibatch_indices]\n",
    "\n",
    "                dist, values = self.actor_critic(minibatch_states)\n",
    "                log_probs_new = dist.log_prob(minibatch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratios = torch.exp(log_probs_new - minibatch_log_probs_old)\n",
    "                surr1 = ratios * minibatch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * minibatch_advantages\n",
    "\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy\n",
    "                critic_loss = self.mse_loss(values, minibatch_returns)\n",
    "\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train(self, env, total_episodes=1000000, horizon=2048, batch_size=64):\n",
    "        all_rewards = []\n",
    "\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            state = env.reset()\n",
    "            state = state[0] if isinstance(state, tuple) else state  # Extract state from tuple if necessary\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                dist, value = self.actor_critic(torch.from_numpy(state).float())\n",
    "                next_state, reward, done, _,_ = env.step(action)\n",
    "                next_state = next_state[0] if isinstance(next_state, tuple) else next_state  # Extract next_state from tuple if necessary\n",
    "\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value.item())\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "\n",
    "                if len(states) % batch_size == 0:\n",
    "                    next_value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {episode + 1}, Total Reward = {episode_rewards}\")\n",
    "\n",
    "        return all_rewards\n",
    "    \n",
    "    def evaluate(self, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state = state[0] if isinstance(state, tuple) else state\n",
    "                \n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state array to tensor\n",
    "               \n",
    "                dist, value = self.agent(state_tensor)\n",
    "                action = dist.sample()\n",
    "               \n",
    "\n",
    "                # Ensure action remains a 1-dimensional array with a single element\n",
    "                action_numpy = action.squeeze().detach().numpy()  # Squeeze to potentially reduce dimensions\n",
    "                if action_numpy.ndim == 0:  # If the result is a scalar, convert it back to an array\n",
    "                    action_numpy = np.array([action_numpy])\n",
    "                \n",
    "               \n",
    "\n",
    "                next_state, reward, done, _, _ = self.env.step(action_numpy)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                #print(\"reward\", reward)\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "# Initialize PPOAgent and train\n",
    "ppo = PPOAgent(4, 1)\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "rewards = ppo.train(env)\n",
    "average_reward = ppo.evaluate()\n",
    "print(f\"Average reward: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiements():\n",
    "    epsilon, lr, epochs, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph:\n",
    "    total rewards vs episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        value = self.critic(x)\n",
    "        return dist, value\n",
    "\n",
    "    \n",
    "    \n",
    "    # def __init__(self, input_dim, action_dim):\n",
    "    #     super(ActorCritic, self).__init__()\n",
    "    #     self.fc_common = nn.Sequential(\n",
    "    #         nn.Linear(input_dim, 64),\n",
    "    #         nn.Tanh(),\n",
    "    #         nn.Linear(64, 64),\n",
    "    #         nn.Tanh()\n",
    "    #     )\n",
    "    #     self.fc_actor = nn.Linear(64, action_dim)\n",
    "    #     self.fc_critic = nn.Linear(64, 1)\n",
    "    #     self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.fc_common(x)\n",
    "    #     mu = self.fc_actor(x)\n",
    "    #     value = self.fc_critic(x)\n",
    "    #     std = self.log_std.exp().expand_as(mu)\n",
    "    #     return mu, std, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, k_epochs=1, minibatch_size=64, gae_lambda=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.actor_critic = ActorCritic(input_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        mu, std, _ = self.actor_critic(state)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            indices = np.arange(states.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, states.shape[0], self.minibatch_size):\n",
    "                end = start + self.minibatch_size\n",
    "                minibatch_indices = indices[start:end]\n",
    "                \n",
    "                minibatch_states = states[minibatch_indices]\n",
    "                minibatch_actions = actions[minibatch_indices]\n",
    "                minibatch_log_probs_old = log_probs_old[minibatch_indices]\n",
    "                minibatch_returns = returns[minibatch_indices]\n",
    "                minibatch_advantages = advantages[minibatch_indices]\n",
    "\n",
    "                mu, std, values = self.actor_critic(minibatch_states)\n",
    "                dist = torch.distributions.Normal(mu, std)\n",
    "                log_probs_new = dist.log_prob(minibatch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratios = torch.exp(log_probs_new - minibatch_log_probs_old)\n",
    "                surr1 = ratios * minibatch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * minibatch_advantages\n",
    "\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy\n",
    "                critic_loss = self.mse_loss(values, minibatch_returns)\n",
    "\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train(self, env, total_timesteps=1000000, horizon=2048):\n",
    "        all_rewards = []\n",
    "        step_counter = 0\n",
    "\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        while step_counter < total_timesteps:\n",
    "            state = env.reset()[0]\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "                step_counter += 1\n",
    "\n",
    "                if step_counter % horizon == 0:\n",
    "                    next_value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {len(all_rewards)}, Total Reward = {episode_rewards}, Total Steps = {step_counter}\")\n",
    "\n",
    "        return all_rewards\n",
    "\n",
    "# Initialize PPOAgent and train\n",
    "ppo = PPOAgent(4, 1)\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "rewards = ppo.train(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        value = self.critic(x)\n",
    "        return dist, value\n",
    "\n",
    "    \n",
    "    \n",
    "    # def __init__(self, input_dim, action_dim):\n",
    "    #     super(ActorCritic, self).__init__()\n",
    "    #     self.fc_common = nn.Sequential(\n",
    "    #         nn.Linear(input_dim, 64),\n",
    "    #         nn.Tanh(),\n",
    "    #         nn.Linear(64, 64),\n",
    "    #         nn.Tanh()\n",
    "    #     )\n",
    "    #     self.fc_actor = nn.Linear(64, action_dim)\n",
    "    #     self.fc_critic = nn.Linear(64, 1)\n",
    "    #     self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.fc_common(x)\n",
    "    #     mu = self.fc_actor(x)\n",
    "    #     value = self.fc_critic(x)\n",
    "    #     std = self.log_std.exp().expand_as(mu)\n",
    "    #     return mu, std, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, k_epochs=1, minibatch_size=64, gae_lambda=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.actor_critic = ActorCritic(input_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        mu, std, _ = self.actor_critic(state)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            indices = np.arange(states.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, states.shape[0], self.minibatch_size):\n",
    "                end = start + self.minibatch_size\n",
    "                minibatch_indices = indices[start:end]\n",
    "                \n",
    "                minibatch_states = states[minibatch_indices]\n",
    "                minibatch_actions = actions[minibatch_indices]\n",
    "                minibatch_log_probs_old = log_probs_old[minibatch_indices]\n",
    "                minibatch_returns = returns[minibatch_indices]\n",
    "                minibatch_advantages = advantages[minibatch_indices]\n",
    "\n",
    "                mu, std, values = self.actor_critic(minibatch_states)\n",
    "                dist = torch.distributions.Normal(mu, std)\n",
    "                log_probs_new = dist.log_prob(minibatch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratios = torch.exp(log_probs_new - minibatch_log_probs_old)\n",
    "                surr1 = ratios * minibatch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * minibatch_advantages\n",
    "\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy\n",
    "                critic_loss = self.mse_loss(values, minibatch_returns)\n",
    "\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train(self, env, total_timesteps=1000000, horizon=2048):\n",
    "        all_rewards = []\n",
    "        step_counter = 0\n",
    "\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        while step_counter < total_timesteps:\n",
    "            state = env.reset()[0]\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "                step_counter += 1\n",
    "\n",
    "                if step_counter % horizon == 0:\n",
    "                    next_value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {len(all_rewards)}, Total Reward = {episode_rewards}, Total Steps = {step_counter}\")\n",
    "\n",
    "        return all_rewards\n",
    "\n",
    "# Initialize PPOAgent and train\n",
    "ppo = PPOAgent(4, 1)\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "rewards = ppo.train(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, horizon=2048, epochs=10, lr=3e-4, gamma=0.99, c1=0.5, c2=0.01, minibatch_size=64, lam=0.95, epsilon=0.2):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.lam = lam\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            states, actions, rewards, log_probs, values, dones, episode_rewards = self.generate_trajectories()\n",
    "            returns, advantages = self.compute_gae(rewards, values, dones)\n",
    "            \n",
    "            # Flatten the trajectories\n",
    "            states = torch.cat(states)\n",
    "            actions = torch.cat(actions)\n",
    "            log_probs = torch.cat(log_probs)\n",
    "            returns = torch.cat(returns)\n",
    "            advantages = torch.cat(advantages)\n",
    "            \n",
    "            print('Updating')\n",
    "            self.update(states, actions, returns, log_probs, advantages)\n",
    "\n",
    "            # Print the total reward per episode\n",
    "            total_reward = sum(episode_rewards)\n",
    "            print(f'Total Reward per Episode: {total_reward}')\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        dones = []\n",
    "        episode_rewards = []\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        for _ in range(self.horizon):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            dist, value = self.agent(state_tensor)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "            \n",
    "            states.append(state_tensor)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(dist.log_prob(action).sum(dim=-1, keepdim=True))\n",
    "            values.append(value)\n",
    "            dones.append(done)\n",
    "            episode_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_reward = 0\n",
    "                state = self.env.reset()[0]\n",
    "\n",
    "        if episode_reward != 0:\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "        return states, actions, rewards, log_probs, values, dones, episode_rewards\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        values = values + [torch.zeros(1, 1)]\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + self.gamma * values[i + 1] * (1 - dones[i]) - values[i]\n",
    "            gae = delta + self.gamma * self.lam * (1 - dones[i]) * gae\n",
    "            returns.insert(0, gae + values[i])\n",
    "            advantages.insert(0, gae)\n",
    "        return returns, advantages\n",
    "\n",
    "    def update(self, states, actions, returns, log_probs, advantages):\n",
    "        for _ in range(self.horizon // self.minibatch_size):\n",
    "            indices = torch.randint(0, self.horizon, size=(self.minibatch_size,))\n",
    "            sampled_states = states[indices]\n",
    "            sampled_actions = actions[indices]\n",
    "            sampled_returns = returns[indices]\n",
    "            sampled_log_probs = log_probs[indices]\n",
    "            sampled_advantages = advantages[indices]\n",
    "\n",
    "            dist, values = self.agent(sampled_states)\n",
    "            new_log_probs = dist.log_prob(sampled_actions).sum(dim=-1, keepdim=True)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            ratio = (new_log_probs - sampled_log_probs).exp()\n",
    "            surr1 = ratio * sampled_advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * sampled_advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (sampled_returns - values).pow(2).mean()\n",
    "\n",
    "            loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def evaluate(self, env, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, _ = self.agent(state_tensor)\n",
    "                action = dist.mean  # Using mean action for evaluation\n",
    "                next_state, reward, done, _, _ = env.step(action.detach().numpy()[0])\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "ppo = PPO(\"InvertedPendulum-v4\")\n",
    "ppo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.autograd\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        dist = Normal(mean, std)\n",
    "        value = self.critic(x)\n",
    "        return dist, value\n",
    "    \n",
    "class PPO:\n",
    "    def __init__(self, env_name, T=2048, minibatch_size=64, timestep = 1000, epochs=10, gamma=0.99, gae_lambda=0.95, clip_param=0.2, lr=3e-4,epsilon=0.2):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.T = T\n",
    "        self.epsilon = epsilon\n",
    "        self.timestep = timestep\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_param = clip_param\n",
    "        self.agent = PPOAgent(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "    \n",
    "        initial_state = self.env.reset()\n",
    "        state = initial_state[0] if isinstance(initial_state, tuple) else initial_state\n",
    "\n",
    "        for time in range(self.timestep):\n",
    "            \n",
    "            states, actions, rewards, dones, values, log_probs = [], [], [], [], [], []\n",
    "            state=self.env.reset()\n",
    "            for t in range(self.T):\n",
    "                # print(f't {t + 1}')\n",
    "                # Handle both tuple and direct array returns for state consistency\n",
    "                state = state[0] if isinstance(state, tuple) else state\n",
    "                \n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state array to tensor\n",
    "\n",
    "                dist, value = self.agent(state_tensor)\n",
    "                \n",
    "                \n",
    "                action = dist.sample()\n",
    "                \n",
    "                log_prob = dist.log_prob(action)\n",
    "                action=action.item()\n",
    "                \n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    action= np.random.choice([0, 1, 2])\n",
    "\n",
    "                # Ensure action remains a 1-dimensional array with a single element\n",
    "                # action_numpy = action.squeeze().detach().numpy()  # Squeeze to potentially reduce dimensions\n",
    "                # if action_numpy.ndim == 0:  # If the result is a scalar, convert it back to an array\n",
    "                #     action_numpy = np.array([action_numpy])  # Convert to 1-dimensional array with one element\n",
    "                action=np.array([action])\n",
    "                # print(\"action\", action)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                \n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                values.append(value.detach())\n",
    "                log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                state = next_state \n",
    "                \n",
    "                # update epsilon if the agent is learning\n",
    "                if self.epsilon!=0:\n",
    "                    self.epsilon = max(0.01, self.epsilon * 0.9999)\n",
    "                \n",
    "                # print(\"reward\", reward)\n",
    "                if state is None:\n",
    "                    raise ValueError(\"Environment reset returned None state.\")\n",
    "\n",
    "            data = states, actions, rewards, dones, values, log_probs\n",
    "            print(f'timestep  {time + 1}, reward {sum(rewards)}, epsilon {self.epsilon}')\n",
    "            # print(\"train ok\")\n",
    "            self.policy_update(data)\n",
    "\n",
    "\n",
    "\n",
    "    def policy_update(self, data):\n",
    "        # print(\"policy update\")\n",
    "        states, actions, rewards, dones, values, log_probs = data\n",
    "        advantages = self.calculate_advantages(rewards, values, dones)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        old_values = torch.cat(values).detach()  # Ensure values are detached\n",
    "        old_log_probs = torch.cat(log_probs).detach() \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # print(f'Epoch {epoch + 1}')\n",
    "            idx = torch.randperm(len(data[0]))[:self.minibatch_size]\n",
    "            sampled_states = states[idx]\n",
    "            sampled_actions = actions[idx]\n",
    "            sampled_advantages = advantages[idx]\n",
    "            sampled_old_log_probs = old_log_probs[idx]\n",
    "\n",
    "            new_dist, new_values = self.agent(sampled_states)\n",
    "            new_log_probs = new_dist.log_prob(sampled_actions)\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - sampled_old_log_probs)\n",
    "            surr1 = ratio * sampled_advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_param, 1 + self.clip_param) * sampled_advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = 0.5 * (sampled_advantages - new_values.squeeze()).pow(2).mean()  # Make sure shapes match\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()  # 'retain_graph=True' is typically not needed unless explicitly required\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    # On the last pass, you may not need to retain the graph, or this can be handled outside the loop\n",
    "\n",
    "\n",
    "    def calculate_advantages(self, rewards, values, dones):\n",
    "        \n",
    "        # Append zero tensor with the same shape as the last value tensor\n",
    "        zero_padding = torch.zeros_like(values[-1])\n",
    "        values.append(zero_padding)\n",
    "        values = torch.cat(values)\n",
    "        gae = 0\n",
    "        advantages = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[step]) * gae\n",
    "            advantages.insert(0, gae + values[step])\n",
    "\n",
    "        return torch.FloatTensor(advantages)\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate(self, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state = state[0] if isinstance(state, tuple) else state\n",
    "                \n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state array to tensor\n",
    "               \n",
    "                dist, value = self.agent(state_tensor)\n",
    "                action = dist.sample()\n",
    "               \n",
    "\n",
    "                # Ensure action remains a 1-dimensional array with a single element\n",
    "                action_numpy = action.squeeze().detach().numpy()  # Squeeze to potentially reduce dimensions\n",
    "                if action_numpy.ndim == 0:  # If the result is a scalar, convert it back to an array\n",
    "                    action_numpy = np.array([action_numpy])\n",
    "                \n",
    "               \n",
    "\n",
    "                next_state, reward, done, _, _ = self.env.step(action_numpy)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                print(\"reward\", reward)\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "# Example usage\n",
    "ppo = PPO(\"InvertedPendulum-v4\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "ppo.train()\n",
    "#average_reward = ppo.evaluate(num_episodes=1000000)\n",
    "#print(f\"Average Reward from Evaluation: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8*0.995**10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ignore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic Network for Continuous Action Spaces\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Learnable log standard deviation\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mean)  # Standard deviation\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        return dist, self.critic(x)\n",
    "\n",
    "# PPO Agent with Clipping Method\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, clip_epsilon=0.2):\n",
    "        self.model = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = 0.99\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        dist, _ = self.model(state_tensor)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "    def train(self, states, actions, rewards, next_states, dones, old_log_probs):\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        actions_tensor = torch.FloatTensor(actions).unsqueeze(1)\n",
    "        next_states_tensor = torch.FloatTensor(next_states)\n",
    "        rewards_tensor = torch.FloatTensor(rewards)\n",
    "        dones_tensor = torch.FloatTensor(dones)\n",
    "        old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "\n",
    "        _, values = self.model(states_tensor)\n",
    "        dists, next_values = self.model(next_states_tensor)\n",
    "\n",
    "        new_log_probs = dists.log_prob(actions_tensor).sum(axis=1, keepdim=True)\n",
    "\n",
    "        advantages = rewards_tensor + self.gamma * next_values.squeeze() * (1 - dones_tensor) - values.squeeze()\n",
    "\n",
    "        ratios = (new_log_probs - old_log_probs_tensor).exp()\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "        surrogate_loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
    "\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        loss = surrogate_loss + critic_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Training Function\n",
    "def train_agent(agent, env, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        states, actions, rewards, next_states, dones, old_log_probs = [], [], [], [], [], []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, done, _,_ = env.step([action])  # Action needs to be a list\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            old_log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            \n",
    "\n",
    "        agent.train(states, actions, rewards, next_states, dones, old_log_probs)\n",
    "        print(f'Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4')\n",
    "# agent = PPOAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "# train_agent(agent, env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent, env, episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.select_action(state)  # Use the trained policy to select actions\n",
    "            state, reward, done, _ , _= env.step([action])  # Action should be in the correct format\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f'Evaluation Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "    \n",
    "    average_reward = sum(total_rewards) / len(total_rewards)\n",
    "    print(f'Average Reward over {episodes} episodes: {average_reward}')\n",
    "    return average_reward\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4', render_mode='human')\n",
    "# evaluate_policy(agent, env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('InvertedPendulum-v4', render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        return dist, self.critic(x)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, horizon=2048, lr=3e-4, clip_epsilon=0.2, gamma=0.99, gae_lambda=0.95, epochs=10, minibatch_size=64):\n",
    "        self.model = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        dist, _ = self.model(state_tensor)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "    def train(self, states, actions, rewards, next_states, dones, old_log_probs):\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        actions_tensor = torch.FloatTensor(actions).unsqueeze(1)\n",
    "        next_states_tensor = torch.FloatTensor(next_states)\n",
    "        rewards_tensor = torch.FloatTensor(rewards)\n",
    "        dones_tensor = torch.FloatTensor(dones)\n",
    "        old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "\n",
    "        _, values = self.model(states_tensor)\n",
    "        dists, next_values = self.model(next_states_tensor)\n",
    "\n",
    "        new_log_probs = dists.log_prob(actions_tensor).sum(axis=1, keepdim=True)\n",
    "\n",
    "        advantages = rewards_tensor + self.gamma * next_values.squeeze() * (1 - dones_tensor) - values.squeeze()\n",
    "\n",
    "        ratios = (new_log_probs - old_log_probs_tensor).exp()\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "        surrogate_loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
    "\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        loss = surrogate_loss + critic_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "def train_agent(agent, env, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        states, actions, rewards, next_states, dones, old_log_probs = [], [], [], [], [], []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step([action])\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            old_log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        agent.train(states, actions, rewards, next_states, dones, old_log_probs)\n",
    "        print(f'Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4')\n",
    "# agent = PPOAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "# train_agent(agent, env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent, env, episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.select_action(state)  # Use the trained policy to select actions\n",
    "            state, reward, done, _ , _= env.step([action])  # Action should be in the correct format\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f'Evaluation Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "    \n",
    "    average_reward = sum(total_rewards) / len(total_rewards)\n",
    "    print(f'Average Reward over {episodes} episodes: {average_reward}')\n",
    "    return average_reward\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4', render_mode='human')\n",
    "# evaluate_policy(agent, env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions import Normal\n",
    "# import numpy as np\n",
    "# import os\n",
    "# # from torch.utils.tensorboard import SummaryWriter\n",
    "# from functools import reduce\n",
    "# from operator import mul\n",
    "\n",
    "# glob_i = 0\n",
    "# glob_error = 0\n",
    "\n",
    "# class PPOAgent():\n",
    "#     def __init__(self, TRAIN, env=None, traj=3, net_size=64, net_std=1,\n",
    "#                 lr=1e-4, bs=100, y=0.99, ep=10, W=None,\n",
    "#                 c1=0.5, c2=0.01):\n",
    "#         self.TRAIN  = TRAIN\n",
    "#         self.LR     = lr\n",
    "#         self.GAMMA  = y\n",
    "#         self.BATCHSIZE  = bs\n",
    "#         self.EPOCHS = ep\n",
    "#         self.NTRAJ = traj\n",
    "#         self.env = env\n",
    "#         self.C1 = c1\n",
    "#         self.C2 = c2\n",
    "\n",
    "#         self.WRITER = W\n",
    "\n",
    "#         self.input_size = reduce(mul, env.observation_space.shape, 1)\n",
    "#         self.out_size   = env.action_space.shape[0]\n",
    "\n",
    "#         self.model = AgentNet(self.input_size, self.out_size, h=net_size, std=net_std)\n",
    "#         self.opt = optim.Adam(self.model.parameters(), lr=self.LR)\n",
    "\n",
    "#         self.value_criterion = nn.MSELoss()\n",
    "\n",
    "#         self.trajectories = []\n",
    "#         self.states   = []\n",
    "#         self.rewards  = []\n",
    "#         self.actions  = []\n",
    "#         self.values   = []\n",
    "#         self.logAprob = []\n",
    "\n",
    "#     def __call__(self, state):\n",
    "#         # Each time an action is required, we save the\n",
    "#         # state value for computing advantages later\n",
    "#         state = torch.FloatTensor(state).reshape(1, self.input_size)\n",
    "#         normal, _ = self.model(state) \n",
    "#         action = normal.sample().reshape(1, self.out_size)\n",
    "\n",
    "#         self.values.append(normal.mean)\n",
    "#         self.actions.append(action)\n",
    "#         self.logAprob.append(normal.log_prob(action).squeeze(0))\n",
    "\n",
    "#         return action.numpy().reshape(self.out_size)\n",
    "\n",
    "#     def observe(self, s, r, s1, done, NEPISODE):\n",
    "#         if not self.TRAIN: return\n",
    "\n",
    "#         s  = torch.FloatTensor(s).reshape(1, self.input_size)\n",
    "#         s1 = torch.FloatTensor(s1).reshape(1, self.input_size)\n",
    "\n",
    "#         self.states.append(s)\n",
    "\n",
    "#         # For rewards we only maintain the value.\n",
    "#         self.rewards.append(r)\n",
    "\n",
    "#         if done:\n",
    "#             # Compute advantages using critic network\n",
    "#             with torch.no_grad():\n",
    "#                 s1_tensor = s1\n",
    "#                 _, next_value = self.model(s1_tensor)\n",
    "#             next_value = next_value.detach().squeeze().numpy()\n",
    "#             advantages = self._compute_advantages(next_value)\n",
    "\n",
    "#             # Update trajectories with advantages\n",
    "#             for traj, adv in zip(self.trajectories, advantages):\n",
    "#                 traj['Adv'] = torch.FloatTensor(adv)\n",
    "\n",
    "#             # Update Condition\n",
    "#             if NEPISODE != 0 and (NEPISODE % self.NTRAJ) == 0: \n",
    "#                 self.update() \n",
    "\n",
    "#             self.states   = []\n",
    "#             self.actions  = []\n",
    "#             self.logAprob = []\n",
    "#             self.values   = []\n",
    "#             self.rewards  = []\n",
    "\n",
    "#     def _compute_advantages(self, next_value):\n",
    "#         advantages = []\n",
    "#         for traj in self.trajectories:\n",
    "#             rewards = traj['r']\n",
    "#             values = traj['V'].numpy()\n",
    "#             deltas = [r + self.GAMMA * next_v - v for r, next_v, v in zip(rewards, [next_value] + values[:-1], values)]\n",
    "#             advantages.extend(self._discount_cumsum(deltas, self.GAMMA * self.C2))\n",
    "#         return advantages\n",
    "\n",
    "#     def _discount_cumsum(self, x, discount):\n",
    "#         \"\"\"\n",
    "#         Compute discounted cumulative sums of vectors.\n",
    "\n",
    "#         Parameters:\n",
    "#             x (list): Input list of numbers.\n",
    "#             discount (float): Discount factor.\n",
    "\n",
    "#         Returns:\n",
    "#             list: Discounted cumulative sums.\n",
    "#         \"\"\"\n",
    "#         discounted = [x[-1]]\n",
    "#         for v in reversed(x[:-1]):\n",
    "#             discounted.append(v + discount * discounted[-1])\n",
    "#         return list(reversed(discounted))\n",
    "\n",
    "#     def update(self):\n",
    "#         EPS = 0.2\n",
    "\n",
    "#         # Compute advantages\n",
    "#         S = torch.cat([x['S'] for x in self.trajectories], 0)\n",
    "#         A = torch.cat([x['A'] for x in self.trajectories], 0)\n",
    "#         Adv = torch.cat([x['Adv'] for x in self.trajectories], 0)\n",
    "#         Log_old = torch.cat([x['LogP'] for x in self.trajectories], 0)\n",
    "#         G = torch.cat([x['G'] for x in self.trajectories], 0)\n",
    "#         V = torch.cat([x['V'] for x in self.trajectories], 0)\n",
    "\n",
    "#         bufsize = S.size(0)\n",
    "\n",
    "#         for ep in range(self.EPOCHS*(bufsize//self.BATCHSIZE+1)):\n",
    "#             ids = np.random.randint(0, bufsize,\n",
    "#                     min(self.BATCHSIZE, bufsize))\n",
    "\n",
    "#             bS, bA, bAdv = S[ids,:], A[ids,:], Adv[ids,:]\n",
    "\n",
    "#             normal, Vnew = self.model(bS)\n",
    "#             logAprob_old = Log_old[ids,:]\n",
    "#             logAprob = normal.log_prob(bA)\n",
    "\n",
    "#             # L_CLIP\n",
    "#             ratio = (logAprob - logAprob_old).exp().squeeze(0)\n",
    "#             m1 = ratio * bAdv\n",
    "#             m2 = torch.clamp(ratio, 1.0 - EPS, 1.0 + EPS) * bAdv\n",
    "#             L_CLIP = torch.min(m1, m2).mean()\n",
    "\n",
    "#             # L_VF\n",
    "#             L_VF = (G[ids,:] - Vnew).pow(2).mean()\n",
    "\n",
    "#             # Entropy\n",
    "#             E = normal.entropy().mean()\n",
    "\n",
    "#             # Total Loss\n",
    "#             L = -L_CLIP + self.C1 * L_VF - self.C2 * E\n",
    "\n",
    "#             # Apply Gradients\n",
    "#             self.opt.zero_grad()\n",
    "#             L.backward()\n",
    "#             self.opt.step()\n",
    "\n",
    "#         # Update Graphs\n",
    "#         # self.WRITER.add_scalar(\"L_VF\", L_VF, glob_i)\n",
    "#         # global glob_i\n",
    "#         # glob_i += 1\n",
    "#         self.trajectories = []\n",
    "\n",
    "#     def load(self, path):\n",
    "#         try:\n",
    "#             self.model.load_state_dict(torch.load(path))\n",
    "#             self.model.eval()\n",
    "#         except FileNotFoundError:\n",
    "#             print(f'Error: {path} not found.')\n",
    "#             exit()\n",
    "\n",
    "#     def save(self, path):\n",
    "#         torch.save(self.model.state_dict(), path)\n",
    "\n",
    "# class AgentNet(nn.Module):\n",
    "#     def __init__(self, inp, out, h=32, std=0):\n",
    "#         super(AgentNet, self).__init__()\n",
    "\n",
    "#         self.actor = nn.Sequential(\n",
    "#             nn.Linear(inp, h),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h, h//2),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h//2, out),\n",
    "#         )\n",
    "#         self.log_std = nn.Parameter(torch.ones(1, out) * std)\n",
    "\n",
    "#         self.critic = nn.Sequential(\n",
    "#             nn.Linear(inp, h),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h, h//2),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h//2, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         actor_output = self.actor(x)\n",
    "#         critic_output = self.critic(x)\n",
    "#         std = self.log_std.exp().expand_as(actor_output)\n",
    "#         dist = Normal(actor_output, std)\n",
    "#         return dist, critic_output\n",
    "\n",
    "# # main.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # import mujoco_py\n",
    "# import gym\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# # from torch.utils.tensorboard import SummaryWriter\n",
    "# # from ppo_agent import PPOAgent\n",
    "# import sys, os, json\n",
    "# from glob import glob\n",
    "# import argparse\n",
    "\n",
    "# # Environemnt Params\n",
    "# MODELS_PATH        = 'models'\n",
    "# DEFAULT_EPISODES   = 2000\n",
    "# DEFAULT_MAX_STEPS  = 2000\n",
    "# DEFAULT_CHECKPOINT = 5\n",
    "\n",
    "# ## These may be updated by arguments\n",
    "# EnvName = 'InvertedPendulum-v4'\n",
    "# TRAIN = False\n",
    "# RENDER = True\n",
    "\n",
    "# # writer = SummaryWriter(max_queue=2)\n",
    "\n",
    "# def main():\n",
    "#     global TRAIN, RENDER, EnvName\n",
    "\n",
    "#     # Training Parameters\n",
    "#     params_path = os.path.join(MODELS_PATH, f'{EnvName}.json')\n",
    "#     with open(params_path, 'r') as f:\n",
    "#         params = json.load(f)\n",
    "\n",
    "#     EPOCHS        = params[\"EPOCHS\"] if \"EPOCHS\" in params else 200\n",
    "#     LR            = params[\"LR\"] if \"LR\" in params else 1e-3\n",
    "#     C2            = params[\"C2\"] if \"C2\" in params else 0\n",
    "#     GAMMA         = params[\"GAMMA\"] if \"GAMMA\" in params else 0.99\n",
    "#     STD           = params[\"STD\"] if \"STD\" in params else 1\n",
    "#     NETSIZE       = params[\"NETSIZE\"] if \"NETSIZE\" in params else 64\n",
    "#     BATCHSIZE     = params[\"BATCHSIZE\"] if \"BATCHSIZE\" in params else 500\n",
    "#     TRAJECTORIES  = params[\"TRAJECTORIES\"] if \"TRAJECTORIES\" in params else 10\n",
    "#     MAX_STEPS     = params[\"MAX_STEPS\"] if \"MAX_STEPS\" in params else 2000\n",
    "#     CHECKPOINT    = params[\"CHECKPOINT\"] if \"CHECKPOINT\" in params else 5\n",
    "#     EPISODES      = params[\"EPISODES\"] if \"EPISODES\" in params else 2000\n",
    "\n",
    "#     print(\"Environment:  \", EnvName)\n",
    "#     print(\"Train:        \", TRAIN)\n",
    "#     print(\"EPOCHS:       \", EPOCHS)\n",
    "#     print(\"LR:           \", LR)\n",
    "#     print(\"C2:           \", C2)\n",
    "#     print(\"GAMMA:        \", GAMMA)\n",
    "#     print(\"STD:          \", STD)\n",
    "#     print(\"NETSIZE:      \", NETSIZE)\n",
    "#     print(\"BATCHSIZE:    \", BATCHSIZE)\n",
    "#     print(\"TRAJECTORIES: \", TRAJECTORIES)\n",
    "#     print(\"MAX_STEPS:    \", MAX_STEPS)\n",
    "#     print(\"CHECKPOINT:   \", CHECKPOINT)\n",
    "#     print(\"EPISODES:     \", EPISODES)\n",
    "\n",
    "#     # save model after collecting N trajectories \n",
    "#     # (which corresponds to when the update is calculated)\n",
    "#     SAVE_STEP = CHECKPOINT * TRAJECTORIES\n",
    "#     save_model_name = os.path.join(MODELS_PATH, EnvName + \".pth\")\n",
    "\n",
    "#     total = 0\n",
    "\n",
    "#     env = gym.make(EnvName, render_mode=\"human\" if RENDER else None)\n",
    "#     agent = PPOAgent(\n",
    "#             TRAIN, env=env,\n",
    "#             lr=LR, c2=C2,\n",
    "#             net_size=NETSIZE,\n",
    "#             net_std=STD,\n",
    "#             y=GAMMA,\n",
    "#             traj=TRAJECTORIES,\n",
    "#             bs=BATCHSIZE,\n",
    "#             ep=EPOCHS\n",
    "#     )\n",
    "#     if not TRAIN: agent.load(save_model_name)\n",
    "\n",
    "#     for i in range(EPISODES):\n",
    "#         state, _ = env.reset()\n",
    "\n",
    "#         for t in range(MAX_STEPS+1):\n",
    "#             # RL Step\n",
    "#             action = agent(state)\n",
    "#             new_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "#             # Impose done=True if last-step\n",
    "#             if t == MAX_STEPS: done = True\n",
    "\n",
    "#             agent.observe(state, reward, new_state, done, i)\n",
    "\n",
    "#             total += reward\n",
    "#             state  = new_state\n",
    "\n",
    "#             if done: break\n",
    "\n",
    "#         # Print Performance\n",
    "#         print(f\"[{i}] Steps: {t}\\tReward: {total}\")\n",
    "#         # writer.add_scalar('Reward', total, i)\n",
    "#         total = 0\n",
    "\n",
    "#         if TRAIN and (i % SAVE_STEP) == SAVE_STEP -1:\n",
    "#             agent.save(save_model_name)\n",
    "#             print(\"Model Checkpoint saved\")\n",
    "\n",
    "#     env.close()\n",
    "\n",
    "\n",
    "# # envs_names = glob(f'{MODELS_PATH}/*.json')\n",
    "# # envs_names = [x.split('/')[-1].split('.')[0] for x in envs_names]\n",
    "\n",
    "# # parser = argparse.ArgumentParser(description=\"Train PPO models and run Gym environments\")\n",
    "# # parser.add_argument('env', type=str, metavar=\"environment\", help=\", \".join(envs_names),\n",
    "# #                     choices=envs_names, default=\"MountainCarContinuous-v0\")\n",
    "# # parser.add_argument('--train', action='store_true')\n",
    "# # args = parser.parse_args()\n",
    "\n",
    "# # EnvName = EnvName\n",
    "# # TRAIN   = TRAIN\n",
    "# # RENDER  = not(TRAIN)\n",
    "\n",
    "# # main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, epochs=200, lr=1e-3, gamma=0.99, c1=0.5, c2=0.01, batch_size=500):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            for _ in range(self.epochs):\n",
    "                self.update(states, actions, rewards, log_probs, values)\n",
    "\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Ensure the state tensor is of type float\n",
    "            dist, value = self.agent(state_tensor)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "            \n",
    "            states.append(state_tensor)  # Append the state tensor\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "            values.append(value)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset()[0]\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            print(states.shape, self.agent)\n",
    "            dist, new_values = self.agent(states)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "            ratio = (new_log_probs - log_probs).exp()\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.c2, 1.0 + self.c2) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (values - returns).pow(2).mean()\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = self.compute_returns(rewards) - values.squeeze(1)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# ppo = PPO(\"InvertedPendulum-v4\")\n",
    "# ppo.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this sorta works: but missing a lot and do not have correct parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this one sorta, but doesnt use correct batch and stuyff\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, epochs=1000, lr=3e-4, gamma=0.99, c1=0.5, c2=0.01, batch_size=64):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            # Detach tensors before passing them to the update method\n",
    "            states_detached = states.detach()\n",
    "            actions_detached = actions.detach()\n",
    "            rewards_detached = rewards.detach()\n",
    "            log_probs_detached = log_probs.detach()\n",
    "            values_detached = values.detach()\n",
    "            print('Updating')\n",
    "            self.update(states_detached, actions_detached, rewards_detached, log_probs_detached, values_detached)\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Ensure the state tensor is of type float\n",
    "            dist, value = self.agent(state_tensor)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "            \n",
    "            states.append(state_tensor)  # Append the state tensor\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "            values.append(value)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset()[0]\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        self.optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        dist, new_values = self.agent(states)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        ratio = (new_log_probs - log_probs).exp()\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.c2, 1.0 + self.c2) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (values - returns).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy\n",
    "\n",
    "        loss.backward(retain_graph=True)  # Retain computational graph\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = self.compute_returns(rewards) - values.squeeze(1)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    def evaluate(self, env, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, _ = self.agent(state_tensor)\n",
    "                action = dist.mean  # Using mean action for evaluation\n",
    "                next_state, reward, done, _, _ = env.step(action.detach().numpy()[0])\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "ppo = PPO(\"InvertedPendulum-v4\")\n",
    "ppo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Create a new environment for evaluation\n",
    "# eval_env = gym.make('InvertedPendulum-v4', render_mode='human')\n",
    "\n",
    "# average_reward = ppo.evaluate(eval_env)\n",
    "# print(f\"Average reward over 10 episodes: {average_reward}\")\n",
    "\n",
    "# # Don't forget to close the evaluation environment when done\n",
    "# eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, horizon=2048, lr=3e-4, epochs=10, minibatch_size=64, gamma=0.99, gae_lambda=0.95):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def train(self, total_timesteps=1000):\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < total_timesteps:\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            # Update timesteps_so_far\n",
    "            print(states.shape)\n",
    "            timesteps_so_far += states.shape[0]\n",
    "\n",
    "            self.update(states, actions, rewards, log_probs, values)\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < self.horizon:\n",
    "            state = self.env.reset()[0]\n",
    "\n",
    "            for _ in range(self.minibatch_size):\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, value = self.agent(state_tensor)\n",
    "                action = dist.sample()\n",
    "\n",
    "                next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "                \n",
    "                states.append(state_tensor)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "                values.append(value)\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    state = self.env.reset()[0]\n",
    "\n",
    "            timesteps_so_far += self.minibatch_size\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        dist, new_values = self.agent(states)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        ratio = (new_log_probs - log_probs).exp()\n",
    "        print(ratio.shape, advantages.shape)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.gae_lambda, 1.0 + self.gae_lambda) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (values - returns).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = actor_loss + critic_loss - 0.01 * entropy\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = self.compute_returns(rewards) - values.squeeze(1)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# ppo = PPO(\"InvertedPendulum-v4\")\n",
    "# ppo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, horizon=2048, lr=3e-4, epochs=10, minibatch_size=64, gamma=0.99, gae_lambda=0.95):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def train(self, total_timesteps=1000000):\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < total_timesteps:\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            # Update timesteps_so_far\n",
    "            timesteps_so_far += states.shape[0]\n",
    "\n",
    "            self.update(states, actions, rewards, log_probs, values)\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < self.horizon:\n",
    "            state = self.env.reset()[0]\n",
    "\n",
    "            for _ in range(self.minibatch_size):\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, value = self.agent(state_tensor)\n",
    "                action = dist.sample()\n",
    "\n",
    "                next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "                \n",
    "                states.append(state_tensor)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "                values.append(value)\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    state = self.env.reset()[0]\n",
    "\n",
    "            timesteps_so_far += self.minibatch_size\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        dist, new_values = self.agent(states)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        # Expand ratio to match the shape of advantages\n",
    "        ratio = (new_log_probs - log_probs).exp().unsqueeze(1)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.gae_lambda, 1.0 + self.gae_lambda) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (values - returns).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = actor_loss + critic_loss - 0.01 * entropy\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = []\n",
    "        G = 0\n",
    "        for r, v in zip(reversed(rewards), reversed(values)):\n",
    "            G = r + self.gamma * G\n",
    "            advantages.insert(0, G - v.item())\n",
    "        advantages = torch.tensor(advantages)\n",
    "        \n",
    "        # Split advantages into minibatches\n",
    "        minibatch_advantages = []\n",
    "        minibatch_size = len(advantages) // self.minibatch_size\n",
    "        for i in range(self.minibatch_size):\n",
    "            start_idx = i * minibatch_size\n",
    "            end_idx = (i + 1) * minibatch_size\n",
    "            minibatch_advantages.append(advantages[start_idx:end_idx])\n",
    "        \n",
    "        # Stack minibatch advantages along a new dimension\n",
    "        minibatch_advantages = torch.stack(minibatch_advantages, dim=1)\n",
    "        \n",
    "        # Calculate mean and standard deviation batch-wise\n",
    "        advantages_mean = minibatch_advantages.mean(dim=1)\n",
    "        advantages_std = minibatch_advantages.std(dim=1) + 1e-8\n",
    "        \n",
    "        # Normalize advantages using batch-wise mean and std\n",
    "        normalized_advantages = (minibatch_advantages - advantages_mean.unsqueeze(1)) / advantages_std.unsqueeze(1)\n",
    "        \n",
    "        return normalized_advantages\n",
    "\n",
    "# ppo = PPO(\"InvertedPendulum-v4\")\n",
    "# ppo.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# joakim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.fc(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        return mu, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, actor_lr=1e-4, critic_lr=1e-3, gamma=0.99, epsilon=0.2, k_epochs=10, c1=0.5, c2=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs  # Number of optimization epochs per batch\n",
    "        self.c1 = c1  # Value function coefficient\n",
    "        self.c2 = c2  # Entropy coefficient\n",
    "        self.actor = Actor(input_dim, action_dim)\n",
    "        self.critic = Critic(input_dim)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        mu, std = self.actor(state)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * 0.95 * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        # Convert lists or arrays to tensors outside the loop\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.k_epochs):  # Multiple optimization epochs\n",
    "            # Optimization steps for both actor and critic\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "\n",
    "            mu, std = self.actor(states)\n",
    "            dist = torch.distributions.Normal(mu, std)\n",
    "            log_probs_new = dist.log_prob(actions).sum(dim=-1)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(log_probs_new - log_probs_old)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - self.c2 * entropy  # Include entropy bonus\n",
    "            critic_loss = self.mse_loss(self.critic(states), returns) * self.c1  # Apply value loss coefficient directly\n",
    "\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "    def train(self, env, episodes=250000, batch_size=2048):\n",
    "        all_rewards = []\n",
    "        step_counter = 0\n",
    "        \n",
    "        # Initialize empty lists to collect data until batch size is reached\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()[0]\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                value = self.critic(torch.from_numpy(state).float()).item()\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                # Accumulate data in the lists\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "                step_counter += 1\n",
    "\n",
    "                if step_counter % batch_size == 0:\n",
    "                    # Update policy with the accumulated data once the batch size is reached\n",
    "                    next_value = self.critic(torch.from_numpy(state).float()).item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    # Clear the accumulated data for the next batch\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {episode + 1}, Total Reward = {episode_rewards}, Total Steps = {step_counter}\")\n",
    "\n",
    "            # Check termination condition (if needed)\n",
    "            if step_counter >= 1000000:\n",
    "                return all_rewards\n",
    "\n",
    "        return all_rewards\n",
    "    \n",
    "    \n",
    "ppo = PPOAgent(4, 1)\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "rewards = ppo.train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.fc(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        return mu, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, k_epochs=10, minibatch_size=64,gae = 0.95):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs  # Number of optimization epochs per batch\n",
    "        \n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        mu, std = self.actor(state)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * 0.95 * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        # Convert lists or arrays to tensors outside the loop\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.k_epochs):  # Multiple optimization epochs\n",
    "            # Optimization steps for both actor and critic\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "\n",
    "            mu, std = self.actor(states)\n",
    "            dist = torch.distributions.Normal(mu, std)\n",
    "            log_probs_new = dist.log_prob(actions).sum(dim=-1)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(log_probs_new - log_probs_old)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - self.c2 * entropy  # Include entropy bonus\n",
    "            critic_loss = self.mse_loss(self.critic(states), returns) * self.c1  # Apply value loss coefficient directly\n",
    "\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "    def train(self, env, episodes=250000, batch_size=2048):\n",
    "        all_rewards = []\n",
    "        step_counter = 0\n",
    "        \n",
    "        # Initialize empty lists to collect data until batch size is reached\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()[0]\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                value = self.critic(torch.from_numpy(state).float()).item()\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                # Accumulate data in the lists\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "                \n",
    "\n",
    "                if len(states) % batch_size == 0:\n",
    "                    # Update policy with the accumulated data once the batch size is reached\n",
    "                    next_value = self.critic(torch.from_numpy(state).float()).item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    # Clear the accumulated data for the next batch\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {episode + 1}, Total Reward = {episode_rewards}\")\n",
    "            \n",
    "\n",
    "        return all_rewards\n",
    "    \n",
    "    \n",
    "ppo = PPOAgent(4, 1)\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "rewards = ppo.train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiments with the ActorCritic network architecture:\n",
      "Running experiment with lr=0.0001, epsilon=0.1, k_epochs=5, batch_size=32\n",
      "Episode 1, Total Reward = 4.0\n",
      "Episode 2, Total Reward = 4.0\n",
      "Episode 3, Total Reward = 14.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 8.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 10.0\n",
      "Episode 8, Total Reward = 11.0\n",
      "Episode 9, Total Reward = 7.0\n",
      "Episode 10, Total Reward = 8.0\n",
      "Episode 11, Total Reward = 5.0\n",
      "Episode 12, Total Reward = 11.0\n",
      "Episode 13, Total Reward = 12.0\n",
      "Episode 14, Total Reward = 15.0\n",
      "Episode 15, Total Reward = 9.0\n",
      "Episode 16, Total Reward = 13.0\n",
      "Episode 17, Total Reward = 8.0\n",
      "Episode 18, Total Reward = 10.0\n",
      "Episode 19, Total Reward = 5.0\n",
      "Episode 20, Total Reward = 7.0\n",
      "Episode 21, Total Reward = 5.0\n",
      "Episode 22, Total Reward = 6.0\n",
      "Episode 23, Total Reward = 7.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 13.0\n",
      "Episode 26, Total Reward = 4.0\n",
      "Episode 27, Total Reward = 6.0\n",
      "Episode 28, Total Reward = 10.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 10.0\n",
      "Episode 31, Total Reward = 10.0\n",
      "Episode 32, Total Reward = 8.0\n",
      "Episode 33, Total Reward = 5.0\n",
      "Episode 34, Total Reward = 7.0\n",
      "Episode 35, Total Reward = 6.0\n",
      "Episode 36, Total Reward = 15.0\n",
      "Episode 37, Total Reward = 4.0\n",
      "Episode 38, Total Reward = 7.0\n",
      "Episode 39, Total Reward = 13.0\n",
      "Episode 40, Total Reward = 10.0\n",
      "Episode 41, Total Reward = 7.0\n",
      "Episode 42, Total Reward = 4.0\n",
      "Episode 43, Total Reward = 12.0\n",
      "Episode 44, Total Reward = 10.0\n",
      "Episode 45, Total Reward = 5.0\n",
      "Episode 46, Total Reward = 4.0\n",
      "Episode 47, Total Reward = 8.0\n",
      "Episode 48, Total Reward = 14.0\n",
      "Episode 49, Total Reward = 9.0\n",
      "Episode 50, Total Reward = 16.0\n",
      "Episode 51, Total Reward = 5.0\n",
      "Episode 52, Total Reward = 14.0\n",
      "Episode 53, Total Reward = 4.0\n",
      "Episode 54, Total Reward = 18.0\n",
      "Episode 55, Total Reward = 15.0\n",
      "Episode 56, Total Reward = 6.0\n",
      "Episode 57, Total Reward = 8.0\n",
      "Episode 58, Total Reward = 4.0\n",
      "Episode 59, Total Reward = 8.0\n",
      "Episode 60, Total Reward = 7.0\n",
      "Episode 61, Total Reward = 10.0\n",
      "Episode 62, Total Reward = 4.0\n",
      "Episode 63, Total Reward = 12.0\n",
      "Episode 64, Total Reward = 9.0\n",
      "Episode 65, Total Reward = 10.0\n",
      "Episode 66, Total Reward = 8.0\n",
      "Episode 67, Total Reward = 9.0\n",
      "Episode 68, Total Reward = 9.0\n",
      "Episode 69, Total Reward = 17.0\n",
      "Episode 70, Total Reward = 4.0\n",
      "Episode 71, Total Reward = 22.0\n",
      "Episode 72, Total Reward = 10.0\n",
      "Episode 73, Total Reward = 21.0\n",
      "Episode 74, Total Reward = 18.0\n",
      "Episode 75, Total Reward = 4.0\n",
      "Episode 76, Total Reward = 8.0\n",
      "Episode 77, Total Reward = 17.0\n",
      "Episode 78, Total Reward = 15.0\n",
      "Episode 79, Total Reward = 32.0\n",
      "Episode 80, Total Reward = 28.0\n",
      "Episode 81, Total Reward = 9.0\n",
      "Episode 82, Total Reward = 6.0\n",
      "Episode 83, Total Reward = 5.0\n",
      "Episode 84, Total Reward = 8.0\n",
      "Episode 85, Total Reward = 13.0\n",
      "Episode 86, Total Reward = 16.0\n",
      "Episode 87, Total Reward = 9.0\n",
      "Episode 88, Total Reward = 18.0\n",
      "Episode 89, Total Reward = 6.0\n",
      "Episode 90, Total Reward = 16.0\n",
      "Episode 91, Total Reward = 16.0\n",
      "Episode 92, Total Reward = 9.0\n",
      "Episode 93, Total Reward = 4.0\n",
      "Episode 94, Total Reward = 8.0\n",
      "Episode 95, Total Reward = 23.0\n",
      "Episode 96, Total Reward = 36.0\n",
      "Episode 97, Total Reward = 11.0\n",
      "Episode 98, Total Reward = 23.0\n",
      "Episode 99, Total Reward = 13.0\n",
      "Episode 100, Total Reward = 14.0\n",
      "Running experiment with lr=0.0001, epsilon=0.1, k_epochs=5, batch_size=64\n",
      "Episode 1, Total Reward = 6.0\n",
      "Episode 2, Total Reward = 4.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 14.0\n",
      "Episode 6, Total Reward = 4.0\n",
      "Episode 7, Total Reward = 6.0\n",
      "Episode 8, Total Reward = 8.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 14.0\n",
      "Episode 12, Total Reward = 9.0\n",
      "Episode 13, Total Reward = 6.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 13.0\n",
      "Episode 16, Total Reward = 5.0\n",
      "Episode 17, Total Reward = 20.0\n",
      "Episode 18, Total Reward = 8.0\n",
      "Episode 19, Total Reward = 14.0\n",
      "Episode 20, Total Reward = 5.0\n",
      "Episode 21, Total Reward = 17.0\n",
      "Episode 22, Total Reward = 6.0\n",
      "Episode 23, Total Reward = 7.0\n",
      "Episode 24, Total Reward = 9.0\n",
      "Episode 25, Total Reward = 12.0\n",
      "Episode 26, Total Reward = 17.0\n",
      "Episode 27, Total Reward = 14.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 15.0\n",
      "Episode 30, Total Reward = 8.0\n",
      "Episode 31, Total Reward = 12.0\n",
      "Episode 32, Total Reward = 7.0\n",
      "Episode 33, Total Reward = 23.0\n",
      "Episode 34, Total Reward = 12.0\n",
      "Episode 35, Total Reward = 3.0\n",
      "Episode 36, Total Reward = 7.0\n",
      "Episode 37, Total Reward = 3.0\n",
      "Episode 38, Total Reward = 9.0\n",
      "Episode 39, Total Reward = 5.0\n",
      "Episode 40, Total Reward = 6.0\n",
      "Episode 41, Total Reward = 8.0\n",
      "Episode 42, Total Reward = 19.0\n",
      "Episode 43, Total Reward = 10.0\n",
      "Episode 44, Total Reward = 12.0\n",
      "Episode 45, Total Reward = 8.0\n",
      "Episode 46, Total Reward = 10.0\n",
      "Episode 47, Total Reward = 5.0\n",
      "Episode 48, Total Reward = 10.0\n",
      "Episode 49, Total Reward = 12.0\n",
      "Episode 50, Total Reward = 8.0\n",
      "Episode 51, Total Reward = 8.0\n",
      "Episode 52, Total Reward = 10.0\n",
      "Episode 53, Total Reward = 8.0\n",
      "Episode 54, Total Reward = 5.0\n",
      "Episode 55, Total Reward = 7.0\n",
      "Episode 56, Total Reward = 8.0\n",
      "Episode 57, Total Reward = 9.0\n",
      "Episode 58, Total Reward = 5.0\n",
      "Episode 59, Total Reward = 9.0\n",
      "Episode 60, Total Reward = 3.0\n",
      "Episode 61, Total Reward = 13.0\n",
      "Episode 62, Total Reward = 9.0\n",
      "Episode 63, Total Reward = 5.0\n",
      "Episode 64, Total Reward = 17.0\n",
      "Episode 65, Total Reward = 11.0\n",
      "Episode 66, Total Reward = 8.0\n",
      "Episode 67, Total Reward = 8.0\n",
      "Episode 68, Total Reward = 12.0\n",
      "Episode 69, Total Reward = 8.0\n",
      "Episode 70, Total Reward = 19.0\n",
      "Episode 71, Total Reward = 13.0\n",
      "Episode 72, Total Reward = 3.0\n",
      "Episode 73, Total Reward = 8.0\n",
      "Episode 74, Total Reward = 14.0\n",
      "Episode 75, Total Reward = 19.0\n",
      "Episode 76, Total Reward = 9.0\n",
      "Episode 77, Total Reward = 4.0\n",
      "Episode 78, Total Reward = 4.0\n",
      "Episode 79, Total Reward = 10.0\n",
      "Episode 80, Total Reward = 10.0\n",
      "Episode 81, Total Reward = 4.0\n",
      "Episode 82, Total Reward = 7.0\n",
      "Episode 83, Total Reward = 6.0\n",
      "Episode 84, Total Reward = 8.0\n",
      "Episode 85, Total Reward = 15.0\n",
      "Episode 86, Total Reward = 4.0\n",
      "Episode 87, Total Reward = 23.0\n",
      "Episode 88, Total Reward = 11.0\n",
      "Episode 89, Total Reward = 4.0\n",
      "Episode 90, Total Reward = 7.0\n",
      "Episode 91, Total Reward = 12.0\n",
      "Episode 92, Total Reward = 7.0\n",
      "Episode 93, Total Reward = 26.0\n",
      "Episode 94, Total Reward = 4.0\n",
      "Episode 95, Total Reward = 6.0\n",
      "Episode 96, Total Reward = 38.0\n",
      "Episode 97, Total Reward = 6.0\n",
      "Episode 98, Total Reward = 21.0\n",
      "Episode 99, Total Reward = 18.0\n",
      "Episode 100, Total Reward = 16.0\n",
      "Running experiment with lr=0.0001, epsilon=0.1, k_epochs=5, batch_size=128\n",
      "Episode 1, Total Reward = 10.0\n",
      "Episode 2, Total Reward = 11.0\n",
      "Episode 3, Total Reward = 15.0\n",
      "Episode 4, Total Reward = 31.0\n",
      "Episode 5, Total Reward = 23.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 15.0\n",
      "Episode 9, Total Reward = 18.0\n",
      "Episode 10, Total Reward = 10.0\n",
      "Episode 11, Total Reward = 6.0\n",
      "Episode 12, Total Reward = 8.0\n",
      "Episode 13, Total Reward = 9.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 8.0\n",
      "Episode 16, Total Reward = 13.0\n",
      "Episode 17, Total Reward = 8.0\n",
      "Episode 18, Total Reward = 9.0\n",
      "Episode 19, Total Reward = 14.0\n",
      "Episode 20, Total Reward = 12.0\n",
      "Episode 21, Total Reward = 4.0\n",
      "Episode 22, Total Reward = 5.0\n",
      "Episode 23, Total Reward = 21.0\n",
      "Episode 24, Total Reward = 7.0\n",
      "Episode 25, Total Reward = 7.0\n",
      "Episode 26, Total Reward = 4.0\n",
      "Episode 27, Total Reward = 8.0\n",
      "Episode 28, Total Reward = 4.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 10.0\n",
      "Episode 31, Total Reward = 5.0\n",
      "Episode 32, Total Reward = 7.0\n",
      "Episode 33, Total Reward = 6.0\n",
      "Episode 34, Total Reward = 10.0\n",
      "Episode 35, Total Reward = 8.0\n",
      "Episode 36, Total Reward = 6.0\n",
      "Episode 37, Total Reward = 5.0\n",
      "Episode 38, Total Reward = 5.0\n",
      "Episode 39, Total Reward = 6.0\n",
      "Episode 40, Total Reward = 16.0\n",
      "Episode 41, Total Reward = 7.0\n",
      "Episode 42, Total Reward = 16.0\n",
      "Episode 43, Total Reward = 9.0\n",
      "Episode 44, Total Reward = 3.0\n",
      "Episode 45, Total Reward = 11.0\n",
      "Episode 46, Total Reward = 12.0\n",
      "Episode 47, Total Reward = 6.0\n",
      "Episode 48, Total Reward = 3.0\n",
      "Episode 49, Total Reward = 15.0\n",
      "Episode 50, Total Reward = 7.0\n",
      "Episode 51, Total Reward = 28.0\n",
      "Episode 52, Total Reward = 10.0\n",
      "Episode 53, Total Reward = 9.0\n",
      "Episode 54, Total Reward = 6.0\n",
      "Episode 55, Total Reward = 10.0\n",
      "Episode 56, Total Reward = 7.0\n",
      "Episode 57, Total Reward = 7.0\n",
      "Episode 58, Total Reward = 5.0\n",
      "Episode 59, Total Reward = 8.0\n",
      "Episode 60, Total Reward = 10.0\n",
      "Episode 61, Total Reward = 11.0\n",
      "Episode 62, Total Reward = 6.0\n",
      "Episode 63, Total Reward = 7.0\n",
      "Episode 64, Total Reward = 6.0\n",
      "Episode 65, Total Reward = 9.0\n",
      "Episode 66, Total Reward = 10.0\n",
      "Episode 67, Total Reward = 5.0\n",
      "Episode 68, Total Reward = 9.0\n",
      "Episode 69, Total Reward = 12.0\n",
      "Episode 70, Total Reward = 25.0\n",
      "Episode 71, Total Reward = 6.0\n",
      "Episode 72, Total Reward = 5.0\n",
      "Episode 73, Total Reward = 18.0\n",
      "Episode 74, Total Reward = 5.0\n",
      "Episode 75, Total Reward = 9.0\n",
      "Episode 76, Total Reward = 10.0\n",
      "Episode 77, Total Reward = 17.0\n",
      "Episode 78, Total Reward = 7.0\n",
      "Episode 79, Total Reward = 9.0\n",
      "Episode 80, Total Reward = 24.0\n",
      "Episode 81, Total Reward = 12.0\n",
      "Episode 82, Total Reward = 9.0\n",
      "Episode 83, Total Reward = 11.0\n",
      "Episode 84, Total Reward = 7.0\n",
      "Episode 85, Total Reward = 9.0\n",
      "Episode 86, Total Reward = 15.0\n",
      "Episode 87, Total Reward = 13.0\n",
      "Episode 88, Total Reward = 8.0\n",
      "Episode 89, Total Reward = 7.0\n",
      "Episode 90, Total Reward = 14.0\n",
      "Episode 91, Total Reward = 7.0\n",
      "Episode 92, Total Reward = 6.0\n",
      "Episode 93, Total Reward = 7.0\n",
      "Episode 94, Total Reward = 19.0\n",
      "Episode 95, Total Reward = 5.0\n",
      "Episode 96, Total Reward = 6.0\n",
      "Episode 97, Total Reward = 5.0\n",
      "Episode 98, Total Reward = 5.0\n",
      "Episode 99, Total Reward = 6.0\n",
      "Episode 100, Total Reward = 12.0\n",
      "Running experiment with lr=0.0001, epsilon=0.1, k_epochs=10, batch_size=32\n",
      "Episode 1, Total Reward = 13.0\n",
      "Episode 2, Total Reward = 7.0\n",
      "Episode 3, Total Reward = 15.0\n",
      "Episode 4, Total Reward = 12.0\n",
      "Episode 5, Total Reward = 7.0\n",
      "Episode 6, Total Reward = 8.0\n",
      "Episode 7, Total Reward = 4.0\n",
      "Episode 8, Total Reward = 9.0\n",
      "Episode 9, Total Reward = 8.0\n",
      "Episode 10, Total Reward = 15.0\n",
      "Episode 11, Total Reward = 6.0\n",
      "Episode 12, Total Reward = 6.0\n",
      "Episode 13, Total Reward = 3.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 4.0\n",
      "Episode 17, Total Reward = 6.0\n",
      "Episode 18, Total Reward = 8.0\n",
      "Episode 19, Total Reward = 6.0\n",
      "Episode 20, Total Reward = 9.0\n",
      "Episode 21, Total Reward = 8.0\n",
      "Episode 22, Total Reward = 4.0\n",
      "Episode 23, Total Reward = 8.0\n",
      "Episode 24, Total Reward = 18.0\n",
      "Episode 25, Total Reward = 7.0\n",
      "Episode 26, Total Reward = 17.0\n",
      "Episode 27, Total Reward = 22.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 11.0\n",
      "Episode 30, Total Reward = 18.0\n",
      "Episode 31, Total Reward = 6.0\n",
      "Episode 32, Total Reward = 21.0\n",
      "Episode 33, Total Reward = 9.0\n",
      "Episode 34, Total Reward = 19.0\n",
      "Episode 35, Total Reward = 4.0\n",
      "Episode 36, Total Reward = 13.0\n",
      "Episode 37, Total Reward = 14.0\n",
      "Episode 38, Total Reward = 5.0\n",
      "Episode 39, Total Reward = 10.0\n",
      "Episode 40, Total Reward = 12.0\n",
      "Episode 41, Total Reward = 7.0\n",
      "Episode 42, Total Reward = 8.0\n",
      "Episode 43, Total Reward = 8.0\n",
      "Episode 44, Total Reward = 15.0\n",
      "Episode 45, Total Reward = 6.0\n",
      "Episode 46, Total Reward = 19.0\n",
      "Episode 47, Total Reward = 23.0\n",
      "Episode 48, Total Reward = 13.0\n",
      "Episode 49, Total Reward = 4.0\n",
      "Episode 50, Total Reward = 23.0\n",
      "Episode 51, Total Reward = 15.0\n",
      "Episode 52, Total Reward = 17.0\n",
      "Episode 53, Total Reward = 13.0\n",
      "Episode 54, Total Reward = 16.0\n",
      "Episode 55, Total Reward = 15.0\n",
      "Episode 56, Total Reward = 19.0\n",
      "Episode 57, Total Reward = 23.0\n",
      "Episode 58, Total Reward = 17.0\n",
      "Episode 59, Total Reward = 15.0\n",
      "Episode 60, Total Reward = 8.0\n",
      "Episode 61, Total Reward = 21.0\n",
      "Episode 62, Total Reward = 51.0\n",
      "Episode 63, Total Reward = 8.0\n",
      "Episode 64, Total Reward = 28.0\n",
      "Episode 65, Total Reward = 10.0\n",
      "Episode 66, Total Reward = 9.0\n",
      "Episode 67, Total Reward = 12.0\n",
      "Episode 68, Total Reward = 12.0\n",
      "Episode 69, Total Reward = 27.0\n",
      "Episode 70, Total Reward = 24.0\n",
      "Episode 71, Total Reward = 8.0\n",
      "Episode 72, Total Reward = 13.0\n",
      "Episode 73, Total Reward = 22.0\n",
      "Episode 74, Total Reward = 13.0\n",
      "Episode 75, Total Reward = 9.0\n",
      "Episode 76, Total Reward = 45.0\n",
      "Episode 77, Total Reward = 26.0\n",
      "Episode 78, Total Reward = 49.0\n",
      "Episode 79, Total Reward = 27.0\n",
      "Episode 80, Total Reward = 5.0\n",
      "Episode 81, Total Reward = 26.0\n",
      "Episode 82, Total Reward = 21.0\n",
      "Episode 83, Total Reward = 11.0\n",
      "Episode 84, Total Reward = 16.0\n",
      "Episode 85, Total Reward = 21.0\n",
      "Episode 86, Total Reward = 27.0\n",
      "Episode 87, Total Reward = 20.0\n",
      "Episode 88, Total Reward = 23.0\n",
      "Episode 89, Total Reward = 15.0\n",
      "Episode 90, Total Reward = 25.0\n",
      "Episode 91, Total Reward = 11.0\n",
      "Episode 92, Total Reward = 37.0\n",
      "Episode 93, Total Reward = 28.0\n",
      "Episode 94, Total Reward = 38.0\n",
      "Episode 95, Total Reward = 18.0\n",
      "Episode 96, Total Reward = 31.0\n",
      "Episode 97, Total Reward = 35.0\n",
      "Episode 98, Total Reward = 18.0\n",
      "Episode 99, Total Reward = 33.0\n",
      "Episode 100, Total Reward = 21.0\n",
      "Running experiment with lr=0.0001, epsilon=0.1, k_epochs=10, batch_size=64\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 14.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 4.0\n",
      "Episode 7, Total Reward = 13.0\n",
      "Episode 8, Total Reward = 3.0\n",
      "Episode 9, Total Reward = 7.0\n",
      "Episode 10, Total Reward = 6.0\n",
      "Episode 11, Total Reward = 4.0\n",
      "Episode 12, Total Reward = 8.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 8.0\n",
      "Episode 15, Total Reward = 5.0\n",
      "Episode 16, Total Reward = 7.0\n",
      "Episode 17, Total Reward = 6.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 14.0\n",
      "Episode 20, Total Reward = 4.0\n",
      "Episode 21, Total Reward = 6.0\n",
      "Episode 22, Total Reward = 7.0\n",
      "Episode 23, Total Reward = 4.0\n",
      "Episode 24, Total Reward = 12.0\n",
      "Episode 25, Total Reward = 5.0\n",
      "Episode 26, Total Reward = 9.0\n",
      "Episode 27, Total Reward = 6.0\n",
      "Episode 28, Total Reward = 13.0\n",
      "Episode 29, Total Reward = 10.0\n",
      "Episode 30, Total Reward = 8.0\n",
      "Episode 31, Total Reward = 23.0\n",
      "Episode 32, Total Reward = 10.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 10.0\n",
      "Episode 35, Total Reward = 7.0\n",
      "Episode 36, Total Reward = 7.0\n",
      "Episode 37, Total Reward = 4.0\n",
      "Episode 38, Total Reward = 4.0\n",
      "Episode 39, Total Reward = 5.0\n",
      "Episode 40, Total Reward = 4.0\n",
      "Episode 41, Total Reward = 3.0\n",
      "Episode 42, Total Reward = 7.0\n",
      "Episode 43, Total Reward = 8.0\n",
      "Episode 44, Total Reward = 4.0\n",
      "Episode 45, Total Reward = 15.0\n",
      "Episode 46, Total Reward = 8.0\n",
      "Episode 47, Total Reward = 15.0\n",
      "Episode 48, Total Reward = 5.0\n",
      "Episode 49, Total Reward = 4.0\n",
      "Episode 50, Total Reward = 8.0\n",
      "Episode 51, Total Reward = 22.0\n",
      "Episode 52, Total Reward = 16.0\n",
      "Episode 53, Total Reward = 4.0\n",
      "Episode 54, Total Reward = 3.0\n",
      "Episode 55, Total Reward = 4.0\n",
      "Episode 56, Total Reward = 11.0\n",
      "Episode 57, Total Reward = 8.0\n",
      "Episode 58, Total Reward = 7.0\n",
      "Episode 59, Total Reward = 5.0\n",
      "Episode 60, Total Reward = 5.0\n",
      "Episode 61, Total Reward = 9.0\n",
      "Episode 62, Total Reward = 15.0\n",
      "Episode 63, Total Reward = 8.0\n",
      "Episode 64, Total Reward = 7.0\n",
      "Episode 65, Total Reward = 10.0\n",
      "Episode 66, Total Reward = 10.0\n",
      "Episode 67, Total Reward = 6.0\n",
      "Episode 68, Total Reward = 7.0\n",
      "Episode 69, Total Reward = 15.0\n",
      "Episode 70, Total Reward = 12.0\n",
      "Episode 71, Total Reward = 6.0\n",
      "Episode 72, Total Reward = 10.0\n",
      "Episode 73, Total Reward = 8.0\n",
      "Episode 74, Total Reward = 6.0\n",
      "Episode 75, Total Reward = 18.0\n",
      "Episode 76, Total Reward = 11.0\n",
      "Episode 77, Total Reward = 11.0\n",
      "Episode 78, Total Reward = 7.0\n",
      "Episode 79, Total Reward = 5.0\n",
      "Episode 80, Total Reward = 24.0\n",
      "Episode 81, Total Reward = 9.0\n",
      "Episode 82, Total Reward = 18.0\n",
      "Episode 83, Total Reward = 5.0\n",
      "Episode 84, Total Reward = 20.0\n",
      "Episode 85, Total Reward = 31.0\n",
      "Episode 86, Total Reward = 19.0\n",
      "Episode 87, Total Reward = 4.0\n",
      "Episode 88, Total Reward = 7.0\n",
      "Episode 89, Total Reward = 20.0\n",
      "Episode 90, Total Reward = 9.0\n",
      "Episode 91, Total Reward = 5.0\n",
      "Episode 92, Total Reward = 25.0\n",
      "Episode 93, Total Reward = 12.0\n",
      "Episode 94, Total Reward = 13.0\n",
      "Episode 95, Total Reward = 9.0\n",
      "Episode 96, Total Reward = 9.0\n",
      "Episode 97, Total Reward = 8.0\n",
      "Episode 98, Total Reward = 23.0\n",
      "Episode 99, Total Reward = 32.0\n",
      "Episode 100, Total Reward = 11.0\n",
      "Running experiment with lr=0.0001, epsilon=0.1, k_epochs=10, batch_size=128\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 25.0\n",
      "Episode 3, Total Reward = 13.0\n",
      "Episode 4, Total Reward = 16.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 9.0\n",
      "Episode 7, Total Reward = 8.0\n",
      "Episode 8, Total Reward = 8.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 3.0\n",
      "Episode 11, Total Reward = 6.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 11.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 33.0\n",
      "Episode 16, Total Reward = 7.0\n",
      "Episode 17, Total Reward = 13.0\n",
      "Episode 18, Total Reward = 6.0\n",
      "Episode 19, Total Reward = 5.0\n",
      "Episode 20, Total Reward = 8.0\n",
      "Episode 21, Total Reward = 6.0\n",
      "Episode 22, Total Reward = 15.0\n",
      "Episode 23, Total Reward = 13.0\n",
      "Episode 24, Total Reward = 15.0\n",
      "Episode 25, Total Reward = 4.0\n",
      "Episode 26, Total Reward = 10.0\n",
      "Episode 27, Total Reward = 5.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 14.0\n",
      "Episode 31, Total Reward = 5.0\n",
      "Episode 32, Total Reward = 12.0\n",
      "Episode 33, Total Reward = 13.0\n",
      "Episode 34, Total Reward = 5.0\n",
      "Episode 35, Total Reward = 38.0\n",
      "Episode 36, Total Reward = 8.0\n",
      "Episode 37, Total Reward = 11.0\n",
      "Episode 38, Total Reward = 10.0\n",
      "Episode 39, Total Reward = 13.0\n",
      "Episode 40, Total Reward = 9.0\n",
      "Episode 41, Total Reward = 8.0\n",
      "Episode 42, Total Reward = 13.0\n",
      "Episode 43, Total Reward = 18.0\n",
      "Episode 44, Total Reward = 6.0\n",
      "Episode 45, Total Reward = 8.0\n",
      "Episode 46, Total Reward = 9.0\n",
      "Episode 47, Total Reward = 6.0\n",
      "Episode 48, Total Reward = 9.0\n",
      "Episode 49, Total Reward = 11.0\n",
      "Episode 50, Total Reward = 7.0\n",
      "Episode 51, Total Reward = 10.0\n",
      "Episode 52, Total Reward = 7.0\n",
      "Episode 53, Total Reward = 9.0\n",
      "Episode 54, Total Reward = 7.0\n",
      "Episode 55, Total Reward = 14.0\n",
      "Episode 56, Total Reward = 9.0\n",
      "Episode 57, Total Reward = 6.0\n",
      "Episode 58, Total Reward = 8.0\n",
      "Episode 59, Total Reward = 16.0\n",
      "Episode 60, Total Reward = 26.0\n",
      "Episode 61, Total Reward = 9.0\n",
      "Episode 62, Total Reward = 9.0\n",
      "Episode 63, Total Reward = 12.0\n",
      "Episode 64, Total Reward = 9.0\n",
      "Episode 65, Total Reward = 7.0\n",
      "Episode 66, Total Reward = 5.0\n",
      "Episode 67, Total Reward = 16.0\n",
      "Episode 68, Total Reward = 7.0\n",
      "Episode 69, Total Reward = 7.0\n",
      "Episode 70, Total Reward = 12.0\n",
      "Episode 71, Total Reward = 15.0\n",
      "Episode 72, Total Reward = 36.0\n",
      "Episode 73, Total Reward = 40.0\n",
      "Episode 74, Total Reward = 5.0\n",
      "Episode 75, Total Reward = 13.0\n",
      "Episode 76, Total Reward = 20.0\n",
      "Episode 77, Total Reward = 10.0\n",
      "Episode 78, Total Reward = 24.0\n",
      "Episode 79, Total Reward = 12.0\n",
      "Episode 80, Total Reward = 22.0\n",
      "Episode 81, Total Reward = 9.0\n",
      "Episode 82, Total Reward = 21.0\n",
      "Episode 83, Total Reward = 6.0\n",
      "Episode 84, Total Reward = 30.0\n",
      "Episode 85, Total Reward = 8.0\n",
      "Episode 86, Total Reward = 21.0\n",
      "Episode 87, Total Reward = 16.0\n",
      "Episode 88, Total Reward = 29.0\n",
      "Episode 89, Total Reward = 30.0\n",
      "Episode 90, Total Reward = 16.0\n",
      "Episode 91, Total Reward = 16.0\n",
      "Episode 92, Total Reward = 13.0\n",
      "Episode 93, Total Reward = 14.0\n",
      "Episode 94, Total Reward = 18.0\n",
      "Episode 95, Total Reward = 25.0\n",
      "Episode 96, Total Reward = 5.0\n",
      "Episode 97, Total Reward = 7.0\n",
      "Episode 98, Total Reward = 10.0\n",
      "Episode 99, Total Reward = 8.0\n",
      "Episode 100, Total Reward = 34.0\n",
      "Running experiment with lr=0.0001, epsilon=0.1, k_epochs=20, batch_size=32\n",
      "Episode 1, Total Reward = 3.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 5.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 8.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 5.0\n",
      "Episode 9, Total Reward = 4.0\n",
      "Episode 10, Total Reward = 6.0\n",
      "Episode 11, Total Reward = 4.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 8.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 5.0\n",
      "Episode 16, Total Reward = 8.0\n",
      "Episode 17, Total Reward = 4.0\n",
      "Episode 18, Total Reward = 8.0\n",
      "Episode 19, Total Reward = 5.0\n",
      "Episode 20, Total Reward = 8.0\n",
      "Episode 21, Total Reward = 19.0\n",
      "Episode 22, Total Reward = 8.0\n",
      "Episode 23, Total Reward = 8.0\n",
      "Episode 24, Total Reward = 7.0\n",
      "Episode 25, Total Reward = 12.0\n",
      "Episode 26, Total Reward = 8.0\n",
      "Episode 27, Total Reward = 7.0\n",
      "Episode 28, Total Reward = 7.0\n",
      "Episode 29, Total Reward = 12.0\n",
      "Episode 30, Total Reward = 6.0\n",
      "Episode 31, Total Reward = 7.0\n",
      "Episode 32, Total Reward = 6.0\n",
      "Episode 33, Total Reward = 10.0\n",
      "Episode 34, Total Reward = 5.0\n",
      "Episode 35, Total Reward = 18.0\n",
      "Episode 36, Total Reward = 10.0\n",
      "Episode 37, Total Reward = 8.0\n",
      "Episode 38, Total Reward = 15.0\n",
      "Episode 39, Total Reward = 6.0\n",
      "Episode 40, Total Reward = 30.0\n",
      "Episode 41, Total Reward = 13.0\n",
      "Episode 42, Total Reward = 7.0\n",
      "Episode 43, Total Reward = 7.0\n",
      "Episode 44, Total Reward = 16.0\n",
      "Episode 45, Total Reward = 10.0\n",
      "Episode 46, Total Reward = 5.0\n",
      "Episode 47, Total Reward = 11.0\n",
      "Episode 48, Total Reward = 11.0\n",
      "Episode 49, Total Reward = 4.0\n",
      "Episode 50, Total Reward = 11.0\n",
      "Episode 51, Total Reward = 8.0\n",
      "Episode 52, Total Reward = 14.0\n",
      "Episode 53, Total Reward = 8.0\n",
      "Episode 54, Total Reward = 8.0\n",
      "Episode 55, Total Reward = 10.0\n",
      "Episode 56, Total Reward = 5.0\n",
      "Episode 57, Total Reward = 7.0\n",
      "Episode 58, Total Reward = 10.0\n",
      "Episode 59, Total Reward = 15.0\n",
      "Episode 60, Total Reward = 8.0\n",
      "Episode 61, Total Reward = 4.0\n",
      "Episode 62, Total Reward = 7.0\n",
      "Episode 63, Total Reward = 21.0\n",
      "Episode 64, Total Reward = 10.0\n",
      "Episode 65, Total Reward = 16.0\n",
      "Episode 66, Total Reward = 19.0\n",
      "Episode 67, Total Reward = 10.0\n",
      "Episode 68, Total Reward = 23.0\n",
      "Episode 69, Total Reward = 14.0\n",
      "Episode 70, Total Reward = 7.0\n",
      "Episode 71, Total Reward = 12.0\n",
      "Episode 72, Total Reward = 6.0\n",
      "Episode 73, Total Reward = 10.0\n",
      "Episode 74, Total Reward = 9.0\n",
      "Episode 75, Total Reward = 10.0\n",
      "Episode 76, Total Reward = 11.0\n",
      "Episode 77, Total Reward = 39.0\n",
      "Episode 78, Total Reward = 25.0\n",
      "Episode 79, Total Reward = 46.0\n",
      "Episode 80, Total Reward = 6.0\n",
      "Episode 81, Total Reward = 13.0\n",
      "Episode 82, Total Reward = 16.0\n",
      "Episode 83, Total Reward = 8.0\n",
      "Episode 84, Total Reward = 35.0\n",
      "Episode 85, Total Reward = 12.0\n",
      "Episode 86, Total Reward = 21.0\n",
      "Episode 87, Total Reward = 11.0\n",
      "Episode 88, Total Reward = 18.0\n",
      "Episode 89, Total Reward = 8.0\n",
      "Episode 90, Total Reward = 28.0\n",
      "Episode 91, Total Reward = 11.0\n",
      "Episode 92, Total Reward = 9.0\n",
      "Episode 93, Total Reward = 15.0\n",
      "Episode 94, Total Reward = 10.0\n",
      "Episode 95, Total Reward = 10.0\n",
      "Episode 96, Total Reward = 12.0\n",
      "Episode 97, Total Reward = 83.0\n",
      "Episode 98, Total Reward = 46.0\n",
      "Episode 99, Total Reward = 55.0\n",
      "Episode 100, Total Reward = 21.0\n",
      "Running experiment with lr=0.0001, epsilon=0.1, k_epochs=20, batch_size=64\n",
      "Episode 1, Total Reward = 6.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 10.0\n",
      "Episode 6, Total Reward = 5.0\n",
      "Episode 7, Total Reward = 20.0\n",
      "Episode 8, Total Reward = 5.0\n",
      "Episode 9, Total Reward = 12.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 5.0\n",
      "Episode 12, Total Reward = 15.0\n",
      "Episode 13, Total Reward = 6.0\n",
      "Episode 14, Total Reward = 8.0\n",
      "Episode 15, Total Reward = 8.0\n",
      "Episode 16, Total Reward = 20.0\n",
      "Episode 17, Total Reward = 17.0\n",
      "Episode 18, Total Reward = 8.0\n",
      "Episode 19, Total Reward = 9.0\n",
      "Episode 20, Total Reward = 11.0\n",
      "Episode 21, Total Reward = 12.0\n",
      "Episode 22, Total Reward = 10.0\n",
      "Episode 23, Total Reward = 54.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 4.0\n",
      "Episode 26, Total Reward = 12.0\n",
      "Episode 27, Total Reward = 10.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 7.0\n",
      "Episode 30, Total Reward = 8.0\n",
      "Episode 31, Total Reward = 11.0\n",
      "Episode 32, Total Reward = 15.0\n",
      "Episode 33, Total Reward = 5.0\n",
      "Episode 34, Total Reward = 19.0\n",
      "Episode 35, Total Reward = 4.0\n",
      "Episode 36, Total Reward = 3.0\n",
      "Episode 37, Total Reward = 12.0\n",
      "Episode 38, Total Reward = 6.0\n",
      "Episode 39, Total Reward = 6.0\n",
      "Episode 40, Total Reward = 51.0\n",
      "Episode 41, Total Reward = 19.0\n",
      "Episode 42, Total Reward = 23.0\n",
      "Episode 43, Total Reward = 13.0\n",
      "Episode 44, Total Reward = 9.0\n",
      "Episode 45, Total Reward = 26.0\n",
      "Episode 46, Total Reward = 38.0\n",
      "Episode 47, Total Reward = 10.0\n",
      "Episode 48, Total Reward = 22.0\n",
      "Episode 49, Total Reward = 7.0\n",
      "Episode 50, Total Reward = 18.0\n",
      "Episode 51, Total Reward = 18.0\n",
      "Episode 52, Total Reward = 12.0\n",
      "Episode 53, Total Reward = 15.0\n",
      "Episode 54, Total Reward = 18.0\n",
      "Episode 55, Total Reward = 16.0\n",
      "Episode 56, Total Reward = 9.0\n",
      "Episode 57, Total Reward = 47.0\n",
      "Episode 58, Total Reward = 25.0\n",
      "Episode 59, Total Reward = 9.0\n",
      "Episode 60, Total Reward = 7.0\n",
      "Episode 61, Total Reward = 25.0\n",
      "Episode 62, Total Reward = 12.0\n",
      "Episode 63, Total Reward = 9.0\n",
      "Episode 64, Total Reward = 33.0\n",
      "Episode 65, Total Reward = 28.0\n",
      "Episode 66, Total Reward = 35.0\n",
      "Episode 67, Total Reward = 31.0\n",
      "Episode 68, Total Reward = 10.0\n",
      "Episode 69, Total Reward = 23.0\n",
      "Episode 70, Total Reward = 18.0\n",
      "Episode 71, Total Reward = 16.0\n",
      "Episode 72, Total Reward = 20.0\n",
      "Episode 73, Total Reward = 33.0\n",
      "Episode 74, Total Reward = 39.0\n",
      "Episode 75, Total Reward = 19.0\n",
      "Episode 76, Total Reward = 35.0\n",
      "Episode 77, Total Reward = 25.0\n",
      "Episode 78, Total Reward = 14.0\n",
      "Episode 79, Total Reward = 10.0\n",
      "Episode 80, Total Reward = 7.0\n",
      "Episode 81, Total Reward = 30.0\n",
      "Episode 82, Total Reward = 39.0\n",
      "Episode 83, Total Reward = 30.0\n",
      "Episode 84, Total Reward = 37.0\n",
      "Episode 85, Total Reward = 23.0\n",
      "Episode 86, Total Reward = 73.0\n",
      "Episode 87, Total Reward = 15.0\n",
      "Episode 88, Total Reward = 97.0\n",
      "Episode 89, Total Reward = 37.0\n",
      "Episode 90, Total Reward = 57.0\n",
      "Episode 91, Total Reward = 27.0\n",
      "Episode 92, Total Reward = 61.0\n",
      "Episode 93, Total Reward = 15.0\n",
      "Episode 94, Total Reward = 86.0\n",
      "Episode 95, Total Reward = 70.0\n",
      "Episode 96, Total Reward = 6.0\n",
      "Episode 97, Total Reward = 73.0\n",
      "Episode 98, Total Reward = 10.0\n",
      "Episode 99, Total Reward = 17.0\n",
      "Episode 100, Total Reward = 41.0\n",
      "Running experiment with lr=0.0001, epsilon=0.1, k_epochs=20, batch_size=128\n",
      "Episode 1, Total Reward = 14.0\n",
      "Episode 2, Total Reward = 8.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 7.0\n",
      "Episode 5, Total Reward = 9.0\n",
      "Episode 6, Total Reward = 9.0\n",
      "Episode 7, Total Reward = 6.0\n",
      "Episode 8, Total Reward = 6.0\n",
      "Episode 9, Total Reward = 3.0\n",
      "Episode 10, Total Reward = 13.0\n",
      "Episode 11, Total Reward = 7.0\n",
      "Episode 12, Total Reward = 9.0\n",
      "Episode 13, Total Reward = 4.0\n",
      "Episode 14, Total Reward = 9.0\n",
      "Episode 15, Total Reward = 9.0\n",
      "Episode 16, Total Reward = 10.0\n",
      "Episode 17, Total Reward = 11.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 6.0\n",
      "Episode 20, Total Reward = 9.0\n",
      "Episode 21, Total Reward = 8.0\n",
      "Episode 22, Total Reward = 13.0\n",
      "Episode 23, Total Reward = 15.0\n",
      "Episode 24, Total Reward = 25.0\n",
      "Episode 25, Total Reward = 3.0\n",
      "Episode 26, Total Reward = 7.0\n",
      "Episode 27, Total Reward = 4.0\n",
      "Episode 28, Total Reward = 7.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 8.0\n",
      "Episode 31, Total Reward = 14.0\n",
      "Episode 32, Total Reward = 9.0\n",
      "Episode 33, Total Reward = 6.0\n",
      "Episode 34, Total Reward = 14.0\n",
      "Episode 35, Total Reward = 13.0\n",
      "Episode 36, Total Reward = 52.0\n",
      "Episode 37, Total Reward = 5.0\n",
      "Episode 38, Total Reward = 6.0\n",
      "Episode 39, Total Reward = 4.0\n",
      "Episode 40, Total Reward = 11.0\n",
      "Episode 41, Total Reward = 15.0\n",
      "Episode 42, Total Reward = 5.0\n",
      "Episode 43, Total Reward = 9.0\n",
      "Episode 44, Total Reward = 17.0\n",
      "Episode 45, Total Reward = 41.0\n",
      "Episode 46, Total Reward = 21.0\n",
      "Episode 47, Total Reward = 7.0\n",
      "Episode 48, Total Reward = 5.0\n",
      "Episode 49, Total Reward = 8.0\n",
      "Episode 50, Total Reward = 5.0\n",
      "Episode 51, Total Reward = 10.0\n",
      "Episode 52, Total Reward = 45.0\n",
      "Episode 53, Total Reward = 11.0\n",
      "Episode 54, Total Reward = 17.0\n",
      "Episode 55, Total Reward = 6.0\n",
      "Episode 56, Total Reward = 8.0\n",
      "Episode 57, Total Reward = 17.0\n",
      "Episode 58, Total Reward = 13.0\n",
      "Episode 59, Total Reward = 17.0\n",
      "Episode 60, Total Reward = 20.0\n",
      "Episode 61, Total Reward = 14.0\n",
      "Episode 62, Total Reward = 6.0\n",
      "Episode 63, Total Reward = 11.0\n",
      "Episode 64, Total Reward = 59.0\n",
      "Episode 65, Total Reward = 16.0\n",
      "Episode 66, Total Reward = 42.0\n",
      "Episode 67, Total Reward = 15.0\n",
      "Episode 68, Total Reward = 9.0\n",
      "Episode 69, Total Reward = 42.0\n",
      "Episode 70, Total Reward = 9.0\n",
      "Episode 71, Total Reward = 39.0\n",
      "Episode 72, Total Reward = 30.0\n",
      "Episode 73, Total Reward = 41.0\n",
      "Episode 74, Total Reward = 20.0\n",
      "Episode 75, Total Reward = 57.0\n",
      "Episode 76, Total Reward = 24.0\n",
      "Episode 77, Total Reward = 38.0\n",
      "Episode 78, Total Reward = 19.0\n",
      "Episode 79, Total Reward = 29.0\n",
      "Episode 80, Total Reward = 18.0\n",
      "Episode 81, Total Reward = 15.0\n",
      "Episode 82, Total Reward = 23.0\n",
      "Episode 83, Total Reward = 9.0\n",
      "Episode 84, Total Reward = 9.0\n",
      "Episode 85, Total Reward = 35.0\n",
      "Episode 86, Total Reward = 24.0\n",
      "Episode 87, Total Reward = 14.0\n",
      "Episode 88, Total Reward = 191.0\n",
      "Episode 89, Total Reward = 24.0\n",
      "Episode 90, Total Reward = 18.0\n",
      "Episode 91, Total Reward = 12.0\n",
      "Episode 92, Total Reward = 35.0\n",
      "Episode 93, Total Reward = 29.0\n",
      "Episode 94, Total Reward = 36.0\n",
      "Episode 95, Total Reward = 44.0\n",
      "Episode 96, Total Reward = 24.0\n",
      "Episode 97, Total Reward = 32.0\n",
      "Episode 98, Total Reward = 19.0\n",
      "Episode 99, Total Reward = 23.0\n",
      "Episode 100, Total Reward = 52.0\n",
      "Running experiment with lr=0.0001, epsilon=0.2, k_epochs=5, batch_size=32\n",
      "Episode 1, Total Reward = 5.0\n",
      "Episode 2, Total Reward = 4.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 10.0\n",
      "Episode 5, Total Reward = 6.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 4.0\n",
      "Episode 8, Total Reward = 10.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 4.0\n",
      "Episode 11, Total Reward = 5.0\n",
      "Episode 12, Total Reward = 25.0\n",
      "Episode 13, Total Reward = 4.0\n",
      "Episode 14, Total Reward = 5.0\n",
      "Episode 15, Total Reward = 9.0\n",
      "Episode 16, Total Reward = 6.0\n",
      "Episode 17, Total Reward = 5.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 14.0\n",
      "Episode 20, Total Reward = 7.0\n",
      "Episode 21, Total Reward = 7.0\n",
      "Episode 22, Total Reward = 10.0\n",
      "Episode 23, Total Reward = 5.0\n",
      "Episode 24, Total Reward = 6.0\n",
      "Episode 25, Total Reward = 6.0\n",
      "Episode 26, Total Reward = 14.0\n",
      "Episode 27, Total Reward = 6.0\n",
      "Episode 28, Total Reward = 24.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 9.0\n",
      "Episode 31, Total Reward = 12.0\n",
      "Episode 32, Total Reward = 5.0\n",
      "Episode 33, Total Reward = 4.0\n",
      "Episode 34, Total Reward = 8.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 11.0\n",
      "Episode 37, Total Reward = 13.0\n",
      "Episode 38, Total Reward = 11.0\n",
      "Episode 39, Total Reward = 25.0\n",
      "Episode 40, Total Reward = 15.0\n",
      "Episode 41, Total Reward = 5.0\n",
      "Episode 42, Total Reward = 21.0\n",
      "Episode 43, Total Reward = 5.0\n",
      "Episode 44, Total Reward = 17.0\n",
      "Episode 45, Total Reward = 6.0\n",
      "Episode 46, Total Reward = 22.0\n",
      "Episode 47, Total Reward = 4.0\n",
      "Episode 48, Total Reward = 11.0\n",
      "Episode 49, Total Reward = 26.0\n",
      "Episode 50, Total Reward = 7.0\n",
      "Episode 51, Total Reward = 5.0\n",
      "Episode 52, Total Reward = 9.0\n",
      "Episode 53, Total Reward = 9.0\n",
      "Episode 54, Total Reward = 7.0\n",
      "Episode 55, Total Reward = 28.0\n",
      "Episode 56, Total Reward = 5.0\n",
      "Episode 57, Total Reward = 10.0\n",
      "Episode 58, Total Reward = 25.0\n",
      "Episode 59, Total Reward = 13.0\n",
      "Episode 60, Total Reward = 13.0\n",
      "Episode 61, Total Reward = 13.0\n",
      "Episode 62, Total Reward = 14.0\n",
      "Episode 63, Total Reward = 11.0\n",
      "Episode 64, Total Reward = 6.0\n",
      "Episode 65, Total Reward = 19.0\n",
      "Episode 66, Total Reward = 5.0\n",
      "Episode 67, Total Reward = 4.0\n",
      "Episode 68, Total Reward = 5.0\n",
      "Episode 69, Total Reward = 24.0\n",
      "Episode 70, Total Reward = 12.0\n",
      "Episode 71, Total Reward = 9.0\n",
      "Episode 72, Total Reward = 14.0\n",
      "Episode 73, Total Reward = 22.0\n",
      "Episode 74, Total Reward = 19.0\n",
      "Episode 75, Total Reward = 35.0\n",
      "Episode 76, Total Reward = 19.0\n",
      "Episode 77, Total Reward = 6.0\n",
      "Episode 78, Total Reward = 18.0\n",
      "Episode 79, Total Reward = 18.0\n",
      "Episode 80, Total Reward = 14.0\n",
      "Episode 81, Total Reward = 13.0\n",
      "Episode 82, Total Reward = 11.0\n",
      "Episode 83, Total Reward = 14.0\n",
      "Episode 84, Total Reward = 33.0\n",
      "Episode 85, Total Reward = 7.0\n",
      "Episode 86, Total Reward = 34.0\n",
      "Episode 87, Total Reward = 11.0\n",
      "Episode 88, Total Reward = 13.0\n",
      "Episode 89, Total Reward = 50.0\n",
      "Episode 90, Total Reward = 41.0\n",
      "Episode 91, Total Reward = 23.0\n",
      "Episode 92, Total Reward = 47.0\n",
      "Episode 93, Total Reward = 10.0\n",
      "Episode 94, Total Reward = 23.0\n",
      "Episode 95, Total Reward = 12.0\n",
      "Episode 96, Total Reward = 13.0\n",
      "Episode 97, Total Reward = 19.0\n",
      "Episode 98, Total Reward = 7.0\n",
      "Episode 99, Total Reward = 16.0\n",
      "Episode 100, Total Reward = 33.0\n",
      "Running experiment with lr=0.0001, epsilon=0.2, k_epochs=5, batch_size=64\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 11.0\n",
      "Episode 4, Total Reward = 3.0\n",
      "Episode 5, Total Reward = 11.0\n",
      "Episode 6, Total Reward = 7.0\n",
      "Episode 7, Total Reward = 24.0\n",
      "Episode 8, Total Reward = 7.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 16.0\n",
      "Episode 12, Total Reward = 7.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 16.0\n",
      "Episode 15, Total Reward = 8.0\n",
      "Episode 16, Total Reward = 4.0\n",
      "Episode 17, Total Reward = 5.0\n",
      "Episode 18, Total Reward = 10.0\n",
      "Episode 19, Total Reward = 12.0\n",
      "Episode 20, Total Reward = 21.0\n",
      "Episode 21, Total Reward = 8.0\n",
      "Episode 22, Total Reward = 26.0\n",
      "Episode 23, Total Reward = 6.0\n",
      "Episode 24, Total Reward = 9.0\n",
      "Episode 25, Total Reward = 4.0\n",
      "Episode 26, Total Reward = 13.0\n",
      "Episode 27, Total Reward = 6.0\n",
      "Episode 28, Total Reward = 11.0\n",
      "Episode 29, Total Reward = 4.0\n",
      "Episode 30, Total Reward = 4.0\n",
      "Episode 31, Total Reward = 7.0\n",
      "Episode 32, Total Reward = 8.0\n",
      "Episode 33, Total Reward = 5.0\n",
      "Episode 34, Total Reward = 6.0\n",
      "Episode 35, Total Reward = 4.0\n",
      "Episode 36, Total Reward = 12.0\n",
      "Episode 37, Total Reward = 20.0\n",
      "Episode 38, Total Reward = 18.0\n",
      "Episode 39, Total Reward = 17.0\n",
      "Episode 40, Total Reward = 11.0\n",
      "Episode 41, Total Reward = 22.0\n",
      "Episode 42, Total Reward = 8.0\n",
      "Episode 43, Total Reward = 8.0\n",
      "Episode 44, Total Reward = 5.0\n",
      "Episode 45, Total Reward = 8.0\n",
      "Episode 46, Total Reward = 4.0\n",
      "Episode 47, Total Reward = 3.0\n",
      "Episode 48, Total Reward = 5.0\n",
      "Episode 49, Total Reward = 11.0\n",
      "Episode 50, Total Reward = 12.0\n",
      "Episode 51, Total Reward = 31.0\n",
      "Episode 52, Total Reward = 5.0\n",
      "Episode 53, Total Reward = 57.0\n",
      "Episode 54, Total Reward = 7.0\n",
      "Episode 55, Total Reward = 24.0\n",
      "Episode 56, Total Reward = 6.0\n",
      "Episode 57, Total Reward = 6.0\n",
      "Episode 58, Total Reward = 6.0\n",
      "Episode 59, Total Reward = 7.0\n",
      "Episode 60, Total Reward = 10.0\n",
      "Episode 61, Total Reward = 5.0\n",
      "Episode 62, Total Reward = 7.0\n",
      "Episode 63, Total Reward = 9.0\n",
      "Episode 64, Total Reward = 7.0\n",
      "Episode 65, Total Reward = 20.0\n",
      "Episode 66, Total Reward = 9.0\n",
      "Episode 67, Total Reward = 5.0\n",
      "Episode 68, Total Reward = 7.0\n",
      "Episode 69, Total Reward = 15.0\n",
      "Episode 70, Total Reward = 3.0\n",
      "Episode 71, Total Reward = 19.0\n",
      "Episode 72, Total Reward = 9.0\n",
      "Episode 73, Total Reward = 22.0\n",
      "Episode 74, Total Reward = 11.0\n",
      "Episode 75, Total Reward = 7.0\n",
      "Episode 76, Total Reward = 13.0\n",
      "Episode 77, Total Reward = 16.0\n",
      "Episode 78, Total Reward = 11.0\n",
      "Episode 79, Total Reward = 14.0\n",
      "Episode 80, Total Reward = 6.0\n",
      "Episode 81, Total Reward = 9.0\n",
      "Episode 82, Total Reward = 9.0\n",
      "Episode 83, Total Reward = 13.0\n",
      "Episode 84, Total Reward = 4.0\n",
      "Episode 85, Total Reward = 14.0\n",
      "Episode 86, Total Reward = 10.0\n",
      "Episode 87, Total Reward = 8.0\n",
      "Episode 88, Total Reward = 8.0\n",
      "Episode 89, Total Reward = 5.0\n",
      "Episode 90, Total Reward = 18.0\n",
      "Episode 91, Total Reward = 25.0\n",
      "Episode 92, Total Reward = 8.0\n",
      "Episode 93, Total Reward = 11.0\n",
      "Episode 94, Total Reward = 14.0\n",
      "Episode 95, Total Reward = 9.0\n",
      "Episode 96, Total Reward = 22.0\n",
      "Episode 97, Total Reward = 11.0\n",
      "Episode 98, Total Reward = 19.0\n",
      "Episode 99, Total Reward = 11.0\n",
      "Episode 100, Total Reward = 16.0\n",
      "Running experiment with lr=0.0001, epsilon=0.2, k_epochs=5, batch_size=128\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 9.0\n",
      "Episode 3, Total Reward = 7.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 5.0\n",
      "Episode 6, Total Reward = 8.0\n",
      "Episode 7, Total Reward = 3.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 15.0\n",
      "Episode 12, Total Reward = 10.0\n",
      "Episode 13, Total Reward = 10.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 23.0\n",
      "Episode 16, Total Reward = 3.0\n",
      "Episode 17, Total Reward = 9.0\n",
      "Episode 18, Total Reward = 7.0\n",
      "Episode 19, Total Reward = 5.0\n",
      "Episode 20, Total Reward = 4.0\n",
      "Episode 21, Total Reward = 5.0\n",
      "Episode 22, Total Reward = 6.0\n",
      "Episode 23, Total Reward = 12.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 8.0\n",
      "Episode 26, Total Reward = 3.0\n",
      "Episode 27, Total Reward = 7.0\n",
      "Episode 28, Total Reward = 8.0\n",
      "Episode 29, Total Reward = 9.0\n",
      "Episode 30, Total Reward = 5.0\n",
      "Episode 31, Total Reward = 7.0\n",
      "Episode 32, Total Reward = 5.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 6.0\n",
      "Episode 35, Total Reward = 15.0\n",
      "Episode 36, Total Reward = 13.0\n",
      "Episode 37, Total Reward = 7.0\n",
      "Episode 38, Total Reward = 15.0\n",
      "Episode 39, Total Reward = 5.0\n",
      "Episode 40, Total Reward = 6.0\n",
      "Episode 41, Total Reward = 10.0\n",
      "Episode 42, Total Reward = 11.0\n",
      "Episode 43, Total Reward = 6.0\n",
      "Episode 44, Total Reward = 15.0\n",
      "Episode 45, Total Reward = 3.0\n",
      "Episode 46, Total Reward = 9.0\n",
      "Episode 47, Total Reward = 14.0\n",
      "Episode 48, Total Reward = 12.0\n",
      "Episode 49, Total Reward = 9.0\n",
      "Episode 50, Total Reward = 4.0\n",
      "Episode 51, Total Reward = 17.0\n",
      "Episode 52, Total Reward = 19.0\n",
      "Episode 53, Total Reward = 11.0\n",
      "Episode 54, Total Reward = 5.0\n",
      "Episode 55, Total Reward = 8.0\n",
      "Episode 56, Total Reward = 6.0\n",
      "Episode 57, Total Reward = 15.0\n",
      "Episode 58, Total Reward = 6.0\n",
      "Episode 59, Total Reward = 8.0\n",
      "Episode 60, Total Reward = 3.0\n",
      "Episode 61, Total Reward = 17.0\n",
      "Episode 62, Total Reward = 11.0\n",
      "Episode 63, Total Reward = 8.0\n",
      "Episode 64, Total Reward = 10.0\n",
      "Episode 65, Total Reward = 5.0\n",
      "Episode 66, Total Reward = 3.0\n",
      "Episode 67, Total Reward = 5.0\n",
      "Episode 68, Total Reward = 5.0\n",
      "Episode 69, Total Reward = 10.0\n",
      "Episode 70, Total Reward = 3.0\n",
      "Episode 71, Total Reward = 6.0\n",
      "Episode 72, Total Reward = 10.0\n",
      "Episode 73, Total Reward = 5.0\n",
      "Episode 74, Total Reward = 4.0\n",
      "Episode 75, Total Reward = 18.0\n",
      "Episode 76, Total Reward = 12.0\n",
      "Episode 77, Total Reward = 21.0\n",
      "Episode 78, Total Reward = 6.0\n",
      "Episode 79, Total Reward = 4.0\n",
      "Episode 80, Total Reward = 7.0\n",
      "Episode 81, Total Reward = 5.0\n",
      "Episode 82, Total Reward = 13.0\n",
      "Episode 83, Total Reward = 10.0\n",
      "Episode 84, Total Reward = 7.0\n",
      "Episode 85, Total Reward = 10.0\n",
      "Episode 86, Total Reward = 6.0\n",
      "Episode 87, Total Reward = 5.0\n",
      "Episode 88, Total Reward = 11.0\n",
      "Episode 89, Total Reward = 20.0\n",
      "Episode 90, Total Reward = 20.0\n",
      "Episode 91, Total Reward = 13.0\n",
      "Episode 92, Total Reward = 6.0\n",
      "Episode 93, Total Reward = 7.0\n",
      "Episode 94, Total Reward = 6.0\n",
      "Episode 95, Total Reward = 4.0\n",
      "Episode 96, Total Reward = 5.0\n",
      "Episode 97, Total Reward = 16.0\n",
      "Episode 98, Total Reward = 7.0\n",
      "Episode 99, Total Reward = 5.0\n",
      "Episode 100, Total Reward = 5.0\n",
      "Running experiment with lr=0.0001, epsilon=0.2, k_epochs=10, batch_size=32\n",
      "Episode 1, Total Reward = 6.0\n",
      "Episode 2, Total Reward = 16.0\n",
      "Episode 3, Total Reward = 5.0\n",
      "Episode 4, Total Reward = 21.0\n",
      "Episode 5, Total Reward = 10.0\n",
      "Episode 6, Total Reward = 7.0\n",
      "Episode 7, Total Reward = 7.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 8.0\n",
      "Episode 11, Total Reward = 13.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 4.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 9.0\n",
      "Episode 16, Total Reward = 11.0\n",
      "Episode 17, Total Reward = 11.0\n",
      "Episode 18, Total Reward = 11.0\n",
      "Episode 19, Total Reward = 7.0\n",
      "Episode 20, Total Reward = 6.0\n",
      "Episode 21, Total Reward = 10.0\n",
      "Episode 22, Total Reward = 7.0\n",
      "Episode 23, Total Reward = 4.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 9.0\n",
      "Episode 26, Total Reward = 11.0\n",
      "Episode 27, Total Reward = 13.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 8.0\n",
      "Episode 30, Total Reward = 6.0\n",
      "Episode 31, Total Reward = 11.0\n",
      "Episode 32, Total Reward = 25.0\n",
      "Episode 33, Total Reward = 4.0\n",
      "Episode 34, Total Reward = 12.0\n",
      "Episode 35, Total Reward = 14.0\n",
      "Episode 36, Total Reward = 7.0\n",
      "Episode 37, Total Reward = 8.0\n",
      "Episode 38, Total Reward = 8.0\n",
      "Episode 39, Total Reward = 7.0\n",
      "Episode 40, Total Reward = 8.0\n",
      "Episode 41, Total Reward = 7.0\n",
      "Episode 42, Total Reward = 9.0\n",
      "Episode 43, Total Reward = 8.0\n",
      "Episode 44, Total Reward = 5.0\n",
      "Episode 45, Total Reward = 10.0\n",
      "Episode 46, Total Reward = 6.0\n",
      "Episode 47, Total Reward = 18.0\n",
      "Episode 48, Total Reward = 7.0\n",
      "Episode 49, Total Reward = 5.0\n",
      "Episode 50, Total Reward = 5.0\n",
      "Episode 51, Total Reward = 7.0\n",
      "Episode 52, Total Reward = 5.0\n",
      "Episode 53, Total Reward = 5.0\n",
      "Episode 54, Total Reward = 16.0\n",
      "Episode 55, Total Reward = 35.0\n",
      "Episode 56, Total Reward = 6.0\n",
      "Episode 57, Total Reward = 34.0\n",
      "Episode 58, Total Reward = 12.0\n",
      "Episode 59, Total Reward = 17.0\n",
      "Episode 60, Total Reward = 7.0\n",
      "Episode 61, Total Reward = 12.0\n",
      "Episode 62, Total Reward = 26.0\n",
      "Episode 63, Total Reward = 35.0\n",
      "Episode 64, Total Reward = 22.0\n",
      "Episode 65, Total Reward = 4.0\n",
      "Episode 66, Total Reward = 14.0\n",
      "Episode 67, Total Reward = 32.0\n",
      "Episode 68, Total Reward = 13.0\n",
      "Episode 69, Total Reward = 33.0\n",
      "Episode 70, Total Reward = 40.0\n",
      "Episode 71, Total Reward = 12.0\n",
      "Episode 72, Total Reward = 16.0\n",
      "Episode 73, Total Reward = 19.0\n",
      "Episode 74, Total Reward = 24.0\n",
      "Episode 75, Total Reward = 12.0\n",
      "Episode 76, Total Reward = 24.0\n",
      "Episode 77, Total Reward = 50.0\n",
      "Episode 78, Total Reward = 7.0\n",
      "Episode 79, Total Reward = 12.0\n",
      "Episode 80, Total Reward = 39.0\n",
      "Episode 81, Total Reward = 27.0\n",
      "Episode 82, Total Reward = 13.0\n",
      "Episode 83, Total Reward = 32.0\n",
      "Episode 84, Total Reward = 23.0\n",
      "Episode 85, Total Reward = 18.0\n",
      "Episode 86, Total Reward = 36.0\n",
      "Episode 87, Total Reward = 11.0\n",
      "Episode 88, Total Reward = 15.0\n",
      "Episode 89, Total Reward = 15.0\n",
      "Episode 90, Total Reward = 24.0\n",
      "Episode 91, Total Reward = 11.0\n",
      "Episode 92, Total Reward = 13.0\n",
      "Episode 93, Total Reward = 31.0\n",
      "Episode 94, Total Reward = 12.0\n",
      "Episode 95, Total Reward = 65.0\n",
      "Episode 96, Total Reward = 27.0\n",
      "Episode 97, Total Reward = 21.0\n",
      "Episode 98, Total Reward = 20.0\n",
      "Episode 99, Total Reward = 36.0\n",
      "Episode 100, Total Reward = 55.0\n",
      "Running experiment with lr=0.0001, epsilon=0.2, k_epochs=10, batch_size=64\n",
      "Episode 1, Total Reward = 4.0\n",
      "Episode 2, Total Reward = 14.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 12.0\n",
      "Episode 5, Total Reward = 9.0\n",
      "Episode 6, Total Reward = 4.0\n",
      "Episode 7, Total Reward = 11.0\n",
      "Episode 8, Total Reward = 7.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 7.0\n",
      "Episode 11, Total Reward = 6.0\n",
      "Episode 12, Total Reward = 6.0\n",
      "Episode 13, Total Reward = 10.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 8.0\n",
      "Episode 16, Total Reward = 5.0\n",
      "Episode 17, Total Reward = 8.0\n",
      "Episode 18, Total Reward = 8.0\n",
      "Episode 19, Total Reward = 7.0\n",
      "Episode 20, Total Reward = 8.0\n",
      "Episode 21, Total Reward = 10.0\n",
      "Episode 22, Total Reward = 20.0\n",
      "Episode 23, Total Reward = 6.0\n",
      "Episode 24, Total Reward = 9.0\n",
      "Episode 25, Total Reward = 8.0\n",
      "Episode 26, Total Reward = 6.0\n",
      "Episode 27, Total Reward = 10.0\n",
      "Episode 28, Total Reward = 9.0\n",
      "Episode 29, Total Reward = 20.0\n",
      "Episode 30, Total Reward = 7.0\n",
      "Episode 31, Total Reward = 5.0\n",
      "Episode 32, Total Reward = 5.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 11.0\n",
      "Episode 35, Total Reward = 5.0\n",
      "Episode 36, Total Reward = 14.0\n",
      "Episode 37, Total Reward = 7.0\n",
      "Episode 38, Total Reward = 5.0\n",
      "Episode 39, Total Reward = 12.0\n",
      "Episode 40, Total Reward = 8.0\n",
      "Episode 41, Total Reward = 25.0\n",
      "Episode 42, Total Reward = 6.0\n",
      "Episode 43, Total Reward = 28.0\n",
      "Episode 44, Total Reward = 5.0\n",
      "Episode 45, Total Reward = 8.0\n",
      "Episode 46, Total Reward = 5.0\n",
      "Episode 47, Total Reward = 5.0\n",
      "Episode 48, Total Reward = 10.0\n",
      "Episode 49, Total Reward = 6.0\n",
      "Episode 50, Total Reward = 5.0\n",
      "Episode 51, Total Reward = 5.0\n",
      "Episode 52, Total Reward = 5.0\n",
      "Episode 53, Total Reward = 14.0\n",
      "Episode 54, Total Reward = 6.0\n",
      "Episode 55, Total Reward = 27.0\n",
      "Episode 56, Total Reward = 12.0\n",
      "Episode 57, Total Reward = 8.0\n",
      "Episode 58, Total Reward = 9.0\n",
      "Episode 59, Total Reward = 15.0\n",
      "Episode 60, Total Reward = 10.0\n",
      "Episode 61, Total Reward = 23.0\n",
      "Episode 62, Total Reward = 10.0\n",
      "Episode 63, Total Reward = 18.0\n",
      "Episode 64, Total Reward = 11.0\n",
      "Episode 65, Total Reward = 19.0\n",
      "Episode 66, Total Reward = 5.0\n",
      "Episode 67, Total Reward = 21.0\n",
      "Episode 68, Total Reward = 21.0\n",
      "Episode 69, Total Reward = 6.0\n",
      "Episode 70, Total Reward = 5.0\n",
      "Episode 71, Total Reward = 10.0\n",
      "Episode 72, Total Reward = 17.0\n",
      "Episode 73, Total Reward = 47.0\n",
      "Episode 74, Total Reward = 11.0\n",
      "Episode 75, Total Reward = 10.0\n",
      "Episode 76, Total Reward = 22.0\n",
      "Episode 77, Total Reward = 13.0\n",
      "Episode 78, Total Reward = 6.0\n",
      "Episode 79, Total Reward = 4.0\n",
      "Episode 80, Total Reward = 7.0\n",
      "Episode 81, Total Reward = 13.0\n",
      "Episode 82, Total Reward = 12.0\n",
      "Episode 83, Total Reward = 10.0\n",
      "Episode 84, Total Reward = 16.0\n",
      "Episode 85, Total Reward = 14.0\n",
      "Episode 86, Total Reward = 10.0\n",
      "Episode 87, Total Reward = 53.0\n",
      "Episode 88, Total Reward = 36.0\n",
      "Episode 89, Total Reward = 5.0\n",
      "Episode 90, Total Reward = 17.0\n",
      "Episode 91, Total Reward = 46.0\n",
      "Episode 92, Total Reward = 24.0\n",
      "Episode 93, Total Reward = 80.0\n",
      "Episode 94, Total Reward = 10.0\n",
      "Episode 95, Total Reward = 18.0\n",
      "Episode 96, Total Reward = 13.0\n",
      "Episode 97, Total Reward = 16.0\n",
      "Episode 98, Total Reward = 55.0\n",
      "Episode 99, Total Reward = 45.0\n",
      "Episode 100, Total Reward = 57.0\n",
      "Running experiment with lr=0.0001, epsilon=0.2, k_epochs=10, batch_size=128\n",
      "Episode 1, Total Reward = 6.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 11.0\n",
      "Episode 4, Total Reward = 4.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 5.0\n",
      "Episode 7, Total Reward = 9.0\n",
      "Episode 8, Total Reward = 7.0\n",
      "Episode 9, Total Reward = 4.0\n",
      "Episode 10, Total Reward = 6.0\n",
      "Episode 11, Total Reward = 10.0\n",
      "Episode 12, Total Reward = 13.0\n",
      "Episode 13, Total Reward = 6.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 7.0\n",
      "Episode 17, Total Reward = 5.0\n",
      "Episode 18, Total Reward = 11.0\n",
      "Episode 19, Total Reward = 5.0\n",
      "Episode 20, Total Reward = 11.0\n",
      "Episode 21, Total Reward = 5.0\n",
      "Episode 22, Total Reward = 18.0\n",
      "Episode 23, Total Reward = 18.0\n",
      "Episode 24, Total Reward = 6.0\n",
      "Episode 25, Total Reward = 9.0\n",
      "Episode 26, Total Reward = 5.0\n",
      "Episode 27, Total Reward = 4.0\n",
      "Episode 28, Total Reward = 11.0\n",
      "Episode 29, Total Reward = 4.0\n",
      "Episode 30, Total Reward = 35.0\n",
      "Episode 31, Total Reward = 5.0\n",
      "Episode 32, Total Reward = 21.0\n",
      "Episode 33, Total Reward = 4.0\n",
      "Episode 34, Total Reward = 7.0\n",
      "Episode 35, Total Reward = 4.0\n",
      "Episode 36, Total Reward = 9.0\n",
      "Episode 37, Total Reward = 10.0\n",
      "Episode 38, Total Reward = 5.0\n",
      "Episode 39, Total Reward = 5.0\n",
      "Episode 40, Total Reward = 4.0\n",
      "Episode 41, Total Reward = 5.0\n",
      "Episode 42, Total Reward = 8.0\n",
      "Episode 43, Total Reward = 7.0\n",
      "Episode 44, Total Reward = 10.0\n",
      "Episode 45, Total Reward = 4.0\n",
      "Episode 46, Total Reward = 5.0\n",
      "Episode 47, Total Reward = 9.0\n",
      "Episode 48, Total Reward = 9.0\n",
      "Episode 49, Total Reward = 12.0\n",
      "Episode 50, Total Reward = 6.0\n",
      "Episode 51, Total Reward = 6.0\n",
      "Episode 52, Total Reward = 10.0\n",
      "Episode 53, Total Reward = 8.0\n",
      "Episode 54, Total Reward = 6.0\n",
      "Episode 55, Total Reward = 9.0\n",
      "Episode 56, Total Reward = 10.0\n",
      "Episode 57, Total Reward = 10.0\n",
      "Episode 58, Total Reward = 22.0\n",
      "Episode 59, Total Reward = 5.0\n",
      "Episode 60, Total Reward = 10.0\n",
      "Episode 61, Total Reward = 18.0\n",
      "Episode 62, Total Reward = 10.0\n",
      "Episode 63, Total Reward = 5.0\n",
      "Episode 64, Total Reward = 6.0\n",
      "Episode 65, Total Reward = 16.0\n",
      "Episode 66, Total Reward = 15.0\n",
      "Episode 67, Total Reward = 31.0\n",
      "Episode 68, Total Reward = 11.0\n",
      "Episode 69, Total Reward = 9.0\n",
      "Episode 70, Total Reward = 42.0\n",
      "Episode 71, Total Reward = 10.0\n",
      "Episode 72, Total Reward = 13.0\n",
      "Episode 73, Total Reward = 13.0\n",
      "Episode 74, Total Reward = 7.0\n",
      "Episode 75, Total Reward = 8.0\n",
      "Episode 76, Total Reward = 41.0\n",
      "Episode 77, Total Reward = 4.0\n",
      "Episode 78, Total Reward = 5.0\n",
      "Episode 79, Total Reward = 24.0\n",
      "Episode 80, Total Reward = 9.0\n",
      "Episode 81, Total Reward = 14.0\n",
      "Episode 82, Total Reward = 7.0\n",
      "Episode 83, Total Reward = 40.0\n",
      "Episode 84, Total Reward = 8.0\n",
      "Episode 85, Total Reward = 12.0\n",
      "Episode 86, Total Reward = 12.0\n",
      "Episode 87, Total Reward = 5.0\n",
      "Episode 88, Total Reward = 12.0\n",
      "Episode 89, Total Reward = 40.0\n",
      "Episode 90, Total Reward = 11.0\n",
      "Episode 91, Total Reward = 14.0\n",
      "Episode 92, Total Reward = 21.0\n",
      "Episode 93, Total Reward = 13.0\n",
      "Episode 94, Total Reward = 6.0\n",
      "Episode 95, Total Reward = 7.0\n",
      "Episode 96, Total Reward = 40.0\n",
      "Episode 97, Total Reward = 13.0\n",
      "Episode 98, Total Reward = 19.0\n",
      "Episode 99, Total Reward = 66.0\n",
      "Episode 100, Total Reward = 15.0\n",
      "Running experiment with lr=0.0001, epsilon=0.2, k_epochs=20, batch_size=32\n",
      "Episode 1, Total Reward = 5.0\n",
      "Episode 2, Total Reward = 7.0\n",
      "Episode 3, Total Reward = 5.0\n",
      "Episode 4, Total Reward = 7.0\n",
      "Episode 5, Total Reward = 16.0\n",
      "Episode 6, Total Reward = 7.0\n",
      "Episode 7, Total Reward = 11.0\n",
      "Episode 8, Total Reward = 15.0\n",
      "Episode 9, Total Reward = 21.0\n",
      "Episode 10, Total Reward = 8.0\n",
      "Episode 11, Total Reward = 10.0\n",
      "Episode 12, Total Reward = 10.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 11.0\n",
      "Episode 15, Total Reward = 20.0\n",
      "Episode 16, Total Reward = 12.0\n",
      "Episode 17, Total Reward = 13.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 6.0\n",
      "Episode 20, Total Reward = 9.0\n",
      "Episode 21, Total Reward = 7.0\n",
      "Episode 22, Total Reward = 14.0\n",
      "Episode 23, Total Reward = 13.0\n",
      "Episode 24, Total Reward = 6.0\n",
      "Episode 25, Total Reward = 9.0\n",
      "Episode 26, Total Reward = 8.0\n",
      "Episode 27, Total Reward = 21.0\n",
      "Episode 28, Total Reward = 4.0\n",
      "Episode 29, Total Reward = 16.0\n",
      "Episode 30, Total Reward = 12.0\n",
      "Episode 31, Total Reward = 14.0\n",
      "Episode 32, Total Reward = 22.0\n",
      "Episode 33, Total Reward = 12.0\n",
      "Episode 34, Total Reward = 3.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 5.0\n",
      "Episode 37, Total Reward = 19.0\n",
      "Episode 38, Total Reward = 18.0\n",
      "Episode 39, Total Reward = 14.0\n",
      "Episode 40, Total Reward = 6.0\n",
      "Episode 41, Total Reward = 5.0\n",
      "Episode 42, Total Reward = 6.0\n",
      "Episode 43, Total Reward = 27.0\n",
      "Episode 44, Total Reward = 17.0\n",
      "Episode 45, Total Reward = 19.0\n",
      "Episode 46, Total Reward = 17.0\n",
      "Episode 47, Total Reward = 23.0\n",
      "Episode 48, Total Reward = 22.0\n",
      "Episode 49, Total Reward = 25.0\n",
      "Episode 50, Total Reward = 15.0\n",
      "Episode 51, Total Reward = 25.0\n",
      "Episode 52, Total Reward = 19.0\n",
      "Episode 53, Total Reward = 22.0\n",
      "Episode 54, Total Reward = 11.0\n",
      "Episode 55, Total Reward = 40.0\n",
      "Episode 56, Total Reward = 9.0\n",
      "Episode 57, Total Reward = 93.0\n",
      "Episode 58, Total Reward = 23.0\n",
      "Episode 59, Total Reward = 23.0\n",
      "Episode 60, Total Reward = 54.0\n",
      "Episode 61, Total Reward = 49.0\n",
      "Episode 62, Total Reward = 21.0\n",
      "Episode 63, Total Reward = 28.0\n",
      "Episode 64, Total Reward = 17.0\n",
      "Episode 65, Total Reward = 17.0\n",
      "Episode 66, Total Reward = 21.0\n",
      "Episode 67, Total Reward = 33.0\n",
      "Episode 68, Total Reward = 30.0\n",
      "Episode 69, Total Reward = 76.0\n",
      "Episode 70, Total Reward = 13.0\n",
      "Episode 71, Total Reward = 26.0\n",
      "Episode 72, Total Reward = 18.0\n",
      "Episode 73, Total Reward = 34.0\n",
      "Episode 74, Total Reward = 18.0\n",
      "Episode 75, Total Reward = 39.0\n",
      "Episode 76, Total Reward = 42.0\n",
      "Episode 77, Total Reward = 60.0\n",
      "Episode 78, Total Reward = 34.0\n",
      "Episode 79, Total Reward = 8.0\n",
      "Episode 80, Total Reward = 28.0\n",
      "Episode 81, Total Reward = 12.0\n",
      "Episode 82, Total Reward = 31.0\n",
      "Episode 83, Total Reward = 40.0\n",
      "Episode 84, Total Reward = 24.0\n",
      "Episode 85, Total Reward = 27.0\n",
      "Episode 86, Total Reward = 18.0\n",
      "Episode 87, Total Reward = 53.0\n",
      "Episode 88, Total Reward = 36.0\n",
      "Episode 89, Total Reward = 11.0\n",
      "Episode 90, Total Reward = 22.0\n",
      "Episode 91, Total Reward = 17.0\n",
      "Episode 92, Total Reward = 52.0\n",
      "Episode 93, Total Reward = 38.0\n",
      "Episode 94, Total Reward = 49.0\n",
      "Episode 95, Total Reward = 85.0\n",
      "Episode 96, Total Reward = 47.0\n",
      "Episode 97, Total Reward = 96.0\n",
      "Episode 98, Total Reward = 24.0\n",
      "Episode 99, Total Reward = 50.0\n",
      "Episode 100, Total Reward = 59.0\n",
      "Running experiment with lr=0.0001, epsilon=0.2, k_epochs=20, batch_size=64\n",
      "Episode 1, Total Reward = 4.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 7.0\n",
      "Episode 5, Total Reward = 7.0\n",
      "Episode 6, Total Reward = 17.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 8.0\n",
      "Episode 9, Total Reward = 9.0\n",
      "Episode 10, Total Reward = 13.0\n",
      "Episode 11, Total Reward = 6.0\n",
      "Episode 12, Total Reward = 6.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 11.0\n",
      "Episode 15, Total Reward = 9.0\n",
      "Episode 16, Total Reward = 17.0\n",
      "Episode 17, Total Reward = 5.0\n",
      "Episode 18, Total Reward = 18.0\n",
      "Episode 19, Total Reward = 7.0\n",
      "Episode 20, Total Reward = 9.0\n",
      "Episode 21, Total Reward = 17.0\n",
      "Episode 22, Total Reward = 5.0\n",
      "Episode 23, Total Reward = 8.0\n",
      "Episode 24, Total Reward = 22.0\n",
      "Episode 25, Total Reward = 10.0\n",
      "Episode 26, Total Reward = 6.0\n",
      "Episode 27, Total Reward = 16.0\n",
      "Episode 28, Total Reward = 16.0\n",
      "Episode 29, Total Reward = 4.0\n",
      "Episode 30, Total Reward = 8.0\n",
      "Episode 31, Total Reward = 10.0\n",
      "Episode 32, Total Reward = 4.0\n",
      "Episode 33, Total Reward = 6.0\n",
      "Episode 34, Total Reward = 15.0\n",
      "Episode 35, Total Reward = 22.0\n",
      "Episode 36, Total Reward = 10.0\n",
      "Episode 37, Total Reward = 10.0\n",
      "Episode 38, Total Reward = 4.0\n",
      "Episode 39, Total Reward = 36.0\n",
      "Episode 40, Total Reward = 9.0\n",
      "Episode 41, Total Reward = 8.0\n",
      "Episode 42, Total Reward = 18.0\n",
      "Episode 43, Total Reward = 27.0\n",
      "Episode 44, Total Reward = 22.0\n",
      "Episode 45, Total Reward = 11.0\n",
      "Episode 46, Total Reward = 9.0\n",
      "Episode 47, Total Reward = 35.0\n",
      "Episode 48, Total Reward = 37.0\n",
      "Episode 49, Total Reward = 33.0\n",
      "Episode 50, Total Reward = 17.0\n",
      "Episode 51, Total Reward = 47.0\n",
      "Episode 52, Total Reward = 8.0\n",
      "Episode 53, Total Reward = 43.0\n",
      "Episode 54, Total Reward = 34.0\n",
      "Episode 55, Total Reward = 30.0\n",
      "Episode 56, Total Reward = 20.0\n",
      "Episode 57, Total Reward = 49.0\n",
      "Episode 58, Total Reward = 55.0\n",
      "Episode 59, Total Reward = 23.0\n",
      "Episode 60, Total Reward = 30.0\n",
      "Episode 61, Total Reward = 9.0\n",
      "Episode 62, Total Reward = 76.0\n",
      "Episode 63, Total Reward = 20.0\n",
      "Episode 64, Total Reward = 13.0\n",
      "Episode 65, Total Reward = 39.0\n",
      "Episode 66, Total Reward = 42.0\n",
      "Episode 67, Total Reward = 25.0\n",
      "Episode 68, Total Reward = 17.0\n",
      "Episode 69, Total Reward = 39.0\n",
      "Episode 70, Total Reward = 11.0\n",
      "Episode 71, Total Reward = 47.0\n",
      "Episode 72, Total Reward = 10.0\n",
      "Episode 73, Total Reward = 24.0\n",
      "Episode 74, Total Reward = 49.0\n",
      "Episode 75, Total Reward = 18.0\n",
      "Episode 76, Total Reward = 24.0\n",
      "Episode 77, Total Reward = 9.0\n",
      "Episode 78, Total Reward = 19.0\n",
      "Episode 79, Total Reward = 33.0\n",
      "Episode 80, Total Reward = 34.0\n",
      "Episode 81, Total Reward = 16.0\n",
      "Episode 82, Total Reward = 42.0\n",
      "Episode 83, Total Reward = 44.0\n",
      "Episode 84, Total Reward = 25.0\n",
      "Episode 85, Total Reward = 41.0\n",
      "Episode 86, Total Reward = 52.0\n",
      "Episode 87, Total Reward = 56.0\n",
      "Episode 88, Total Reward = 41.0\n",
      "Episode 89, Total Reward = 52.0\n",
      "Episode 90, Total Reward = 37.0\n",
      "Episode 91, Total Reward = 34.0\n",
      "Episode 92, Total Reward = 80.0\n",
      "Episode 93, Total Reward = 98.0\n",
      "Episode 94, Total Reward = 68.0\n",
      "Episode 95, Total Reward = 40.0\n",
      "Episode 96, Total Reward = 71.0\n",
      "Episode 97, Total Reward = 49.0\n",
      "Episode 98, Total Reward = 51.0\n",
      "Episode 99, Total Reward = 60.0\n",
      "Episode 100, Total Reward = 47.0\n",
      "Running experiment with lr=0.0001, epsilon=0.2, k_epochs=20, batch_size=128\n",
      "Episode 1, Total Reward = 5.0\n",
      "Episode 2, Total Reward = 9.0\n",
      "Episode 3, Total Reward = 5.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 10.0\n",
      "Episode 6, Total Reward = 11.0\n",
      "Episode 7, Total Reward = 11.0\n",
      "Episode 8, Total Reward = 33.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 15.0\n",
      "Episode 11, Total Reward = 7.0\n",
      "Episode 12, Total Reward = 9.0\n",
      "Episode 13, Total Reward = 13.0\n",
      "Episode 14, Total Reward = 5.0\n",
      "Episode 15, Total Reward = 6.0\n",
      "Episode 16, Total Reward = 4.0\n",
      "Episode 17, Total Reward = 5.0\n",
      "Episode 18, Total Reward = 11.0\n",
      "Episode 19, Total Reward = 5.0\n",
      "Episode 20, Total Reward = 8.0\n",
      "Episode 21, Total Reward = 15.0\n",
      "Episode 22, Total Reward = 10.0\n",
      "Episode 23, Total Reward = 17.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 10.0\n",
      "Episode 26, Total Reward = 7.0\n",
      "Episode 27, Total Reward = 20.0\n",
      "Episode 28, Total Reward = 7.0\n",
      "Episode 29, Total Reward = 7.0\n",
      "Episode 30, Total Reward = 24.0\n",
      "Episode 31, Total Reward = 12.0\n",
      "Episode 32, Total Reward = 20.0\n",
      "Episode 33, Total Reward = 12.0\n",
      "Episode 34, Total Reward = 24.0\n",
      "Episode 35, Total Reward = 12.0\n",
      "Episode 36, Total Reward = 36.0\n",
      "Episode 37, Total Reward = 11.0\n",
      "Episode 38, Total Reward = 16.0\n",
      "Episode 39, Total Reward = 14.0\n",
      "Episode 40, Total Reward = 26.0\n",
      "Episode 41, Total Reward = 8.0\n",
      "Episode 42, Total Reward = 6.0\n",
      "Episode 43, Total Reward = 7.0\n",
      "Episode 44, Total Reward = 19.0\n",
      "Episode 45, Total Reward = 9.0\n",
      "Episode 46, Total Reward = 38.0\n",
      "Episode 47, Total Reward = 9.0\n",
      "Episode 48, Total Reward = 45.0\n",
      "Episode 49, Total Reward = 31.0\n",
      "Episode 50, Total Reward = 6.0\n",
      "Episode 51, Total Reward = 41.0\n",
      "Episode 52, Total Reward = 27.0\n",
      "Episode 53, Total Reward = 20.0\n",
      "Episode 54, Total Reward = 23.0\n",
      "Episode 55, Total Reward = 20.0\n",
      "Episode 56, Total Reward = 21.0\n",
      "Episode 57, Total Reward = 33.0\n",
      "Episode 58, Total Reward = 21.0\n",
      "Episode 59, Total Reward = 44.0\n",
      "Episode 60, Total Reward = 9.0\n",
      "Episode 61, Total Reward = 44.0\n",
      "Episode 62, Total Reward = 15.0\n",
      "Episode 63, Total Reward = 46.0\n",
      "Episode 64, Total Reward = 15.0\n",
      "Episode 65, Total Reward = 29.0\n",
      "Episode 66, Total Reward = 20.0\n",
      "Episode 67, Total Reward = 12.0\n",
      "Episode 68, Total Reward = 66.0\n",
      "Episode 69, Total Reward = 32.0\n",
      "Episode 70, Total Reward = 16.0\n",
      "Episode 71, Total Reward = 28.0\n",
      "Episode 72, Total Reward = 60.0\n",
      "Episode 73, Total Reward = 32.0\n",
      "Episode 74, Total Reward = 33.0\n",
      "Episode 75, Total Reward = 22.0\n",
      "Episode 76, Total Reward = 40.0\n",
      "Episode 77, Total Reward = 44.0\n",
      "Episode 78, Total Reward = 166.0\n",
      "Episode 79, Total Reward = 32.0\n",
      "Episode 80, Total Reward = 31.0\n",
      "Episode 81, Total Reward = 57.0\n",
      "Episode 82, Total Reward = 43.0\n",
      "Episode 83, Total Reward = 28.0\n",
      "Episode 84, Total Reward = 97.0\n",
      "Episode 85, Total Reward = 13.0\n",
      "Episode 86, Total Reward = 74.0\n",
      "Episode 87, Total Reward = 66.0\n",
      "Episode 88, Total Reward = 47.0\n",
      "Episode 89, Total Reward = 77.0\n",
      "Episode 90, Total Reward = 58.0\n",
      "Episode 91, Total Reward = 37.0\n",
      "Episode 92, Total Reward = 43.0\n",
      "Episode 93, Total Reward = 49.0\n",
      "Episode 94, Total Reward = 15.0\n",
      "Episode 95, Total Reward = 41.0\n",
      "Episode 96, Total Reward = 19.0\n",
      "Episode 97, Total Reward = 53.0\n",
      "Episode 98, Total Reward = 78.0\n",
      "Episode 99, Total Reward = 50.0\n",
      "Episode 100, Total Reward = 170.0\n",
      "Running experiment with lr=0.0001, epsilon=0.3, k_epochs=5, batch_size=32\n",
      "Episode 1, Total Reward = 24.0\n",
      "Episode 2, Total Reward = 24.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 13.0\n",
      "Episode 5, Total Reward = 3.0\n",
      "Episode 6, Total Reward = 19.0\n",
      "Episode 7, Total Reward = 10.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 23.0\n",
      "Episode 11, Total Reward = 4.0\n",
      "Episode 12, Total Reward = 4.0\n",
      "Episode 13, Total Reward = 17.0\n",
      "Episode 14, Total Reward = 4.0\n",
      "Episode 15, Total Reward = 4.0\n",
      "Episode 16, Total Reward = 5.0\n",
      "Episode 17, Total Reward = 6.0\n",
      "Episode 18, Total Reward = 10.0\n",
      "Episode 19, Total Reward = 15.0\n",
      "Episode 20, Total Reward = 6.0\n",
      "Episode 21, Total Reward = 8.0\n",
      "Episode 22, Total Reward = 4.0\n",
      "Episode 23, Total Reward = 6.0\n",
      "Episode 24, Total Reward = 18.0\n",
      "Episode 25, Total Reward = 6.0\n",
      "Episode 26, Total Reward = 5.0\n",
      "Episode 27, Total Reward = 4.0\n",
      "Episode 28, Total Reward = 7.0\n",
      "Episode 29, Total Reward = 10.0\n",
      "Episode 30, Total Reward = 10.0\n",
      "Episode 31, Total Reward = 7.0\n",
      "Episode 32, Total Reward = 12.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 18.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 8.0\n",
      "Episode 37, Total Reward = 18.0\n",
      "Episode 38, Total Reward = 8.0\n",
      "Episode 39, Total Reward = 17.0\n",
      "Episode 40, Total Reward = 7.0\n",
      "Episode 41, Total Reward = 7.0\n",
      "Episode 42, Total Reward = 23.0\n",
      "Episode 43, Total Reward = 9.0\n",
      "Episode 44, Total Reward = 6.0\n",
      "Episode 45, Total Reward = 7.0\n",
      "Episode 46, Total Reward = 29.0\n",
      "Episode 47, Total Reward = 19.0\n",
      "Episode 48, Total Reward = 16.0\n",
      "Episode 49, Total Reward = 6.0\n",
      "Episode 50, Total Reward = 7.0\n",
      "Episode 51, Total Reward = 7.0\n",
      "Episode 52, Total Reward = 9.0\n",
      "Episode 53, Total Reward = 11.0\n",
      "Episode 54, Total Reward = 9.0\n",
      "Episode 55, Total Reward = 10.0\n",
      "Episode 56, Total Reward = 13.0\n",
      "Episode 57, Total Reward = 12.0\n",
      "Episode 58, Total Reward = 15.0\n",
      "Episode 59, Total Reward = 6.0\n",
      "Episode 60, Total Reward = 12.0\n",
      "Episode 61, Total Reward = 21.0\n",
      "Episode 62, Total Reward = 12.0\n",
      "Episode 63, Total Reward = 11.0\n",
      "Episode 64, Total Reward = 31.0\n",
      "Episode 65, Total Reward = 6.0\n",
      "Episode 66, Total Reward = 15.0\n",
      "Episode 67, Total Reward = 7.0\n",
      "Episode 68, Total Reward = 15.0\n",
      "Episode 69, Total Reward = 27.0\n",
      "Episode 70, Total Reward = 7.0\n",
      "Episode 71, Total Reward = 8.0\n",
      "Episode 72, Total Reward = 7.0\n",
      "Episode 73, Total Reward = 5.0\n",
      "Episode 74, Total Reward = 9.0\n",
      "Episode 75, Total Reward = 16.0\n",
      "Episode 76, Total Reward = 7.0\n",
      "Episode 77, Total Reward = 31.0\n",
      "Episode 78, Total Reward = 19.0\n",
      "Episode 79, Total Reward = 6.0\n",
      "Episode 80, Total Reward = 11.0\n",
      "Episode 81, Total Reward = 7.0\n",
      "Episode 82, Total Reward = 16.0\n",
      "Episode 83, Total Reward = 44.0\n",
      "Episode 84, Total Reward = 30.0\n",
      "Episode 85, Total Reward = 12.0\n",
      "Episode 86, Total Reward = 30.0\n",
      "Episode 87, Total Reward = 12.0\n",
      "Episode 88, Total Reward = 13.0\n",
      "Episode 89, Total Reward = 19.0\n",
      "Episode 90, Total Reward = 55.0\n",
      "Episode 91, Total Reward = 15.0\n",
      "Episode 92, Total Reward = 15.0\n",
      "Episode 93, Total Reward = 6.0\n",
      "Episode 94, Total Reward = 34.0\n",
      "Episode 95, Total Reward = 20.0\n",
      "Episode 96, Total Reward = 11.0\n",
      "Episode 97, Total Reward = 25.0\n",
      "Episode 98, Total Reward = 22.0\n",
      "Episode 99, Total Reward = 23.0\n",
      "Episode 100, Total Reward = 10.0\n",
      "Running experiment with lr=0.0001, epsilon=0.3, k_epochs=5, batch_size=64\n",
      "Episode 1, Total Reward = 11.0\n",
      "Episode 2, Total Reward = 4.0\n",
      "Episode 3, Total Reward = 11.0\n",
      "Episode 4, Total Reward = 9.0\n",
      "Episode 5, Total Reward = 8.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 8.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 8.0\n",
      "Episode 12, Total Reward = 7.0\n",
      "Episode 13, Total Reward = 7.0\n",
      "Episode 14, Total Reward = 13.0\n",
      "Episode 15, Total Reward = 9.0\n",
      "Episode 16, Total Reward = 11.0\n",
      "Episode 17, Total Reward = 7.0\n",
      "Episode 18, Total Reward = 4.0\n",
      "Episode 19, Total Reward = 7.0\n",
      "Episode 20, Total Reward = 6.0\n",
      "Episode 21, Total Reward = 6.0\n",
      "Episode 22, Total Reward = 6.0\n",
      "Episode 23, Total Reward = 15.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 5.0\n",
      "Episode 26, Total Reward = 5.0\n",
      "Episode 27, Total Reward = 5.0\n",
      "Episode 28, Total Reward = 9.0\n",
      "Episode 29, Total Reward = 5.0\n",
      "Episode 30, Total Reward = 4.0\n",
      "Episode 31, Total Reward = 16.0\n",
      "Episode 32, Total Reward = 19.0\n",
      "Episode 33, Total Reward = 8.0\n",
      "Episode 34, Total Reward = 11.0\n",
      "Episode 35, Total Reward = 16.0\n",
      "Episode 36, Total Reward = 8.0\n",
      "Episode 37, Total Reward = 4.0\n",
      "Episode 38, Total Reward = 9.0\n",
      "Episode 39, Total Reward = 9.0\n",
      "Episode 40, Total Reward = 11.0\n",
      "Episode 41, Total Reward = 15.0\n",
      "Episode 42, Total Reward = 13.0\n",
      "Episode 43, Total Reward = 17.0\n",
      "Episode 44, Total Reward = 9.0\n",
      "Episode 45, Total Reward = 18.0\n",
      "Episode 46, Total Reward = 12.0\n",
      "Episode 47, Total Reward = 17.0\n",
      "Episode 48, Total Reward = 18.0\n",
      "Episode 49, Total Reward = 6.0\n",
      "Episode 50, Total Reward = 14.0\n",
      "Episode 51, Total Reward = 21.0\n",
      "Episode 52, Total Reward = 4.0\n",
      "Episode 53, Total Reward = 9.0\n",
      "Episode 54, Total Reward = 10.0\n",
      "Episode 55, Total Reward = 17.0\n",
      "Episode 56, Total Reward = 12.0\n",
      "Episode 57, Total Reward = 8.0\n",
      "Episode 58, Total Reward = 17.0\n",
      "Episode 59, Total Reward = 7.0\n",
      "Episode 60, Total Reward = 13.0\n",
      "Episode 61, Total Reward = 5.0\n",
      "Episode 62, Total Reward = 9.0\n",
      "Episode 63, Total Reward = 13.0\n",
      "Episode 64, Total Reward = 13.0\n",
      "Episode 65, Total Reward = 7.0\n",
      "Episode 66, Total Reward = 6.0\n",
      "Episode 67, Total Reward = 4.0\n",
      "Episode 68, Total Reward = 16.0\n",
      "Episode 69, Total Reward = 26.0\n",
      "Episode 70, Total Reward = 8.0\n",
      "Episode 71, Total Reward = 7.0\n",
      "Episode 72, Total Reward = 15.0\n",
      "Episode 73, Total Reward = 7.0\n",
      "Episode 74, Total Reward = 4.0\n",
      "Episode 75, Total Reward = 16.0\n",
      "Episode 76, Total Reward = 11.0\n",
      "Episode 77, Total Reward = 23.0\n",
      "Episode 78, Total Reward = 16.0\n",
      "Episode 79, Total Reward = 9.0\n",
      "Episode 80, Total Reward = 17.0\n",
      "Episode 81, Total Reward = 3.0\n",
      "Episode 82, Total Reward = 28.0\n",
      "Episode 83, Total Reward = 16.0\n",
      "Episode 84, Total Reward = 23.0\n",
      "Episode 85, Total Reward = 15.0\n",
      "Episode 86, Total Reward = 3.0\n",
      "Episode 87, Total Reward = 45.0\n",
      "Episode 88, Total Reward = 14.0\n",
      "Episode 89, Total Reward = 8.0\n",
      "Episode 90, Total Reward = 4.0\n",
      "Episode 91, Total Reward = 8.0\n",
      "Episode 92, Total Reward = 9.0\n",
      "Episode 93, Total Reward = 29.0\n",
      "Episode 94, Total Reward = 4.0\n",
      "Episode 95, Total Reward = 21.0\n",
      "Episode 96, Total Reward = 5.0\n",
      "Episode 97, Total Reward = 19.0\n",
      "Episode 98, Total Reward = 18.0\n",
      "Episode 99, Total Reward = 17.0\n",
      "Episode 100, Total Reward = 18.0\n",
      "Running experiment with lr=0.0001, epsilon=0.3, k_epochs=5, batch_size=128\n",
      "Episode 1, Total Reward = 15.0\n",
      "Episode 2, Total Reward = 13.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 18.0\n",
      "Episode 6, Total Reward = 16.0\n",
      "Episode 7, Total Reward = 6.0\n",
      "Episode 8, Total Reward = 7.0\n",
      "Episode 9, Total Reward = 4.0\n",
      "Episode 10, Total Reward = 3.0\n",
      "Episode 11, Total Reward = 5.0\n",
      "Episode 12, Total Reward = 7.0\n",
      "Episode 13, Total Reward = 8.0\n",
      "Episode 14, Total Reward = 5.0\n",
      "Episode 15, Total Reward = 14.0\n",
      "Episode 16, Total Reward = 6.0\n",
      "Episode 17, Total Reward = 12.0\n",
      "Episode 18, Total Reward = 9.0\n",
      "Episode 19, Total Reward = 7.0\n",
      "Episode 20, Total Reward = 19.0\n",
      "Episode 21, Total Reward = 4.0\n",
      "Episode 22, Total Reward = 5.0\n",
      "Episode 23, Total Reward = 9.0\n",
      "Episode 24, Total Reward = 13.0\n",
      "Episode 25, Total Reward = 6.0\n",
      "Episode 26, Total Reward = 11.0\n",
      "Episode 27, Total Reward = 5.0\n",
      "Episode 28, Total Reward = 4.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 7.0\n",
      "Episode 31, Total Reward = 11.0\n",
      "Episode 32, Total Reward = 5.0\n",
      "Episode 33, Total Reward = 6.0\n",
      "Episode 34, Total Reward = 6.0\n",
      "Episode 35, Total Reward = 4.0\n",
      "Episode 36, Total Reward = 4.0\n",
      "Episode 37, Total Reward = 3.0\n",
      "Episode 38, Total Reward = 10.0\n",
      "Episode 39, Total Reward = 10.0\n",
      "Episode 40, Total Reward = 5.0\n",
      "Episode 41, Total Reward = 7.0\n",
      "Episode 42, Total Reward = 11.0\n",
      "Episode 43, Total Reward = 6.0\n",
      "Episode 44, Total Reward = 7.0\n",
      "Episode 45, Total Reward = 13.0\n",
      "Episode 46, Total Reward = 7.0\n",
      "Episode 47, Total Reward = 5.0\n",
      "Episode 48, Total Reward = 7.0\n",
      "Episode 49, Total Reward = 5.0\n",
      "Episode 50, Total Reward = 9.0\n",
      "Episode 51, Total Reward = 6.0\n",
      "Episode 52, Total Reward = 9.0\n",
      "Episode 53, Total Reward = 24.0\n",
      "Episode 54, Total Reward = 6.0\n",
      "Episode 55, Total Reward = 12.0\n",
      "Episode 56, Total Reward = 12.0\n",
      "Episode 57, Total Reward = 4.0\n",
      "Episode 58, Total Reward = 5.0\n",
      "Episode 59, Total Reward = 9.0\n",
      "Episode 60, Total Reward = 8.0\n",
      "Episode 61, Total Reward = 12.0\n",
      "Episode 62, Total Reward = 14.0\n",
      "Episode 63, Total Reward = 20.0\n",
      "Episode 64, Total Reward = 12.0\n",
      "Episode 65, Total Reward = 5.0\n",
      "Episode 66, Total Reward = 16.0\n",
      "Episode 67, Total Reward = 10.0\n",
      "Episode 68, Total Reward = 8.0\n",
      "Episode 69, Total Reward = 8.0\n",
      "Episode 70, Total Reward = 7.0\n",
      "Episode 71, Total Reward = 5.0\n",
      "Episode 72, Total Reward = 16.0\n",
      "Episode 73, Total Reward = 6.0\n",
      "Episode 74, Total Reward = 9.0\n",
      "Episode 75, Total Reward = 6.0\n",
      "Episode 76, Total Reward = 3.0\n",
      "Episode 77, Total Reward = 6.0\n",
      "Episode 78, Total Reward = 6.0\n",
      "Episode 79, Total Reward = 14.0\n",
      "Episode 80, Total Reward = 14.0\n",
      "Episode 81, Total Reward = 23.0\n",
      "Episode 82, Total Reward = 5.0\n",
      "Episode 83, Total Reward = 12.0\n",
      "Episode 84, Total Reward = 11.0\n",
      "Episode 85, Total Reward = 4.0\n",
      "Episode 86, Total Reward = 20.0\n",
      "Episode 87, Total Reward = 7.0\n",
      "Episode 88, Total Reward = 12.0\n",
      "Episode 89, Total Reward = 11.0\n",
      "Episode 90, Total Reward = 7.0\n",
      "Episode 91, Total Reward = 8.0\n",
      "Episode 92, Total Reward = 18.0\n",
      "Episode 93, Total Reward = 5.0\n",
      "Episode 94, Total Reward = 17.0\n",
      "Episode 95, Total Reward = 4.0\n",
      "Episode 96, Total Reward = 6.0\n",
      "Episode 97, Total Reward = 22.0\n",
      "Episode 98, Total Reward = 13.0\n",
      "Episode 99, Total Reward = 14.0\n",
      "Episode 100, Total Reward = 11.0\n",
      "Running experiment with lr=0.0001, epsilon=0.3, k_epochs=10, batch_size=32\n",
      "Episode 1, Total Reward = 3.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 7.0\n",
      "Episode 4, Total Reward = 9.0\n",
      "Episode 5, Total Reward = 5.0\n",
      "Episode 6, Total Reward = 7.0\n",
      "Episode 7, Total Reward = 9.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 8.0\n",
      "Episode 10, Total Reward = 10.0\n",
      "Episode 11, Total Reward = 9.0\n",
      "Episode 12, Total Reward = 8.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 12.0\n",
      "Episode 15, Total Reward = 4.0\n",
      "Episode 16, Total Reward = 6.0\n",
      "Episode 17, Total Reward = 4.0\n",
      "Episode 18, Total Reward = 48.0\n",
      "Episode 19, Total Reward = 16.0\n",
      "Episode 20, Total Reward = 5.0\n",
      "Episode 21, Total Reward = 7.0\n",
      "Episode 22, Total Reward = 7.0\n",
      "Episode 23, Total Reward = 4.0\n",
      "Episode 24, Total Reward = 8.0\n",
      "Episode 25, Total Reward = 11.0\n",
      "Episode 26, Total Reward = 15.0\n",
      "Episode 27, Total Reward = 4.0\n",
      "Episode 28, Total Reward = 24.0\n",
      "Episode 29, Total Reward = 11.0\n",
      "Episode 30, Total Reward = 7.0\n",
      "Episode 31, Total Reward = 6.0\n",
      "Episode 32, Total Reward = 5.0\n",
      "Episode 33, Total Reward = 9.0\n",
      "Episode 34, Total Reward = 8.0\n",
      "Episode 35, Total Reward = 8.0\n",
      "Episode 36, Total Reward = 5.0\n",
      "Episode 37, Total Reward = 19.0\n",
      "Episode 38, Total Reward = 10.0\n",
      "Episode 39, Total Reward = 16.0\n",
      "Episode 40, Total Reward = 16.0\n",
      "Episode 41, Total Reward = 10.0\n",
      "Episode 42, Total Reward = 5.0\n",
      "Episode 43, Total Reward = 9.0\n",
      "Episode 44, Total Reward = 7.0\n",
      "Episode 45, Total Reward = 24.0\n",
      "Episode 46, Total Reward = 26.0\n",
      "Episode 47, Total Reward = 8.0\n",
      "Episode 48, Total Reward = 5.0\n",
      "Episode 49, Total Reward = 10.0\n",
      "Episode 50, Total Reward = 22.0\n",
      "Episode 51, Total Reward = 6.0\n",
      "Episode 52, Total Reward = 18.0\n",
      "Episode 53, Total Reward = 10.0\n",
      "Episode 54, Total Reward = 9.0\n",
      "Episode 55, Total Reward = 15.0\n",
      "Episode 56, Total Reward = 6.0\n",
      "Episode 57, Total Reward = 6.0\n",
      "Episode 58, Total Reward = 15.0\n",
      "Episode 59, Total Reward = 16.0\n",
      "Episode 60, Total Reward = 10.0\n",
      "Episode 61, Total Reward = 7.0\n",
      "Episode 62, Total Reward = 8.0\n",
      "Episode 63, Total Reward = 6.0\n",
      "Episode 64, Total Reward = 8.0\n",
      "Episode 65, Total Reward = 10.0\n",
      "Episode 66, Total Reward = 4.0\n",
      "Episode 67, Total Reward = 6.0\n",
      "Episode 68, Total Reward = 11.0\n",
      "Episode 69, Total Reward = 10.0\n",
      "Episode 70, Total Reward = 12.0\n",
      "Episode 71, Total Reward = 8.0\n",
      "Episode 72, Total Reward = 9.0\n",
      "Episode 73, Total Reward = 8.0\n",
      "Episode 74, Total Reward = 17.0\n",
      "Episode 75, Total Reward = 12.0\n",
      "Episode 76, Total Reward = 5.0\n",
      "Episode 77, Total Reward = 10.0\n",
      "Episode 78, Total Reward = 10.0\n",
      "Episode 79, Total Reward = 26.0\n",
      "Episode 80, Total Reward = 8.0\n",
      "Episode 81, Total Reward = 12.0\n",
      "Episode 82, Total Reward = 25.0\n",
      "Episode 83, Total Reward = 15.0\n",
      "Episode 84, Total Reward = 20.0\n",
      "Episode 85, Total Reward = 13.0\n",
      "Episode 86, Total Reward = 23.0\n",
      "Episode 87, Total Reward = 8.0\n",
      "Episode 88, Total Reward = 23.0\n",
      "Episode 89, Total Reward = 32.0\n",
      "Episode 90, Total Reward = 22.0\n",
      "Episode 91, Total Reward = 10.0\n",
      "Episode 92, Total Reward = 11.0\n",
      "Episode 93, Total Reward = 23.0\n",
      "Episode 94, Total Reward = 16.0\n",
      "Episode 95, Total Reward = 45.0\n",
      "Episode 96, Total Reward = 30.0\n",
      "Episode 97, Total Reward = 27.0\n",
      "Episode 98, Total Reward = 26.0\n",
      "Episode 99, Total Reward = 23.0\n",
      "Episode 100, Total Reward = 7.0\n",
      "Running experiment with lr=0.0001, epsilon=0.3, k_epochs=10, batch_size=64\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 12.0\n",
      "Episode 3, Total Reward = 7.0\n",
      "Episode 4, Total Reward = 15.0\n",
      "Episode 5, Total Reward = 7.0\n",
      "Episode 6, Total Reward = 13.0\n",
      "Episode 7, Total Reward = 7.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 9.0\n",
      "Episode 10, Total Reward = 7.0\n",
      "Episode 11, Total Reward = 5.0\n",
      "Episode 12, Total Reward = 14.0\n",
      "Episode 13, Total Reward = 10.0\n",
      "Episode 14, Total Reward = 13.0\n",
      "Episode 15, Total Reward = 6.0\n",
      "Episode 16, Total Reward = 6.0\n",
      "Episode 17, Total Reward = 19.0\n",
      "Episode 18, Total Reward = 6.0\n",
      "Episode 19, Total Reward = 7.0\n",
      "Episode 20, Total Reward = 9.0\n",
      "Episode 21, Total Reward = 9.0\n",
      "Episode 22, Total Reward = 4.0\n",
      "Episode 23, Total Reward = 13.0\n",
      "Episode 24, Total Reward = 6.0\n",
      "Episode 25, Total Reward = 7.0\n",
      "Episode 26, Total Reward = 6.0\n",
      "Episode 27, Total Reward = 5.0\n",
      "Episode 28, Total Reward = 16.0\n",
      "Episode 29, Total Reward = 5.0\n",
      "Episode 30, Total Reward = 10.0\n",
      "Episode 31, Total Reward = 19.0\n",
      "Episode 32, Total Reward = 7.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 6.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 6.0\n",
      "Episode 37, Total Reward = 4.0\n",
      "Episode 38, Total Reward = 20.0\n",
      "Episode 39, Total Reward = 5.0\n",
      "Episode 40, Total Reward = 6.0\n",
      "Episode 41, Total Reward = 18.0\n",
      "Episode 42, Total Reward = 6.0\n",
      "Episode 43, Total Reward = 10.0\n",
      "Episode 44, Total Reward = 10.0\n",
      "Episode 45, Total Reward = 5.0\n",
      "Episode 46, Total Reward = 7.0\n",
      "Episode 47, Total Reward = 15.0\n",
      "Episode 48, Total Reward = 6.0\n",
      "Episode 49, Total Reward = 11.0\n",
      "Episode 50, Total Reward = 20.0\n",
      "Episode 51, Total Reward = 5.0\n",
      "Episode 52, Total Reward = 16.0\n",
      "Episode 53, Total Reward = 11.0\n",
      "Episode 54, Total Reward = 9.0\n",
      "Episode 55, Total Reward = 6.0\n",
      "Episode 56, Total Reward = 4.0\n",
      "Episode 57, Total Reward = 16.0\n",
      "Episode 58, Total Reward = 9.0\n",
      "Episode 59, Total Reward = 4.0\n",
      "Episode 60, Total Reward = 6.0\n",
      "Episode 61, Total Reward = 9.0\n",
      "Episode 62, Total Reward = 9.0\n",
      "Episode 63, Total Reward = 8.0\n",
      "Episode 64, Total Reward = 14.0\n",
      "Episode 65, Total Reward = 12.0\n",
      "Episode 66, Total Reward = 21.0\n",
      "Episode 67, Total Reward = 6.0\n",
      "Episode 68, Total Reward = 8.0\n",
      "Episode 69, Total Reward = 6.0\n",
      "Episode 70, Total Reward = 18.0\n",
      "Episode 71, Total Reward = 13.0\n",
      "Episode 72, Total Reward = 5.0\n",
      "Episode 73, Total Reward = 28.0\n",
      "Episode 74, Total Reward = 16.0\n",
      "Episode 75, Total Reward = 32.0\n",
      "Episode 76, Total Reward = 7.0\n",
      "Episode 77, Total Reward = 52.0\n",
      "Episode 78, Total Reward = 19.0\n",
      "Episode 79, Total Reward = 15.0\n",
      "Episode 80, Total Reward = 5.0\n",
      "Episode 81, Total Reward = 7.0\n",
      "Episode 82, Total Reward = 4.0\n",
      "Episode 83, Total Reward = 20.0\n",
      "Episode 84, Total Reward = 16.0\n",
      "Episode 85, Total Reward = 14.0\n",
      "Episode 86, Total Reward = 16.0\n",
      "Episode 87, Total Reward = 4.0\n",
      "Episode 88, Total Reward = 16.0\n",
      "Episode 89, Total Reward = 8.0\n",
      "Episode 90, Total Reward = 12.0\n",
      "Episode 91, Total Reward = 23.0\n",
      "Episode 92, Total Reward = 9.0\n",
      "Episode 93, Total Reward = 15.0\n",
      "Episode 94, Total Reward = 12.0\n",
      "Episode 95, Total Reward = 27.0\n",
      "Episode 96, Total Reward = 7.0\n",
      "Episode 97, Total Reward = 21.0\n",
      "Episode 98, Total Reward = 5.0\n",
      "Episode 99, Total Reward = 9.0\n",
      "Episode 100, Total Reward = 10.0\n",
      "Running experiment with lr=0.0001, epsilon=0.3, k_epochs=10, batch_size=128\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 11.0\n",
      "Episode 3, Total Reward = 7.0\n",
      "Episode 4, Total Reward = 4.0\n",
      "Episode 5, Total Reward = 6.0\n",
      "Episode 6, Total Reward = 16.0\n",
      "Episode 7, Total Reward = 4.0\n",
      "Episode 8, Total Reward = 5.0\n",
      "Episode 9, Total Reward = 23.0\n",
      "Episode 10, Total Reward = 9.0\n",
      "Episode 11, Total Reward = 9.0\n",
      "Episode 12, Total Reward = 4.0\n",
      "Episode 13, Total Reward = 10.0\n",
      "Episode 14, Total Reward = 12.0\n",
      "Episode 15, Total Reward = 18.0\n",
      "Episode 16, Total Reward = 3.0\n",
      "Episode 17, Total Reward = 6.0\n",
      "Episode 18, Total Reward = 12.0\n",
      "Episode 19, Total Reward = 11.0\n",
      "Episode 20, Total Reward = 6.0\n",
      "Episode 21, Total Reward = 11.0\n",
      "Episode 22, Total Reward = 7.0\n",
      "Episode 23, Total Reward = 18.0\n",
      "Episode 24, Total Reward = 14.0\n",
      "Episode 25, Total Reward = 12.0\n",
      "Episode 26, Total Reward = 7.0\n",
      "Episode 27, Total Reward = 5.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 11.0\n",
      "Episode 30, Total Reward = 14.0\n",
      "Episode 31, Total Reward = 13.0\n",
      "Episode 32, Total Reward = 10.0\n",
      "Episode 33, Total Reward = 18.0\n",
      "Episode 34, Total Reward = 7.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 5.0\n",
      "Episode 37, Total Reward = 8.0\n",
      "Episode 38, Total Reward = 9.0\n",
      "Episode 39, Total Reward = 14.0\n",
      "Episode 40, Total Reward = 7.0\n",
      "Episode 41, Total Reward = 18.0\n",
      "Episode 42, Total Reward = 4.0\n",
      "Episode 43, Total Reward = 7.0\n",
      "Episode 44, Total Reward = 17.0\n",
      "Episode 45, Total Reward = 6.0\n",
      "Episode 46, Total Reward = 6.0\n",
      "Episode 47, Total Reward = 4.0\n",
      "Episode 48, Total Reward = 23.0\n",
      "Episode 49, Total Reward = 5.0\n",
      "Episode 50, Total Reward = 6.0\n",
      "Episode 51, Total Reward = 17.0\n",
      "Episode 52, Total Reward = 6.0\n",
      "Episode 53, Total Reward = 16.0\n",
      "Episode 54, Total Reward = 17.0\n",
      "Episode 55, Total Reward = 13.0\n",
      "Episode 56, Total Reward = 7.0\n",
      "Episode 57, Total Reward = 14.0\n",
      "Episode 58, Total Reward = 41.0\n",
      "Episode 59, Total Reward = 7.0\n",
      "Episode 60, Total Reward = 11.0\n",
      "Episode 61, Total Reward = 6.0\n",
      "Episode 62, Total Reward = 3.0\n",
      "Episode 63, Total Reward = 20.0\n",
      "Episode 64, Total Reward = 9.0\n",
      "Episode 65, Total Reward = 7.0\n",
      "Episode 66, Total Reward = 12.0\n",
      "Episode 67, Total Reward = 24.0\n",
      "Episode 68, Total Reward = 9.0\n",
      "Episode 69, Total Reward = 8.0\n",
      "Episode 70, Total Reward = 9.0\n",
      "Episode 71, Total Reward = 13.0\n",
      "Episode 72, Total Reward = 6.0\n",
      "Episode 73, Total Reward = 13.0\n",
      "Episode 74, Total Reward = 6.0\n",
      "Episode 75, Total Reward = 6.0\n",
      "Episode 76, Total Reward = 7.0\n",
      "Episode 77, Total Reward = 11.0\n",
      "Episode 78, Total Reward = 7.0\n",
      "Episode 79, Total Reward = 7.0\n",
      "Episode 80, Total Reward = 15.0\n",
      "Episode 81, Total Reward = 10.0\n",
      "Episode 82, Total Reward = 13.0\n",
      "Episode 83, Total Reward = 22.0\n",
      "Episode 84, Total Reward = 10.0\n",
      "Episode 85, Total Reward = 19.0\n",
      "Episode 86, Total Reward = 14.0\n",
      "Episode 87, Total Reward = 14.0\n",
      "Episode 88, Total Reward = 8.0\n",
      "Episode 89, Total Reward = 38.0\n",
      "Episode 90, Total Reward = 10.0\n",
      "Episode 91, Total Reward = 17.0\n",
      "Episode 92, Total Reward = 10.0\n",
      "Episode 93, Total Reward = 14.0\n",
      "Episode 94, Total Reward = 17.0\n",
      "Episode 95, Total Reward = 9.0\n",
      "Episode 96, Total Reward = 52.0\n",
      "Episode 97, Total Reward = 32.0\n",
      "Episode 98, Total Reward = 18.0\n",
      "Episode 99, Total Reward = 29.0\n",
      "Episode 100, Total Reward = 11.0\n",
      "Running experiment with lr=0.0001, epsilon=0.3, k_epochs=20, batch_size=32\n",
      "Episode 1, Total Reward = 5.0\n",
      "Episode 2, Total Reward = 19.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 13.0\n",
      "Episode 5, Total Reward = 21.0\n",
      "Episode 6, Total Reward = 4.0\n",
      "Episode 7, Total Reward = 4.0\n",
      "Episode 8, Total Reward = 8.0\n",
      "Episode 9, Total Reward = 14.0\n",
      "Episode 10, Total Reward = 8.0\n",
      "Episode 11, Total Reward = 8.0\n",
      "Episode 12, Total Reward = 4.0\n",
      "Episode 13, Total Reward = 4.0\n",
      "Episode 14, Total Reward = 11.0\n",
      "Episode 15, Total Reward = 8.0\n",
      "Episode 16, Total Reward = 9.0\n",
      "Episode 17, Total Reward = 10.0\n",
      "Episode 18, Total Reward = 10.0\n",
      "Episode 19, Total Reward = 5.0\n",
      "Episode 20, Total Reward = 8.0\n",
      "Episode 21, Total Reward = 10.0\n",
      "Episode 22, Total Reward = 12.0\n",
      "Episode 23, Total Reward = 17.0\n",
      "Episode 24, Total Reward = 26.0\n",
      "Episode 25, Total Reward = 8.0\n",
      "Episode 26, Total Reward = 4.0\n",
      "Episode 27, Total Reward = 23.0\n",
      "Episode 28, Total Reward = 20.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 9.0\n",
      "Episode 31, Total Reward = 17.0\n",
      "Episode 32, Total Reward = 37.0\n",
      "Episode 33, Total Reward = 25.0\n",
      "Episode 34, Total Reward = 8.0\n",
      "Episode 35, Total Reward = 53.0\n",
      "Episode 36, Total Reward = 8.0\n",
      "Episode 37, Total Reward = 35.0\n",
      "Episode 38, Total Reward = 6.0\n",
      "Episode 39, Total Reward = 14.0\n",
      "Episode 40, Total Reward = 22.0\n",
      "Episode 41, Total Reward = 13.0\n",
      "Episode 42, Total Reward = 24.0\n",
      "Episode 43, Total Reward = 29.0\n",
      "Episode 44, Total Reward = 26.0\n",
      "Episode 45, Total Reward = 23.0\n",
      "Episode 46, Total Reward = 28.0\n",
      "Episode 47, Total Reward = 64.0\n",
      "Episode 48, Total Reward = 63.0\n",
      "Episode 49, Total Reward = 12.0\n",
      "Episode 50, Total Reward = 24.0\n",
      "Episode 51, Total Reward = 17.0\n",
      "Episode 52, Total Reward = 30.0\n",
      "Episode 53, Total Reward = 38.0\n",
      "Episode 54, Total Reward = 11.0\n",
      "Episode 55, Total Reward = 61.0\n",
      "Episode 56, Total Reward = 17.0\n",
      "Episode 57, Total Reward = 34.0\n",
      "Episode 58, Total Reward = 20.0\n",
      "Episode 59, Total Reward = 21.0\n",
      "Episode 60, Total Reward = 19.0\n",
      "Episode 61, Total Reward = 41.0\n",
      "Episode 62, Total Reward = 89.0\n",
      "Episode 63, Total Reward = 68.0\n",
      "Episode 64, Total Reward = 67.0\n",
      "Episode 65, Total Reward = 17.0\n",
      "Episode 66, Total Reward = 97.0\n",
      "Episode 67, Total Reward = 24.0\n",
      "Episode 68, Total Reward = 49.0\n",
      "Episode 69, Total Reward = 41.0\n",
      "Episode 70, Total Reward = 53.0\n",
      "Episode 71, Total Reward = 62.0\n",
      "Episode 72, Total Reward = 40.0\n",
      "Episode 73, Total Reward = 41.0\n",
      "Episode 74, Total Reward = 20.0\n",
      "Episode 75, Total Reward = 18.0\n",
      "Episode 76, Total Reward = 27.0\n",
      "Episode 77, Total Reward = 38.0\n",
      "Episode 78, Total Reward = 68.0\n",
      "Episode 79, Total Reward = 45.0\n",
      "Episode 80, Total Reward = 17.0\n",
      "Episode 81, Total Reward = 26.0\n",
      "Episode 82, Total Reward = 28.0\n",
      "Episode 83, Total Reward = 103.0\n",
      "Episode 84, Total Reward = 51.0\n",
      "Episode 85, Total Reward = 21.0\n",
      "Episode 86, Total Reward = 35.0\n",
      "Episode 87, Total Reward = 49.0\n",
      "Episode 88, Total Reward = 49.0\n",
      "Episode 89, Total Reward = 131.0\n",
      "Episode 90, Total Reward = 43.0\n",
      "Episode 91, Total Reward = 34.0\n",
      "Episode 92, Total Reward = 64.0\n",
      "Episode 93, Total Reward = 50.0\n",
      "Episode 94, Total Reward = 152.0\n",
      "Episode 95, Total Reward = 100.0\n",
      "Episode 96, Total Reward = 107.0\n",
      "Episode 97, Total Reward = 44.0\n",
      "Episode 98, Total Reward = 80.0\n",
      "Episode 99, Total Reward = 46.0\n",
      "Episode 100, Total Reward = 48.0\n",
      "Running experiment with lr=0.0001, epsilon=0.3, k_epochs=20, batch_size=64\n",
      "Episode 1, Total Reward = 4.0\n",
      "Episode 2, Total Reward = 4.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 11.0\n",
      "Episode 5, Total Reward = 10.0\n",
      "Episode 6, Total Reward = 23.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 3.0\n",
      "Episode 9, Total Reward = 16.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 14.0\n",
      "Episode 12, Total Reward = 6.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 13.0\n",
      "Episode 15, Total Reward = 5.0\n",
      "Episode 16, Total Reward = 14.0\n",
      "Episode 17, Total Reward = 14.0\n",
      "Episode 18, Total Reward = 6.0\n",
      "Episode 19, Total Reward = 16.0\n",
      "Episode 20, Total Reward = 12.0\n",
      "Episode 21, Total Reward = 12.0\n",
      "Episode 22, Total Reward = 11.0\n",
      "Episode 23, Total Reward = 11.0\n",
      "Episode 24, Total Reward = 8.0\n",
      "Episode 25, Total Reward = 11.0\n",
      "Episode 26, Total Reward = 19.0\n",
      "Episode 27, Total Reward = 14.0\n",
      "Episode 28, Total Reward = 29.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 12.0\n",
      "Episode 31, Total Reward = 12.0\n",
      "Episode 32, Total Reward = 20.0\n",
      "Episode 33, Total Reward = 29.0\n",
      "Episode 34, Total Reward = 11.0\n",
      "Episode 35, Total Reward = 12.0\n",
      "Episode 36, Total Reward = 18.0\n",
      "Episode 37, Total Reward = 12.0\n",
      "Episode 38, Total Reward = 10.0\n",
      "Episode 39, Total Reward = 21.0\n",
      "Episode 40, Total Reward = 7.0\n",
      "Episode 41, Total Reward = 22.0\n",
      "Episode 42, Total Reward = 16.0\n",
      "Episode 43, Total Reward = 11.0\n",
      "Episode 44, Total Reward = 16.0\n",
      "Episode 45, Total Reward = 12.0\n",
      "Episode 46, Total Reward = 14.0\n",
      "Episode 47, Total Reward = 9.0\n",
      "Episode 48, Total Reward = 15.0\n",
      "Episode 49, Total Reward = 19.0\n",
      "Episode 50, Total Reward = 19.0\n",
      "Episode 51, Total Reward = 13.0\n",
      "Episode 52, Total Reward = 39.0\n",
      "Episode 53, Total Reward = 24.0\n",
      "Episode 54, Total Reward = 10.0\n",
      "Episode 55, Total Reward = 16.0\n",
      "Episode 56, Total Reward = 26.0\n",
      "Episode 57, Total Reward = 34.0\n",
      "Episode 58, Total Reward = 10.0\n",
      "Episode 59, Total Reward = 13.0\n",
      "Episode 60, Total Reward = 52.0\n",
      "Episode 61, Total Reward = 37.0\n",
      "Episode 62, Total Reward = 14.0\n",
      "Episode 63, Total Reward = 13.0\n",
      "Episode 64, Total Reward = 25.0\n",
      "Episode 65, Total Reward = 52.0\n",
      "Episode 66, Total Reward = 32.0\n",
      "Episode 67, Total Reward = 40.0\n",
      "Episode 68, Total Reward = 20.0\n",
      "Episode 69, Total Reward = 36.0\n",
      "Episode 70, Total Reward = 30.0\n",
      "Episode 71, Total Reward = 25.0\n",
      "Episode 72, Total Reward = 77.0\n",
      "Episode 73, Total Reward = 22.0\n",
      "Episode 74, Total Reward = 43.0\n",
      "Episode 75, Total Reward = 31.0\n",
      "Episode 76, Total Reward = 24.0\n",
      "Episode 77, Total Reward = 34.0\n",
      "Episode 78, Total Reward = 16.0\n",
      "Episode 79, Total Reward = 55.0\n",
      "Episode 80, Total Reward = 108.0\n",
      "Episode 81, Total Reward = 44.0\n",
      "Episode 82, Total Reward = 50.0\n",
      "Episode 83, Total Reward = 45.0\n",
      "Episode 84, Total Reward = 84.0\n",
      "Episode 85, Total Reward = 49.0\n",
      "Episode 86, Total Reward = 83.0\n",
      "Episode 87, Total Reward = 53.0\n",
      "Episode 88, Total Reward = 30.0\n",
      "Episode 89, Total Reward = 40.0\n",
      "Episode 90, Total Reward = 28.0\n",
      "Episode 91, Total Reward = 25.0\n",
      "Episode 92, Total Reward = 19.0\n",
      "Episode 93, Total Reward = 53.0\n",
      "Episode 94, Total Reward = 58.0\n",
      "Episode 95, Total Reward = 50.0\n",
      "Episode 96, Total Reward = 97.0\n",
      "Episode 97, Total Reward = 49.0\n",
      "Episode 98, Total Reward = 86.0\n",
      "Episode 99, Total Reward = 13.0\n",
      "Episode 100, Total Reward = 15.0\n",
      "Running experiment with lr=0.0001, epsilon=0.3, k_epochs=20, batch_size=128\n",
      "Episode 1, Total Reward = 3.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 16.0\n",
      "Episode 5, Total Reward = 8.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 6.0\n",
      "Episode 8, Total Reward = 6.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 9.0\n",
      "Episode 11, Total Reward = 17.0\n",
      "Episode 12, Total Reward = 4.0\n",
      "Episode 13, Total Reward = 6.0\n",
      "Episode 14, Total Reward = 5.0\n",
      "Episode 15, Total Reward = 11.0\n",
      "Episode 16, Total Reward = 22.0\n",
      "Episode 17, Total Reward = 12.0\n",
      "Episode 18, Total Reward = 14.0\n",
      "Episode 19, Total Reward = 13.0\n",
      "Episode 20, Total Reward = 5.0\n",
      "Episode 21, Total Reward = 4.0\n",
      "Episode 22, Total Reward = 17.0\n",
      "Episode 23, Total Reward = 10.0\n",
      "Episode 24, Total Reward = 24.0\n",
      "Episode 25, Total Reward = 11.0\n",
      "Episode 26, Total Reward = 17.0\n",
      "Episode 27, Total Reward = 5.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 10.0\n",
      "Episode 30, Total Reward = 8.0\n",
      "Episode 31, Total Reward = 11.0\n",
      "Episode 32, Total Reward = 18.0\n",
      "Episode 33, Total Reward = 5.0\n",
      "Episode 34, Total Reward = 10.0\n",
      "Episode 35, Total Reward = 25.0\n",
      "Episode 36, Total Reward = 43.0\n",
      "Episode 37, Total Reward = 12.0\n",
      "Episode 38, Total Reward = 7.0\n",
      "Episode 39, Total Reward = 11.0\n",
      "Episode 40, Total Reward = 16.0\n",
      "Episode 41, Total Reward = 18.0\n",
      "Episode 42, Total Reward = 31.0\n",
      "Episode 43, Total Reward = 7.0\n",
      "Episode 44, Total Reward = 19.0\n",
      "Episode 45, Total Reward = 10.0\n",
      "Episode 46, Total Reward = 16.0\n",
      "Episode 47, Total Reward = 8.0\n",
      "Episode 48, Total Reward = 8.0\n",
      "Episode 49, Total Reward = 21.0\n",
      "Episode 50, Total Reward = 10.0\n",
      "Episode 51, Total Reward = 10.0\n",
      "Episode 52, Total Reward = 36.0\n",
      "Episode 53, Total Reward = 11.0\n",
      "Episode 54, Total Reward = 34.0\n",
      "Episode 55, Total Reward = 30.0\n",
      "Episode 56, Total Reward = 23.0\n",
      "Episode 57, Total Reward = 21.0\n",
      "Episode 58, Total Reward = 8.0\n",
      "Episode 59, Total Reward = 16.0\n",
      "Episode 60, Total Reward = 15.0\n",
      "Episode 61, Total Reward = 30.0\n",
      "Episode 62, Total Reward = 25.0\n",
      "Episode 63, Total Reward = 39.0\n",
      "Episode 64, Total Reward = 45.0\n",
      "Episode 65, Total Reward = 29.0\n",
      "Episode 66, Total Reward = 29.0\n",
      "Episode 67, Total Reward = 38.0\n",
      "Episode 68, Total Reward = 32.0\n",
      "Episode 69, Total Reward = 8.0\n",
      "Episode 70, Total Reward = 22.0\n",
      "Episode 71, Total Reward = 11.0\n",
      "Episode 72, Total Reward = 72.0\n",
      "Episode 73, Total Reward = 30.0\n",
      "Episode 74, Total Reward = 51.0\n",
      "Episode 75, Total Reward = 47.0\n",
      "Episode 76, Total Reward = 34.0\n",
      "Episode 77, Total Reward = 50.0\n",
      "Episode 78, Total Reward = 36.0\n",
      "Episode 79, Total Reward = 26.0\n",
      "Episode 80, Total Reward = 16.0\n",
      "Episode 81, Total Reward = 31.0\n",
      "Episode 82, Total Reward = 25.0\n",
      "Episode 83, Total Reward = 18.0\n",
      "Episode 84, Total Reward = 45.0\n",
      "Episode 85, Total Reward = 64.0\n",
      "Episode 86, Total Reward = 50.0\n",
      "Episode 87, Total Reward = 75.0\n",
      "Episode 88, Total Reward = 39.0\n",
      "Episode 89, Total Reward = 61.0\n",
      "Episode 90, Total Reward = 44.0\n",
      "Episode 91, Total Reward = 15.0\n",
      "Episode 92, Total Reward = 32.0\n",
      "Episode 93, Total Reward = 21.0\n",
      "Episode 94, Total Reward = 68.0\n",
      "Episode 95, Total Reward = 36.0\n",
      "Episode 96, Total Reward = 147.0\n",
      "Episode 97, Total Reward = 49.0\n",
      "Episode 98, Total Reward = 29.0\n",
      "Episode 99, Total Reward = 30.0\n",
      "Episode 100, Total Reward = 26.0\n",
      "Running experiment with lr=0.0003, epsilon=0.1, k_epochs=5, batch_size=32\n",
      "Episode 1, Total Reward = 8.0\n",
      "Episode 2, Total Reward = 8.0\n",
      "Episode 3, Total Reward = 10.0\n",
      "Episode 4, Total Reward = 16.0\n",
      "Episode 5, Total Reward = 5.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 4.0\n",
      "Episode 8, Total Reward = 20.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 14.0\n",
      "Episode 11, Total Reward = 11.0\n",
      "Episode 12, Total Reward = 11.0\n",
      "Episode 13, Total Reward = 6.0\n",
      "Episode 14, Total Reward = 15.0\n",
      "Episode 15, Total Reward = 11.0\n",
      "Episode 16, Total Reward = 7.0\n",
      "Episode 17, Total Reward = 5.0\n",
      "Episode 18, Total Reward = 4.0\n",
      "Episode 19, Total Reward = 24.0\n",
      "Episode 20, Total Reward = 17.0\n",
      "Episode 21, Total Reward = 10.0\n",
      "Episode 22, Total Reward = 16.0\n",
      "Episode 23, Total Reward = 8.0\n",
      "Episode 24, Total Reward = 3.0\n",
      "Episode 25, Total Reward = 7.0\n",
      "Episode 26, Total Reward = 20.0\n",
      "Episode 27, Total Reward = 6.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 8.0\n",
      "Episode 30, Total Reward = 27.0\n",
      "Episode 31, Total Reward = 6.0\n",
      "Episode 32, Total Reward = 4.0\n",
      "Episode 33, Total Reward = 6.0\n",
      "Episode 34, Total Reward = 8.0\n",
      "Episode 35, Total Reward = 7.0\n",
      "Episode 36, Total Reward = 15.0\n",
      "Episode 37, Total Reward = 5.0\n",
      "Episode 38, Total Reward = 7.0\n",
      "Episode 39, Total Reward = 31.0\n",
      "Episode 40, Total Reward = 6.0\n",
      "Episode 41, Total Reward = 19.0\n",
      "Episode 42, Total Reward = 6.0\n",
      "Episode 43, Total Reward = 19.0\n",
      "Episode 44, Total Reward = 10.0\n",
      "Episode 45, Total Reward = 20.0\n",
      "Episode 46, Total Reward = 8.0\n",
      "Episode 47, Total Reward = 6.0\n",
      "Episode 48, Total Reward = 16.0\n",
      "Episode 49, Total Reward = 4.0\n",
      "Episode 50, Total Reward = 12.0\n",
      "Episode 51, Total Reward = 28.0\n",
      "Episode 52, Total Reward = 8.0\n",
      "Episode 53, Total Reward = 8.0\n",
      "Episode 54, Total Reward = 6.0\n",
      "Episode 55, Total Reward = 23.0\n",
      "Episode 56, Total Reward = 28.0\n",
      "Episode 57, Total Reward = 9.0\n",
      "Episode 58, Total Reward = 17.0\n",
      "Episode 59, Total Reward = 24.0\n",
      "Episode 60, Total Reward = 36.0\n",
      "Episode 61, Total Reward = 9.0\n",
      "Episode 62, Total Reward = 5.0\n",
      "Episode 63, Total Reward = 18.0\n",
      "Episode 64, Total Reward = 27.0\n",
      "Episode 65, Total Reward = 15.0\n",
      "Episode 66, Total Reward = 13.0\n",
      "Episode 67, Total Reward = 37.0\n",
      "Episode 68, Total Reward = 30.0\n",
      "Episode 69, Total Reward = 8.0\n",
      "Episode 70, Total Reward = 5.0\n",
      "Episode 71, Total Reward = 17.0\n",
      "Episode 72, Total Reward = 7.0\n",
      "Episode 73, Total Reward = 15.0\n",
      "Episode 74, Total Reward = 20.0\n",
      "Episode 75, Total Reward = 30.0\n",
      "Episode 76, Total Reward = 11.0\n",
      "Episode 77, Total Reward = 58.0\n",
      "Episode 78, Total Reward = 62.0\n",
      "Episode 79, Total Reward = 39.0\n",
      "Episode 80, Total Reward = 17.0\n",
      "Episode 81, Total Reward = 21.0\n",
      "Episode 82, Total Reward = 20.0\n",
      "Episode 83, Total Reward = 75.0\n",
      "Episode 84, Total Reward = 31.0\n",
      "Episode 85, Total Reward = 59.0\n",
      "Episode 86, Total Reward = 48.0\n",
      "Episode 87, Total Reward = 24.0\n",
      "Episode 88, Total Reward = 17.0\n",
      "Episode 89, Total Reward = 19.0\n",
      "Episode 90, Total Reward = 70.0\n",
      "Episode 91, Total Reward = 33.0\n",
      "Episode 92, Total Reward = 47.0\n",
      "Episode 93, Total Reward = 17.0\n",
      "Episode 94, Total Reward = 12.0\n",
      "Episode 95, Total Reward = 8.0\n",
      "Episode 96, Total Reward = 99.0\n",
      "Episode 97, Total Reward = 74.0\n",
      "Episode 98, Total Reward = 44.0\n",
      "Episode 99, Total Reward = 33.0\n",
      "Episode 100, Total Reward = 92.0\n",
      "Running experiment with lr=0.0003, epsilon=0.1, k_epochs=5, batch_size=64\n",
      "Episode 1, Total Reward = 8.0\n",
      "Episode 2, Total Reward = 8.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 4.0\n",
      "Episode 5, Total Reward = 8.0\n",
      "Episode 6, Total Reward = 4.0\n",
      "Episode 7, Total Reward = 4.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 9.0\n",
      "Episode 10, Total Reward = 12.0\n",
      "Episode 11, Total Reward = 15.0\n",
      "Episode 12, Total Reward = 11.0\n",
      "Episode 13, Total Reward = 4.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 12.0\n",
      "Episode 17, Total Reward = 6.0\n",
      "Episode 18, Total Reward = 10.0\n",
      "Episode 19, Total Reward = 8.0\n",
      "Episode 20, Total Reward = 4.0\n",
      "Episode 21, Total Reward = 12.0\n",
      "Episode 22, Total Reward = 5.0\n",
      "Episode 23, Total Reward = 5.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 7.0\n",
      "Episode 26, Total Reward = 14.0\n",
      "Episode 27, Total Reward = 25.0\n",
      "Episode 28, Total Reward = 10.0\n",
      "Episode 29, Total Reward = 5.0\n",
      "Episode 30, Total Reward = 6.0\n",
      "Episode 31, Total Reward = 6.0\n",
      "Episode 32, Total Reward = 8.0\n",
      "Episode 33, Total Reward = 6.0\n",
      "Episode 34, Total Reward = 9.0\n",
      "Episode 35, Total Reward = 8.0\n",
      "Episode 36, Total Reward = 6.0\n",
      "Episode 37, Total Reward = 10.0\n",
      "Episode 38, Total Reward = 5.0\n",
      "Episode 39, Total Reward = 23.0\n",
      "Episode 40, Total Reward = 6.0\n",
      "Episode 41, Total Reward = 13.0\n",
      "Episode 42, Total Reward = 24.0\n",
      "Episode 43, Total Reward = 7.0\n",
      "Episode 44, Total Reward = 26.0\n",
      "Episode 45, Total Reward = 6.0\n",
      "Episode 46, Total Reward = 9.0\n",
      "Episode 47, Total Reward = 12.0\n",
      "Episode 48, Total Reward = 25.0\n",
      "Episode 49, Total Reward = 14.0\n",
      "Episode 50, Total Reward = 13.0\n",
      "Episode 51, Total Reward = 8.0\n",
      "Episode 52, Total Reward = 6.0\n",
      "Episode 53, Total Reward = 6.0\n",
      "Episode 54, Total Reward = 5.0\n",
      "Episode 55, Total Reward = 7.0\n",
      "Episode 56, Total Reward = 8.0\n",
      "Episode 57, Total Reward = 12.0\n",
      "Episode 58, Total Reward = 5.0\n",
      "Episode 59, Total Reward = 6.0\n",
      "Episode 60, Total Reward = 11.0\n",
      "Episode 61, Total Reward = 12.0\n",
      "Episode 62, Total Reward = 17.0\n",
      "Episode 63, Total Reward = 16.0\n",
      "Episode 64, Total Reward = 36.0\n",
      "Episode 65, Total Reward = 13.0\n",
      "Episode 66, Total Reward = 6.0\n",
      "Episode 67, Total Reward = 7.0\n",
      "Episode 68, Total Reward = 11.0\n",
      "Episode 69, Total Reward = 7.0\n",
      "Episode 70, Total Reward = 11.0\n",
      "Episode 71, Total Reward = 29.0\n",
      "Episode 72, Total Reward = 13.0\n",
      "Episode 73, Total Reward = 43.0\n",
      "Episode 74, Total Reward = 11.0\n",
      "Episode 75, Total Reward = 14.0\n",
      "Episode 76, Total Reward = 21.0\n",
      "Episode 77, Total Reward = 23.0\n",
      "Episode 78, Total Reward = 6.0\n",
      "Episode 79, Total Reward = 7.0\n",
      "Episode 80, Total Reward = 7.0\n",
      "Episode 81, Total Reward = 14.0\n",
      "Episode 82, Total Reward = 9.0\n",
      "Episode 83, Total Reward = 13.0\n",
      "Episode 84, Total Reward = 22.0\n",
      "Episode 85, Total Reward = 38.0\n",
      "Episode 86, Total Reward = 9.0\n",
      "Episode 87, Total Reward = 12.0\n",
      "Episode 88, Total Reward = 18.0\n",
      "Episode 89, Total Reward = 21.0\n",
      "Episode 90, Total Reward = 13.0\n",
      "Episode 91, Total Reward = 42.0\n",
      "Episode 92, Total Reward = 7.0\n",
      "Episode 93, Total Reward = 19.0\n",
      "Episode 94, Total Reward = 23.0\n",
      "Episode 95, Total Reward = 19.0\n",
      "Episode 96, Total Reward = 12.0\n",
      "Episode 97, Total Reward = 25.0\n",
      "Episode 98, Total Reward = 15.0\n",
      "Episode 99, Total Reward = 32.0\n",
      "Episode 100, Total Reward = 23.0\n",
      "Running experiment with lr=0.0003, epsilon=0.1, k_epochs=5, batch_size=128\n",
      "Episode 1, Total Reward = 14.0\n",
      "Episode 2, Total Reward = 12.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 7.0\n",
      "Episode 6, Total Reward = 8.0\n",
      "Episode 7, Total Reward = 7.0\n",
      "Episode 8, Total Reward = 11.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 10.0\n",
      "Episode 12, Total Reward = 7.0\n",
      "Episode 13, Total Reward = 4.0\n",
      "Episode 14, Total Reward = 8.0\n",
      "Episode 15, Total Reward = 5.0\n",
      "Episode 16, Total Reward = 7.0\n",
      "Episode 17, Total Reward = 8.0\n",
      "Episode 18, Total Reward = 6.0\n",
      "Episode 19, Total Reward = 11.0\n",
      "Episode 20, Total Reward = 11.0\n",
      "Episode 21, Total Reward = 8.0\n",
      "Episode 22, Total Reward = 11.0\n",
      "Episode 23, Total Reward = 6.0\n",
      "Episode 24, Total Reward = 10.0\n",
      "Episode 25, Total Reward = 25.0\n",
      "Episode 26, Total Reward = 5.0\n",
      "Episode 27, Total Reward = 15.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 8.0\n",
      "Episode 30, Total Reward = 15.0\n",
      "Episode 31, Total Reward = 19.0\n",
      "Episode 32, Total Reward = 16.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 17.0\n",
      "Episode 35, Total Reward = 8.0\n",
      "Episode 36, Total Reward = 4.0\n",
      "Episode 37, Total Reward = 8.0\n",
      "Episode 38, Total Reward = 10.0\n",
      "Episode 39, Total Reward = 24.0\n",
      "Episode 40, Total Reward = 15.0\n",
      "Episode 41, Total Reward = 11.0\n",
      "Episode 42, Total Reward = 9.0\n",
      "Episode 43, Total Reward = 22.0\n",
      "Episode 44, Total Reward = 28.0\n",
      "Episode 45, Total Reward = 13.0\n",
      "Episode 46, Total Reward = 28.0\n",
      "Episode 47, Total Reward = 24.0\n",
      "Episode 48, Total Reward = 11.0\n",
      "Episode 49, Total Reward = 18.0\n",
      "Episode 50, Total Reward = 41.0\n",
      "Episode 51, Total Reward = 7.0\n",
      "Episode 52, Total Reward = 21.0\n",
      "Episode 53, Total Reward = 9.0\n",
      "Episode 54, Total Reward = 11.0\n",
      "Episode 55, Total Reward = 7.0\n",
      "Episode 56, Total Reward = 14.0\n",
      "Episode 57, Total Reward = 31.0\n",
      "Episode 58, Total Reward = 16.0\n",
      "Episode 59, Total Reward = 20.0\n",
      "Episode 60, Total Reward = 9.0\n",
      "Episode 61, Total Reward = 16.0\n",
      "Episode 62, Total Reward = 18.0\n",
      "Episode 63, Total Reward = 8.0\n",
      "Episode 64, Total Reward = 10.0\n",
      "Episode 65, Total Reward = 31.0\n",
      "Episode 66, Total Reward = 21.0\n",
      "Episode 67, Total Reward = 32.0\n",
      "Episode 68, Total Reward = 17.0\n",
      "Episode 69, Total Reward = 10.0\n",
      "Episode 70, Total Reward = 28.0\n",
      "Episode 71, Total Reward = 55.0\n",
      "Episode 72, Total Reward = 12.0\n",
      "Episode 73, Total Reward = 34.0\n",
      "Episode 74, Total Reward = 17.0\n",
      "Episode 75, Total Reward = 10.0\n",
      "Episode 76, Total Reward = 26.0\n",
      "Episode 77, Total Reward = 25.0\n",
      "Episode 78, Total Reward = 31.0\n",
      "Episode 79, Total Reward = 15.0\n",
      "Episode 80, Total Reward = 62.0\n",
      "Episode 81, Total Reward = 10.0\n",
      "Episode 82, Total Reward = 35.0\n",
      "Episode 83, Total Reward = 37.0\n",
      "Episode 84, Total Reward = 24.0\n",
      "Episode 85, Total Reward = 15.0\n",
      "Episode 86, Total Reward = 18.0\n",
      "Episode 87, Total Reward = 35.0\n",
      "Episode 88, Total Reward = 24.0\n",
      "Episode 89, Total Reward = 13.0\n",
      "Episode 90, Total Reward = 12.0\n",
      "Episode 91, Total Reward = 47.0\n",
      "Episode 92, Total Reward = 22.0\n",
      "Episode 93, Total Reward = 13.0\n",
      "Episode 94, Total Reward = 47.0\n",
      "Episode 95, Total Reward = 35.0\n",
      "Episode 96, Total Reward = 31.0\n",
      "Episode 97, Total Reward = 21.0\n",
      "Episode 98, Total Reward = 26.0\n",
      "Episode 99, Total Reward = 33.0\n",
      "Episode 100, Total Reward = 58.0\n",
      "Running experiment with lr=0.0003, epsilon=0.1, k_epochs=10, batch_size=32\n",
      "Episode 1, Total Reward = 16.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 5.0\n",
      "Episode 4, Total Reward = 9.0\n",
      "Episode 5, Total Reward = 8.0\n",
      "Episode 6, Total Reward = 5.0\n",
      "Episode 7, Total Reward = 6.0\n",
      "Episode 8, Total Reward = 5.0\n",
      "Episode 9, Total Reward = 4.0\n",
      "Episode 10, Total Reward = 12.0\n",
      "Episode 11, Total Reward = 8.0\n",
      "Episode 12, Total Reward = 7.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 15.0\n",
      "Episode 15, Total Reward = 10.0\n",
      "Episode 16, Total Reward = 6.0\n",
      "Episode 17, Total Reward = 4.0\n",
      "Episode 18, Total Reward = 4.0\n",
      "Episode 19, Total Reward = 3.0\n",
      "Episode 20, Total Reward = 6.0\n",
      "Episode 21, Total Reward = 7.0\n",
      "Episode 22, Total Reward = 11.0\n",
      "Episode 23, Total Reward = 4.0\n",
      "Episode 24, Total Reward = 12.0\n",
      "Episode 25, Total Reward = 18.0\n",
      "Episode 26, Total Reward = 12.0\n",
      "Episode 27, Total Reward = 17.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 5.0\n",
      "Episode 30, Total Reward = 25.0\n",
      "Episode 31, Total Reward = 14.0\n",
      "Episode 32, Total Reward = 11.0\n",
      "Episode 33, Total Reward = 12.0\n",
      "Episode 34, Total Reward = 4.0\n",
      "Episode 35, Total Reward = 11.0\n",
      "Episode 36, Total Reward = 15.0\n",
      "Episode 37, Total Reward = 7.0\n",
      "Episode 38, Total Reward = 9.0\n",
      "Episode 39, Total Reward = 14.0\n",
      "Episode 40, Total Reward = 6.0\n",
      "Episode 41, Total Reward = 11.0\n",
      "Episode 42, Total Reward = 8.0\n",
      "Episode 43, Total Reward = 11.0\n",
      "Episode 44, Total Reward = 5.0\n",
      "Episode 45, Total Reward = 19.0\n",
      "Episode 46, Total Reward = 41.0\n",
      "Episode 47, Total Reward = 13.0\n",
      "Episode 48, Total Reward = 12.0\n",
      "Episode 49, Total Reward = 8.0\n",
      "Episode 50, Total Reward = 13.0\n",
      "Episode 51, Total Reward = 41.0\n",
      "Episode 52, Total Reward = 19.0\n",
      "Episode 53, Total Reward = 13.0\n",
      "Episode 54, Total Reward = 19.0\n",
      "Episode 55, Total Reward = 25.0\n",
      "Episode 56, Total Reward = 14.0\n",
      "Episode 57, Total Reward = 36.0\n",
      "Episode 58, Total Reward = 58.0\n",
      "Episode 59, Total Reward = 8.0\n",
      "Episode 60, Total Reward = 17.0\n",
      "Episode 61, Total Reward = 63.0\n",
      "Episode 62, Total Reward = 31.0\n",
      "Episode 63, Total Reward = 15.0\n",
      "Episode 64, Total Reward = 12.0\n",
      "Episode 65, Total Reward = 41.0\n",
      "Episode 66, Total Reward = 57.0\n",
      "Episode 67, Total Reward = 46.0\n",
      "Episode 68, Total Reward = 24.0\n",
      "Episode 69, Total Reward = 14.0\n",
      "Episode 70, Total Reward = 9.0\n",
      "Episode 71, Total Reward = 44.0\n",
      "Episode 72, Total Reward = 29.0\n",
      "Episode 73, Total Reward = 21.0\n",
      "Episode 74, Total Reward = 9.0\n",
      "Episode 75, Total Reward = 24.0\n",
      "Episode 76, Total Reward = 15.0\n",
      "Episode 77, Total Reward = 111.0\n",
      "Episode 78, Total Reward = 66.0\n",
      "Episode 79, Total Reward = 78.0\n",
      "Episode 80, Total Reward = 53.0\n",
      "Episode 81, Total Reward = 70.0\n",
      "Episode 82, Total Reward = 19.0\n",
      "Episode 83, Total Reward = 96.0\n",
      "Episode 84, Total Reward = 47.0\n",
      "Episode 85, Total Reward = 44.0\n",
      "Episode 86, Total Reward = 49.0\n",
      "Episode 87, Total Reward = 11.0\n",
      "Episode 88, Total Reward = 72.0\n",
      "Episode 89, Total Reward = 39.0\n",
      "Episode 90, Total Reward = 98.0\n",
      "Episode 91, Total Reward = 9.0\n",
      "Episode 92, Total Reward = 88.0\n",
      "Episode 93, Total Reward = 54.0\n",
      "Episode 94, Total Reward = 77.0\n",
      "Episode 95, Total Reward = 55.0\n",
      "Episode 96, Total Reward = 42.0\n",
      "Episode 97, Total Reward = 36.0\n",
      "Episode 98, Total Reward = 86.0\n",
      "Episode 99, Total Reward = 20.0\n",
      "Episode 100, Total Reward = 145.0\n",
      "Running experiment with lr=0.0003, epsilon=0.1, k_epochs=10, batch_size=64\n",
      "Episode 1, Total Reward = 8.0\n",
      "Episode 2, Total Reward = 4.0\n",
      "Episode 3, Total Reward = 12.0\n",
      "Episode 4, Total Reward = 8.0\n",
      "Episode 5, Total Reward = 5.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 25.0\n",
      "Episode 12, Total Reward = 14.0\n",
      "Episode 13, Total Reward = 14.0\n",
      "Episode 14, Total Reward = 9.0\n",
      "Episode 15, Total Reward = 3.0\n",
      "Episode 16, Total Reward = 15.0\n",
      "Episode 17, Total Reward = 6.0\n",
      "Episode 18, Total Reward = 11.0\n",
      "Episode 19, Total Reward = 11.0\n",
      "Episode 20, Total Reward = 15.0\n",
      "Episode 21, Total Reward = 17.0\n",
      "Episode 22, Total Reward = 8.0\n",
      "Episode 23, Total Reward = 4.0\n",
      "Episode 24, Total Reward = 6.0\n",
      "Episode 25, Total Reward = 4.0\n",
      "Episode 26, Total Reward = 11.0\n",
      "Episode 27, Total Reward = 10.0\n",
      "Episode 28, Total Reward = 10.0\n",
      "Episode 29, Total Reward = 4.0\n",
      "Episode 30, Total Reward = 6.0\n",
      "Episode 31, Total Reward = 15.0\n",
      "Episode 32, Total Reward = 5.0\n",
      "Episode 33, Total Reward = 10.0\n",
      "Episode 34, Total Reward = 19.0\n",
      "Episode 35, Total Reward = 18.0\n",
      "Episode 36, Total Reward = 24.0\n",
      "Episode 37, Total Reward = 10.0\n",
      "Episode 38, Total Reward = 5.0\n",
      "Episode 39, Total Reward = 24.0\n",
      "Episode 40, Total Reward = 25.0\n",
      "Episode 41, Total Reward = 16.0\n",
      "Episode 42, Total Reward = 9.0\n",
      "Episode 43, Total Reward = 5.0\n",
      "Episode 44, Total Reward = 12.0\n",
      "Episode 45, Total Reward = 10.0\n",
      "Episode 46, Total Reward = 19.0\n",
      "Episode 47, Total Reward = 12.0\n",
      "Episode 48, Total Reward = 15.0\n",
      "Episode 49, Total Reward = 14.0\n",
      "Episode 50, Total Reward = 7.0\n",
      "Episode 51, Total Reward = 7.0\n",
      "Episode 52, Total Reward = 13.0\n",
      "Episode 53, Total Reward = 7.0\n",
      "Episode 54, Total Reward = 15.0\n",
      "Episode 55, Total Reward = 18.0\n",
      "Episode 56, Total Reward = 35.0\n",
      "Episode 57, Total Reward = 12.0\n",
      "Episode 58, Total Reward = 30.0\n",
      "Episode 59, Total Reward = 7.0\n",
      "Episode 60, Total Reward = 96.0\n",
      "Episode 61, Total Reward = 35.0\n",
      "Episode 62, Total Reward = 38.0\n",
      "Episode 63, Total Reward = 34.0\n",
      "Episode 64, Total Reward = 33.0\n",
      "Episode 65, Total Reward = 11.0\n",
      "Episode 66, Total Reward = 20.0\n",
      "Episode 67, Total Reward = 26.0\n",
      "Episode 68, Total Reward = 29.0\n",
      "Episode 69, Total Reward = 9.0\n",
      "Episode 70, Total Reward = 19.0\n",
      "Episode 71, Total Reward = 10.0\n",
      "Episode 72, Total Reward = 56.0\n",
      "Episode 73, Total Reward = 34.0\n",
      "Episode 74, Total Reward = 96.0\n",
      "Episode 75, Total Reward = 11.0\n",
      "Episode 76, Total Reward = 29.0\n",
      "Episode 77, Total Reward = 24.0\n",
      "Episode 78, Total Reward = 41.0\n",
      "Episode 79, Total Reward = 45.0\n",
      "Episode 80, Total Reward = 82.0\n",
      "Episode 81, Total Reward = 91.0\n",
      "Episode 82, Total Reward = 35.0\n",
      "Episode 83, Total Reward = 42.0\n",
      "Episode 84, Total Reward = 54.0\n",
      "Episode 85, Total Reward = 79.0\n",
      "Episode 86, Total Reward = 36.0\n",
      "Episode 87, Total Reward = 22.0\n",
      "Episode 88, Total Reward = 26.0\n",
      "Episode 89, Total Reward = 27.0\n",
      "Episode 90, Total Reward = 52.0\n",
      "Episode 91, Total Reward = 89.0\n",
      "Episode 92, Total Reward = 34.0\n",
      "Episode 93, Total Reward = 97.0\n",
      "Episode 94, Total Reward = 53.0\n",
      "Episode 95, Total Reward = 91.0\n",
      "Episode 96, Total Reward = 37.0\n",
      "Episode 97, Total Reward = 42.0\n",
      "Episode 98, Total Reward = 40.0\n",
      "Episode 99, Total Reward = 45.0\n",
      "Episode 100, Total Reward = 61.0\n",
      "Running experiment with lr=0.0003, epsilon=0.1, k_epochs=10, batch_size=128\n",
      "Episode 1, Total Reward = 11.0\n",
      "Episode 2, Total Reward = 9.0\n",
      "Episode 3, Total Reward = 9.0\n",
      "Episode 4, Total Reward = 7.0\n",
      "Episode 5, Total Reward = 12.0\n",
      "Episode 6, Total Reward = 7.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 8.0\n",
      "Episode 9, Total Reward = 17.0\n",
      "Episode 10, Total Reward = 28.0\n",
      "Episode 11, Total Reward = 6.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 18.0\n",
      "Episode 14, Total Reward = 9.0\n",
      "Episode 15, Total Reward = 13.0\n",
      "Episode 16, Total Reward = 7.0\n",
      "Episode 17, Total Reward = 10.0\n",
      "Episode 18, Total Reward = 8.0\n",
      "Episode 19, Total Reward = 7.0\n",
      "Episode 20, Total Reward = 13.0\n",
      "Episode 21, Total Reward = 5.0\n",
      "Episode 22, Total Reward = 18.0\n",
      "Episode 23, Total Reward = 8.0\n",
      "Episode 24, Total Reward = 12.0\n",
      "Episode 25, Total Reward = 9.0\n",
      "Episode 26, Total Reward = 5.0\n",
      "Episode 27, Total Reward = 22.0\n",
      "Episode 28, Total Reward = 32.0\n",
      "Episode 29, Total Reward = 7.0\n",
      "Episode 30, Total Reward = 37.0\n",
      "Episode 31, Total Reward = 9.0\n",
      "Episode 32, Total Reward = 12.0\n",
      "Episode 33, Total Reward = 25.0\n",
      "Episode 34, Total Reward = 14.0\n",
      "Episode 35, Total Reward = 8.0\n",
      "Episode 36, Total Reward = 30.0\n",
      "Episode 37, Total Reward = 29.0\n",
      "Episode 38, Total Reward = 8.0\n",
      "Episode 39, Total Reward = 18.0\n",
      "Episode 40, Total Reward = 23.0\n",
      "Episode 41, Total Reward = 37.0\n",
      "Episode 42, Total Reward = 11.0\n",
      "Episode 43, Total Reward = 18.0\n",
      "Episode 44, Total Reward = 25.0\n",
      "Episode 45, Total Reward = 30.0\n",
      "Episode 46, Total Reward = 14.0\n",
      "Episode 47, Total Reward = 56.0\n",
      "Episode 48, Total Reward = 29.0\n",
      "Episode 49, Total Reward = 19.0\n",
      "Episode 50, Total Reward = 27.0\n",
      "Episode 51, Total Reward = 60.0\n",
      "Episode 52, Total Reward = 25.0\n",
      "Episode 53, Total Reward = 19.0\n",
      "Episode 54, Total Reward = 16.0\n",
      "Episode 55, Total Reward = 28.0\n",
      "Episode 56, Total Reward = 32.0\n",
      "Episode 57, Total Reward = 14.0\n",
      "Episode 58, Total Reward = 38.0\n",
      "Episode 59, Total Reward = 25.0\n",
      "Episode 60, Total Reward = 26.0\n",
      "Episode 61, Total Reward = 7.0\n",
      "Episode 62, Total Reward = 41.0\n",
      "Episode 63, Total Reward = 11.0\n",
      "Episode 64, Total Reward = 41.0\n",
      "Episode 65, Total Reward = 9.0\n",
      "Episode 66, Total Reward = 24.0\n",
      "Episode 67, Total Reward = 41.0\n",
      "Episode 68, Total Reward = 53.0\n",
      "Episode 69, Total Reward = 49.0\n",
      "Episode 70, Total Reward = 12.0\n",
      "Episode 71, Total Reward = 71.0\n",
      "Episode 72, Total Reward = 85.0\n",
      "Episode 73, Total Reward = 9.0\n",
      "Episode 74, Total Reward = 48.0\n",
      "Episode 75, Total Reward = 12.0\n",
      "Episode 76, Total Reward = 12.0\n",
      "Episode 77, Total Reward = 14.0\n",
      "Episode 78, Total Reward = 50.0\n",
      "Episode 79, Total Reward = 20.0\n",
      "Episode 80, Total Reward = 53.0\n",
      "Episode 81, Total Reward = 37.0\n",
      "Episode 82, Total Reward = 78.0\n",
      "Episode 83, Total Reward = 4.0\n",
      "Episode 84, Total Reward = 17.0\n",
      "Episode 85, Total Reward = 14.0\n",
      "Episode 86, Total Reward = 68.0\n",
      "Episode 87, Total Reward = 64.0\n",
      "Episode 88, Total Reward = 40.0\n",
      "Episode 89, Total Reward = 74.0\n",
      "Episode 90, Total Reward = 61.0\n",
      "Episode 91, Total Reward = 77.0\n",
      "Episode 92, Total Reward = 77.0\n",
      "Episode 93, Total Reward = 72.0\n",
      "Episode 94, Total Reward = 47.0\n",
      "Episode 95, Total Reward = 41.0\n",
      "Episode 96, Total Reward = 11.0\n",
      "Episode 97, Total Reward = 55.0\n",
      "Episode 98, Total Reward = 43.0\n",
      "Episode 99, Total Reward = 61.0\n",
      "Episode 100, Total Reward = 53.0\n",
      "Running experiment with lr=0.0003, epsilon=0.1, k_epochs=20, batch_size=32\n",
      "Episode 1, Total Reward = 8.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 7.0\n",
      "Episode 5, Total Reward = 5.0\n",
      "Episode 6, Total Reward = 10.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 8.0\n",
      "Episode 9, Total Reward = 8.0\n",
      "Episode 10, Total Reward = 12.0\n",
      "Episode 11, Total Reward = 3.0\n",
      "Episode 12, Total Reward = 13.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 3.0\n",
      "Episode 17, Total Reward = 22.0\n",
      "Episode 18, Total Reward = 6.0\n",
      "Episode 19, Total Reward = 9.0\n",
      "Episode 20, Total Reward = 6.0\n",
      "Episode 21, Total Reward = 5.0\n",
      "Episode 22, Total Reward = 20.0\n",
      "Episode 23, Total Reward = 10.0\n",
      "Episode 24, Total Reward = 11.0\n",
      "Episode 25, Total Reward = 26.0\n",
      "Episode 26, Total Reward = 15.0\n",
      "Episode 27, Total Reward = 11.0\n",
      "Episode 28, Total Reward = 4.0\n",
      "Episode 29, Total Reward = 31.0\n",
      "Episode 30, Total Reward = 15.0\n",
      "Episode 31, Total Reward = 10.0\n",
      "Episode 32, Total Reward = 12.0\n",
      "Episode 33, Total Reward = 5.0\n",
      "Episode 34, Total Reward = 13.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 11.0\n",
      "Episode 37, Total Reward = 13.0\n",
      "Episode 38, Total Reward = 7.0\n",
      "Episode 39, Total Reward = 12.0\n",
      "Episode 40, Total Reward = 5.0\n",
      "Episode 41, Total Reward = 17.0\n",
      "Episode 42, Total Reward = 26.0\n",
      "Episode 43, Total Reward = 25.0\n",
      "Episode 44, Total Reward = 20.0\n",
      "Episode 45, Total Reward = 8.0\n",
      "Episode 46, Total Reward = 13.0\n",
      "Episode 47, Total Reward = 26.0\n",
      "Episode 48, Total Reward = 16.0\n",
      "Episode 49, Total Reward = 56.0\n",
      "Episode 50, Total Reward = 11.0\n",
      "Episode 51, Total Reward = 9.0\n",
      "Episode 52, Total Reward = 8.0\n",
      "Episode 53, Total Reward = 22.0\n",
      "Episode 54, Total Reward = 6.0\n",
      "Episode 55, Total Reward = 10.0\n",
      "Episode 56, Total Reward = 30.0\n",
      "Episode 57, Total Reward = 45.0\n",
      "Episode 58, Total Reward = 21.0\n",
      "Episode 59, Total Reward = 12.0\n",
      "Episode 60, Total Reward = 6.0\n",
      "Episode 61, Total Reward = 23.0\n",
      "Episode 62, Total Reward = 67.0\n",
      "Episode 63, Total Reward = 15.0\n",
      "Episode 64, Total Reward = 58.0\n",
      "Episode 65, Total Reward = 5.0\n",
      "Episode 66, Total Reward = 22.0\n",
      "Episode 67, Total Reward = 41.0\n",
      "Episode 68, Total Reward = 15.0\n",
      "Episode 69, Total Reward = 58.0\n",
      "Episode 70, Total Reward = 36.0\n",
      "Episode 71, Total Reward = 43.0\n",
      "Episode 72, Total Reward = 21.0\n",
      "Episode 73, Total Reward = 71.0\n",
      "Episode 74, Total Reward = 16.0\n",
      "Episode 75, Total Reward = 14.0\n",
      "Episode 76, Total Reward = 39.0\n",
      "Episode 77, Total Reward = 30.0\n",
      "Episode 78, Total Reward = 77.0\n",
      "Episode 79, Total Reward = 20.0\n",
      "Episode 80, Total Reward = 76.0\n",
      "Episode 81, Total Reward = 20.0\n",
      "Episode 82, Total Reward = 41.0\n",
      "Episode 83, Total Reward = 55.0\n",
      "Episode 84, Total Reward = 28.0\n",
      "Episode 85, Total Reward = 45.0\n",
      "Episode 86, Total Reward = 41.0\n",
      "Episode 87, Total Reward = 13.0\n",
      "Episode 88, Total Reward = 97.0\n",
      "Episode 89, Total Reward = 36.0\n",
      "Episode 90, Total Reward = 44.0\n",
      "Episode 91, Total Reward = 68.0\n",
      "Episode 92, Total Reward = 48.0\n",
      "Episode 93, Total Reward = 14.0\n",
      "Episode 94, Total Reward = 21.0\n",
      "Episode 95, Total Reward = 52.0\n",
      "Episode 96, Total Reward = 45.0\n",
      "Episode 97, Total Reward = 15.0\n",
      "Episode 98, Total Reward = 21.0\n",
      "Episode 99, Total Reward = 71.0\n",
      "Episode 100, Total Reward = 57.0\n",
      "Running experiment with lr=0.0003, epsilon=0.1, k_epochs=20, batch_size=64\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 10.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 6.0\n",
      "Episode 6, Total Reward = 4.0\n",
      "Episode 7, Total Reward = 15.0\n",
      "Episode 8, Total Reward = 7.0\n",
      "Episode 9, Total Reward = 8.0\n",
      "Episode 10, Total Reward = 7.0\n",
      "Episode 11, Total Reward = 6.0\n",
      "Episode 12, Total Reward = 24.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 20.0\n",
      "Episode 16, Total Reward = 17.0\n",
      "Episode 17, Total Reward = 7.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 16.0\n",
      "Episode 20, Total Reward = 6.0\n",
      "Episode 21, Total Reward = 5.0\n",
      "Episode 22, Total Reward = 6.0\n",
      "Episode 23, Total Reward = 23.0\n",
      "Episode 24, Total Reward = 6.0\n",
      "Episode 25, Total Reward = 12.0\n",
      "Episode 26, Total Reward = 12.0\n",
      "Episode 27, Total Reward = 4.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 5.0\n",
      "Episode 30, Total Reward = 23.0\n",
      "Episode 31, Total Reward = 28.0\n",
      "Episode 32, Total Reward = 9.0\n",
      "Episode 33, Total Reward = 6.0\n",
      "Episode 34, Total Reward = 17.0\n",
      "Episode 35, Total Reward = 4.0\n",
      "Episode 36, Total Reward = 31.0\n",
      "Episode 37, Total Reward = 9.0\n",
      "Episode 38, Total Reward = 11.0\n",
      "Episode 39, Total Reward = 9.0\n",
      "Episode 40, Total Reward = 23.0\n",
      "Episode 41, Total Reward = 37.0\n",
      "Episode 42, Total Reward = 7.0\n",
      "Episode 43, Total Reward = 19.0\n",
      "Episode 44, Total Reward = 25.0\n",
      "Episode 45, Total Reward = 8.0\n",
      "Episode 46, Total Reward = 12.0\n",
      "Episode 47, Total Reward = 20.0\n",
      "Episode 48, Total Reward = 13.0\n",
      "Episode 49, Total Reward = 49.0\n",
      "Episode 50, Total Reward = 14.0\n",
      "Episode 51, Total Reward = 17.0\n",
      "Episode 52, Total Reward = 22.0\n",
      "Episode 53, Total Reward = 14.0\n",
      "Episode 54, Total Reward = 8.0\n",
      "Episode 55, Total Reward = 35.0\n",
      "Episode 56, Total Reward = 34.0\n",
      "Episode 57, Total Reward = 19.0\n",
      "Episode 58, Total Reward = 16.0\n",
      "Episode 59, Total Reward = 22.0\n",
      "Episode 60, Total Reward = 63.0\n",
      "Episode 61, Total Reward = 53.0\n",
      "Episode 62, Total Reward = 57.0\n",
      "Episode 63, Total Reward = 56.0\n",
      "Episode 64, Total Reward = 31.0\n",
      "Episode 65, Total Reward = 73.0\n",
      "Episode 66, Total Reward = 39.0\n",
      "Episode 67, Total Reward = 42.0\n",
      "Episode 68, Total Reward = 88.0\n",
      "Episode 69, Total Reward = 71.0\n",
      "Episode 70, Total Reward = 24.0\n",
      "Episode 71, Total Reward = 24.0\n",
      "Episode 72, Total Reward = 92.0\n",
      "Episode 73, Total Reward = 25.0\n",
      "Episode 74, Total Reward = 91.0\n",
      "Episode 75, Total Reward = 91.0\n",
      "Episode 76, Total Reward = 58.0\n",
      "Episode 77, Total Reward = 36.0\n",
      "Episode 78, Total Reward = 113.0\n",
      "Episode 79, Total Reward = 36.0\n",
      "Episode 80, Total Reward = 51.0\n",
      "Episode 81, Total Reward = 26.0\n",
      "Episode 82, Total Reward = 10.0\n",
      "Episode 83, Total Reward = 76.0\n",
      "Episode 84, Total Reward = 56.0\n",
      "Episode 85, Total Reward = 84.0\n",
      "Episode 86, Total Reward = 77.0\n",
      "Episode 87, Total Reward = 43.0\n",
      "Episode 88, Total Reward = 73.0\n",
      "Episode 89, Total Reward = 104.0\n",
      "Episode 90, Total Reward = 45.0\n",
      "Episode 91, Total Reward = 16.0\n",
      "Episode 92, Total Reward = 58.0\n",
      "Episode 93, Total Reward = 67.0\n",
      "Episode 94, Total Reward = 27.0\n",
      "Episode 95, Total Reward = 54.0\n",
      "Episode 96, Total Reward = 83.0\n",
      "Episode 97, Total Reward = 67.0\n",
      "Episode 98, Total Reward = 87.0\n",
      "Episode 99, Total Reward = 73.0\n",
      "Episode 100, Total Reward = 123.0\n",
      "Running experiment with lr=0.0003, epsilon=0.1, k_epochs=20, batch_size=128\n",
      "Episode 1, Total Reward = 3.0\n",
      "Episode 2, Total Reward = 14.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 11.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 4.0\n",
      "Episode 8, Total Reward = 10.0\n",
      "Episode 9, Total Reward = 10.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 11.0\n",
      "Episode 12, Total Reward = 8.0\n",
      "Episode 13, Total Reward = 9.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 22.0\n",
      "Episode 16, Total Reward = 8.0\n",
      "Episode 17, Total Reward = 8.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 4.0\n",
      "Episode 20, Total Reward = 9.0\n",
      "Episode 21, Total Reward = 4.0\n",
      "Episode 22, Total Reward = 5.0\n",
      "Episode 23, Total Reward = 4.0\n",
      "Episode 24, Total Reward = 6.0\n",
      "Episode 25, Total Reward = 5.0\n",
      "Episode 26, Total Reward = 8.0\n",
      "Episode 27, Total Reward = 4.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 11.0\n",
      "Episode 30, Total Reward = 11.0\n",
      "Episode 31, Total Reward = 8.0\n",
      "Episode 32, Total Reward = 4.0\n",
      "Episode 33, Total Reward = 11.0\n",
      "Episode 34, Total Reward = 11.0\n",
      "Episode 35, Total Reward = 17.0\n",
      "Episode 36, Total Reward = 5.0\n",
      "Episode 37, Total Reward = 49.0\n",
      "Episode 38, Total Reward = 7.0\n",
      "Episode 39, Total Reward = 10.0\n",
      "Episode 40, Total Reward = 7.0\n",
      "Episode 41, Total Reward = 14.0\n",
      "Episode 42, Total Reward = 22.0\n",
      "Episode 43, Total Reward = 11.0\n",
      "Episode 44, Total Reward = 67.0\n",
      "Episode 45, Total Reward = 14.0\n",
      "Episode 46, Total Reward = 7.0\n",
      "Episode 47, Total Reward = 8.0\n",
      "Episode 48, Total Reward = 23.0\n",
      "Episode 49, Total Reward = 8.0\n",
      "Episode 50, Total Reward = 13.0\n",
      "Episode 51, Total Reward = 14.0\n",
      "Episode 52, Total Reward = 14.0\n",
      "Episode 53, Total Reward = 5.0\n",
      "Episode 54, Total Reward = 22.0\n",
      "Episode 55, Total Reward = 11.0\n",
      "Episode 56, Total Reward = 16.0\n",
      "Episode 57, Total Reward = 7.0\n",
      "Episode 58, Total Reward = 8.0\n",
      "Episode 59, Total Reward = 31.0\n",
      "Episode 60, Total Reward = 5.0\n",
      "Episode 61, Total Reward = 14.0\n",
      "Episode 62, Total Reward = 11.0\n",
      "Episode 63, Total Reward = 23.0\n",
      "Episode 64, Total Reward = 20.0\n",
      "Episode 65, Total Reward = 50.0\n",
      "Episode 66, Total Reward = 33.0\n",
      "Episode 67, Total Reward = 22.0\n",
      "Episode 68, Total Reward = 30.0\n",
      "Episode 69, Total Reward = 33.0\n",
      "Episode 70, Total Reward = 19.0\n",
      "Episode 71, Total Reward = 52.0\n",
      "Episode 72, Total Reward = 69.0\n",
      "Episode 73, Total Reward = 22.0\n",
      "Episode 74, Total Reward = 45.0\n",
      "Episode 75, Total Reward = 59.0\n",
      "Episode 76, Total Reward = 106.0\n",
      "Episode 77, Total Reward = 9.0\n",
      "Episode 78, Total Reward = 48.0\n",
      "Episode 79, Total Reward = 12.0\n",
      "Episode 80, Total Reward = 39.0\n",
      "Episode 81, Total Reward = 52.0\n",
      "Episode 82, Total Reward = 10.0\n",
      "Episode 83, Total Reward = 98.0\n",
      "Episode 84, Total Reward = 33.0\n",
      "Episode 85, Total Reward = 48.0\n",
      "Episode 86, Total Reward = 133.0\n",
      "Episode 87, Total Reward = 53.0\n",
      "Episode 88, Total Reward = 46.0\n",
      "Episode 89, Total Reward = 44.0\n",
      "Episode 90, Total Reward = 15.0\n",
      "Episode 91, Total Reward = 144.0\n",
      "Episode 92, Total Reward = 47.0\n",
      "Episode 93, Total Reward = 37.0\n",
      "Episode 94, Total Reward = 14.0\n",
      "Episode 95, Total Reward = 23.0\n",
      "Episode 96, Total Reward = 13.0\n",
      "Episode 97, Total Reward = 15.0\n",
      "Episode 98, Total Reward = 59.0\n",
      "Episode 99, Total Reward = 67.0\n",
      "Episode 100, Total Reward = 20.0\n",
      "Running experiment with lr=0.0003, epsilon=0.2, k_epochs=5, batch_size=32\n",
      "Episode 1, Total Reward = 6.0\n",
      "Episode 2, Total Reward = 4.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 4.0\n",
      "Episode 5, Total Reward = 14.0\n",
      "Episode 6, Total Reward = 5.0\n",
      "Episode 7, Total Reward = 9.0\n",
      "Episode 8, Total Reward = 10.0\n",
      "Episode 9, Total Reward = 17.0\n",
      "Episode 10, Total Reward = 6.0\n",
      "Episode 11, Total Reward = 5.0\n",
      "Episode 12, Total Reward = 7.0\n",
      "Episode 13, Total Reward = 7.0\n",
      "Episode 14, Total Reward = 9.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 7.0\n",
      "Episode 17, Total Reward = 10.0\n",
      "Episode 18, Total Reward = 10.0\n",
      "Episode 19, Total Reward = 6.0\n",
      "Episode 20, Total Reward = 5.0\n",
      "Episode 21, Total Reward = 14.0\n",
      "Episode 22, Total Reward = 14.0\n",
      "Episode 23, Total Reward = 14.0\n",
      "Episode 24, Total Reward = 7.0\n",
      "Episode 25, Total Reward = 10.0\n",
      "Episode 26, Total Reward = 7.0\n",
      "Episode 27, Total Reward = 7.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 7.0\n",
      "Episode 30, Total Reward = 3.0\n",
      "Episode 31, Total Reward = 7.0\n",
      "Episode 32, Total Reward = 11.0\n",
      "Episode 33, Total Reward = 14.0\n",
      "Episode 34, Total Reward = 8.0\n",
      "Episode 35, Total Reward = 4.0\n",
      "Episode 36, Total Reward = 11.0\n",
      "Episode 37, Total Reward = 10.0\n",
      "Episode 38, Total Reward = 9.0\n",
      "Episode 39, Total Reward = 9.0\n",
      "Episode 40, Total Reward = 5.0\n",
      "Episode 41, Total Reward = 10.0\n",
      "Episode 42, Total Reward = 21.0\n",
      "Episode 43, Total Reward = 13.0\n",
      "Episode 44, Total Reward = 36.0\n",
      "Episode 45, Total Reward = 15.0\n",
      "Episode 46, Total Reward = 12.0\n",
      "Episode 47, Total Reward = 22.0\n",
      "Episode 48, Total Reward = 10.0\n",
      "Episode 49, Total Reward = 23.0\n",
      "Episode 50, Total Reward = 17.0\n",
      "Episode 51, Total Reward = 9.0\n",
      "Episode 52, Total Reward = 9.0\n",
      "Episode 53, Total Reward = 57.0\n",
      "Episode 54, Total Reward = 18.0\n",
      "Episode 55, Total Reward = 6.0\n",
      "Episode 56, Total Reward = 26.0\n",
      "Episode 57, Total Reward = 22.0\n",
      "Episode 58, Total Reward = 10.0\n",
      "Episode 59, Total Reward = 26.0\n",
      "Episode 60, Total Reward = 12.0\n",
      "Episode 61, Total Reward = 10.0\n",
      "Episode 62, Total Reward = 46.0\n",
      "Episode 63, Total Reward = 67.0\n",
      "Episode 64, Total Reward = 47.0\n",
      "Episode 65, Total Reward = 7.0\n",
      "Episode 66, Total Reward = 21.0\n",
      "Episode 67, Total Reward = 73.0\n",
      "Episode 68, Total Reward = 36.0\n",
      "Episode 69, Total Reward = 23.0\n",
      "Episode 70, Total Reward = 40.0\n",
      "Episode 71, Total Reward = 21.0\n",
      "Episode 72, Total Reward = 50.0\n",
      "Episode 73, Total Reward = 10.0\n",
      "Episode 74, Total Reward = 33.0\n",
      "Episode 75, Total Reward = 54.0\n",
      "Episode 76, Total Reward = 56.0\n",
      "Episode 77, Total Reward = 21.0\n",
      "Episode 78, Total Reward = 32.0\n",
      "Episode 79, Total Reward = 58.0\n",
      "Episode 80, Total Reward = 58.0\n",
      "Episode 81, Total Reward = 59.0\n",
      "Episode 82, Total Reward = 72.0\n",
      "Episode 83, Total Reward = 84.0\n",
      "Episode 84, Total Reward = 29.0\n",
      "Episode 85, Total Reward = 24.0\n",
      "Episode 86, Total Reward = 35.0\n",
      "Episode 87, Total Reward = 19.0\n",
      "Episode 88, Total Reward = 105.0\n",
      "Episode 89, Total Reward = 7.0\n",
      "Episode 90, Total Reward = 57.0\n",
      "Episode 91, Total Reward = 72.0\n",
      "Episode 92, Total Reward = 96.0\n",
      "Episode 93, Total Reward = 42.0\n",
      "Episode 94, Total Reward = 60.0\n",
      "Episode 95, Total Reward = 77.0\n",
      "Episode 96, Total Reward = 50.0\n",
      "Episode 97, Total Reward = 78.0\n",
      "Episode 98, Total Reward = 89.0\n",
      "Episode 99, Total Reward = 71.0\n",
      "Episode 100, Total Reward = 89.0\n",
      "Running experiment with lr=0.0003, epsilon=0.2, k_epochs=5, batch_size=64\n",
      "Episode 1, Total Reward = 19.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 7.0\n",
      "Episode 6, Total Reward = 3.0\n",
      "Episode 7, Total Reward = 12.0\n",
      "Episode 8, Total Reward = 11.0\n",
      "Episode 9, Total Reward = 7.0\n",
      "Episode 10, Total Reward = 4.0\n",
      "Episode 11, Total Reward = 7.0\n",
      "Episode 12, Total Reward = 22.0\n",
      "Episode 13, Total Reward = 14.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 3.0\n",
      "Episode 16, Total Reward = 10.0\n",
      "Episode 17, Total Reward = 40.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 11.0\n",
      "Episode 20, Total Reward = 23.0\n",
      "Episode 21, Total Reward = 5.0\n",
      "Episode 22, Total Reward = 7.0\n",
      "Episode 23, Total Reward = 9.0\n",
      "Episode 24, Total Reward = 11.0\n",
      "Episode 25, Total Reward = 31.0\n",
      "Episode 26, Total Reward = 6.0\n",
      "Episode 27, Total Reward = 14.0\n",
      "Episode 28, Total Reward = 16.0\n",
      "Episode 29, Total Reward = 9.0\n",
      "Episode 30, Total Reward = 14.0\n",
      "Episode 31, Total Reward = 10.0\n",
      "Episode 32, Total Reward = 10.0\n",
      "Episode 33, Total Reward = 10.0\n",
      "Episode 34, Total Reward = 10.0\n",
      "Episode 35, Total Reward = 12.0\n",
      "Episode 36, Total Reward = 5.0\n",
      "Episode 37, Total Reward = 12.0\n",
      "Episode 38, Total Reward = 16.0\n",
      "Episode 39, Total Reward = 6.0\n",
      "Episode 40, Total Reward = 16.0\n",
      "Episode 41, Total Reward = 32.0\n",
      "Episode 42, Total Reward = 21.0\n",
      "Episode 43, Total Reward = 27.0\n",
      "Episode 44, Total Reward = 9.0\n",
      "Episode 45, Total Reward = 18.0\n",
      "Episode 46, Total Reward = 11.0\n",
      "Episode 47, Total Reward = 45.0\n",
      "Episode 48, Total Reward = 14.0\n",
      "Episode 49, Total Reward = 10.0\n",
      "Episode 50, Total Reward = 18.0\n",
      "Episode 51, Total Reward = 20.0\n",
      "Episode 52, Total Reward = 19.0\n",
      "Episode 53, Total Reward = 14.0\n",
      "Episode 54, Total Reward = 29.0\n",
      "Episode 55, Total Reward = 39.0\n",
      "Episode 56, Total Reward = 19.0\n",
      "Episode 57, Total Reward = 13.0\n",
      "Episode 58, Total Reward = 13.0\n",
      "Episode 59, Total Reward = 22.0\n",
      "Episode 60, Total Reward = 19.0\n",
      "Episode 61, Total Reward = 16.0\n",
      "Episode 62, Total Reward = 15.0\n",
      "Episode 63, Total Reward = 10.0\n",
      "Episode 64, Total Reward = 39.0\n",
      "Episode 65, Total Reward = 19.0\n",
      "Episode 66, Total Reward = 23.0\n",
      "Episode 67, Total Reward = 18.0\n",
      "Episode 68, Total Reward = 26.0\n",
      "Episode 69, Total Reward = 45.0\n",
      "Episode 70, Total Reward = 14.0\n",
      "Episode 71, Total Reward = 24.0\n",
      "Episode 72, Total Reward = 18.0\n",
      "Episode 73, Total Reward = 34.0\n",
      "Episode 74, Total Reward = 18.0\n",
      "Episode 75, Total Reward = 18.0\n",
      "Episode 76, Total Reward = 14.0\n",
      "Episode 77, Total Reward = 32.0\n",
      "Episode 78, Total Reward = 16.0\n",
      "Episode 79, Total Reward = 39.0\n",
      "Episode 80, Total Reward = 9.0\n",
      "Episode 81, Total Reward = 63.0\n",
      "Episode 82, Total Reward = 4.0\n",
      "Episode 83, Total Reward = 28.0\n",
      "Episode 84, Total Reward = 35.0\n",
      "Episode 85, Total Reward = 17.0\n",
      "Episode 86, Total Reward = 12.0\n",
      "Episode 87, Total Reward = 27.0\n",
      "Episode 88, Total Reward = 15.0\n",
      "Episode 89, Total Reward = 28.0\n",
      "Episode 90, Total Reward = 8.0\n",
      "Episode 91, Total Reward = 26.0\n",
      "Episode 92, Total Reward = 40.0\n",
      "Episode 93, Total Reward = 26.0\n",
      "Episode 94, Total Reward = 10.0\n",
      "Episode 95, Total Reward = 15.0\n",
      "Episode 96, Total Reward = 27.0\n",
      "Episode 97, Total Reward = 52.0\n",
      "Episode 98, Total Reward = 10.0\n",
      "Episode 99, Total Reward = 89.0\n",
      "Episode 100, Total Reward = 13.0\n",
      "Running experiment with lr=0.0003, epsilon=0.2, k_epochs=5, batch_size=128\n",
      "Episode 1, Total Reward = 4.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 5.0\n",
      "Episode 7, Total Reward = 4.0\n",
      "Episode 8, Total Reward = 6.0\n",
      "Episode 9, Total Reward = 8.0\n",
      "Episode 10, Total Reward = 7.0\n",
      "Episode 11, Total Reward = 13.0\n",
      "Episode 12, Total Reward = 4.0\n",
      "Episode 13, Total Reward = 10.0\n",
      "Episode 14, Total Reward = 5.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 4.0\n",
      "Episode 17, Total Reward = 6.0\n",
      "Episode 18, Total Reward = 4.0\n",
      "Episode 19, Total Reward = 17.0\n",
      "Episode 20, Total Reward = 9.0\n",
      "Episode 21, Total Reward = 27.0\n",
      "Episode 22, Total Reward = 8.0\n",
      "Episode 23, Total Reward = 26.0\n",
      "Episode 24, Total Reward = 7.0\n",
      "Episode 25, Total Reward = 6.0\n",
      "Episode 26, Total Reward = 9.0\n",
      "Episode 27, Total Reward = 7.0\n",
      "Episode 28, Total Reward = 7.0\n",
      "Episode 29, Total Reward = 5.0\n",
      "Episode 30, Total Reward = 5.0\n",
      "Episode 31, Total Reward = 7.0\n",
      "Episode 32, Total Reward = 7.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 7.0\n",
      "Episode 35, Total Reward = 5.0\n",
      "Episode 36, Total Reward = 8.0\n",
      "Episode 37, Total Reward = 9.0\n",
      "Episode 38, Total Reward = 12.0\n",
      "Episode 39, Total Reward = 5.0\n",
      "Episode 40, Total Reward = 8.0\n",
      "Episode 41, Total Reward = 6.0\n",
      "Episode 42, Total Reward = 18.0\n",
      "Episode 43, Total Reward = 6.0\n",
      "Episode 44, Total Reward = 31.0\n",
      "Episode 45, Total Reward = 6.0\n",
      "Episode 46, Total Reward = 10.0\n",
      "Episode 47, Total Reward = 12.0\n",
      "Episode 48, Total Reward = 13.0\n",
      "Episode 49, Total Reward = 8.0\n",
      "Episode 50, Total Reward = 4.0\n",
      "Episode 51, Total Reward = 5.0\n",
      "Episode 52, Total Reward = 14.0\n",
      "Episode 53, Total Reward = 18.0\n",
      "Episode 54, Total Reward = 8.0\n",
      "Episode 55, Total Reward = 4.0\n",
      "Episode 56, Total Reward = 29.0\n",
      "Episode 57, Total Reward = 15.0\n",
      "Episode 58, Total Reward = 17.0\n",
      "Episode 59, Total Reward = 14.0\n",
      "Episode 60, Total Reward = 12.0\n",
      "Episode 61, Total Reward = 5.0\n",
      "Episode 62, Total Reward = 11.0\n",
      "Episode 63, Total Reward = 7.0\n",
      "Episode 64, Total Reward = 28.0\n",
      "Episode 65, Total Reward = 9.0\n",
      "Episode 66, Total Reward = 10.0\n",
      "Episode 67, Total Reward = 8.0\n",
      "Episode 68, Total Reward = 26.0\n",
      "Episode 69, Total Reward = 8.0\n",
      "Episode 70, Total Reward = 37.0\n",
      "Episode 71, Total Reward = 22.0\n",
      "Episode 72, Total Reward = 21.0\n",
      "Episode 73, Total Reward = 27.0\n",
      "Episode 74, Total Reward = 20.0\n",
      "Episode 75, Total Reward = 13.0\n",
      "Episode 76, Total Reward = 37.0\n",
      "Episode 77, Total Reward = 44.0\n",
      "Episode 78, Total Reward = 12.0\n",
      "Episode 79, Total Reward = 24.0\n",
      "Episode 80, Total Reward = 9.0\n",
      "Episode 81, Total Reward = 12.0\n",
      "Episode 82, Total Reward = 21.0\n",
      "Episode 83, Total Reward = 68.0\n",
      "Episode 84, Total Reward = 52.0\n",
      "Episode 85, Total Reward = 27.0\n",
      "Episode 86, Total Reward = 39.0\n",
      "Episode 87, Total Reward = 11.0\n",
      "Episode 88, Total Reward = 14.0\n",
      "Episode 89, Total Reward = 29.0\n",
      "Episode 90, Total Reward = 29.0\n",
      "Episode 91, Total Reward = 20.0\n",
      "Episode 92, Total Reward = 60.0\n",
      "Episode 93, Total Reward = 16.0\n",
      "Episode 94, Total Reward = 66.0\n",
      "Episode 95, Total Reward = 11.0\n",
      "Episode 96, Total Reward = 16.0\n",
      "Episode 97, Total Reward = 52.0\n",
      "Episode 98, Total Reward = 14.0\n",
      "Episode 99, Total Reward = 46.0\n",
      "Episode 100, Total Reward = 58.0\n",
      "Running experiment with lr=0.0003, epsilon=0.2, k_epochs=10, batch_size=32\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 11.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 11.0\n",
      "Episode 6, Total Reward = 11.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 7.0\n",
      "Episode 9, Total Reward = 18.0\n",
      "Episode 10, Total Reward = 12.0\n",
      "Episode 11, Total Reward = 5.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 10.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 5.0\n",
      "Episode 16, Total Reward = 16.0\n",
      "Episode 17, Total Reward = 6.0\n",
      "Episode 18, Total Reward = 19.0\n",
      "Episode 19, Total Reward = 8.0\n",
      "Episode 20, Total Reward = 38.0\n",
      "Episode 21, Total Reward = 20.0\n",
      "Episode 22, Total Reward = 20.0\n",
      "Episode 23, Total Reward = 12.0\n",
      "Episode 24, Total Reward = 21.0\n",
      "Episode 25, Total Reward = 21.0\n",
      "Episode 26, Total Reward = 37.0\n",
      "Episode 27, Total Reward = 24.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 22.0\n",
      "Episode 30, Total Reward = 9.0\n",
      "Episode 31, Total Reward = 19.0\n",
      "Episode 32, Total Reward = 5.0\n",
      "Episode 33, Total Reward = 33.0\n",
      "Episode 34, Total Reward = 37.0\n",
      "Episode 35, Total Reward = 32.0\n",
      "Episode 36, Total Reward = 15.0\n",
      "Episode 37, Total Reward = 9.0\n",
      "Episode 38, Total Reward = 45.0\n",
      "Episode 39, Total Reward = 39.0\n",
      "Episode 40, Total Reward = 40.0\n",
      "Episode 41, Total Reward = 47.0\n",
      "Episode 42, Total Reward = 8.0\n",
      "Episode 43, Total Reward = 23.0\n",
      "Episode 44, Total Reward = 6.0\n",
      "Episode 45, Total Reward = 31.0\n",
      "Episode 46, Total Reward = 18.0\n",
      "Episode 47, Total Reward = 32.0\n",
      "Episode 48, Total Reward = 47.0\n",
      "Episode 49, Total Reward = 48.0\n",
      "Episode 50, Total Reward = 41.0\n",
      "Episode 51, Total Reward = 26.0\n",
      "Episode 52, Total Reward = 60.0\n",
      "Episode 53, Total Reward = 47.0\n",
      "Episode 54, Total Reward = 59.0\n",
      "Episode 55, Total Reward = 14.0\n",
      "Episode 56, Total Reward = 24.0\n",
      "Episode 57, Total Reward = 55.0\n",
      "Episode 58, Total Reward = 21.0\n",
      "Episode 59, Total Reward = 26.0\n",
      "Episode 60, Total Reward = 17.0\n",
      "Episode 61, Total Reward = 26.0\n",
      "Episode 62, Total Reward = 71.0\n",
      "Episode 63, Total Reward = 100.0\n",
      "Episode 64, Total Reward = 91.0\n",
      "Episode 65, Total Reward = 45.0\n",
      "Episode 66, Total Reward = 44.0\n",
      "Episode 67, Total Reward = 40.0\n",
      "Episode 68, Total Reward = 75.0\n",
      "Episode 69, Total Reward = 66.0\n",
      "Episode 70, Total Reward = 15.0\n",
      "Episode 71, Total Reward = 119.0\n",
      "Episode 72, Total Reward = 61.0\n",
      "Episode 73, Total Reward = 50.0\n",
      "Episode 74, Total Reward = 56.0\n",
      "Episode 75, Total Reward = 35.0\n",
      "Episode 76, Total Reward = 71.0\n",
      "Episode 77, Total Reward = 106.0\n",
      "Episode 78, Total Reward = 48.0\n",
      "Episode 79, Total Reward = 16.0\n",
      "Episode 80, Total Reward = 48.0\n",
      "Episode 81, Total Reward = 92.0\n",
      "Episode 82, Total Reward = 56.0\n",
      "Episode 83, Total Reward = 69.0\n",
      "Episode 84, Total Reward = 39.0\n",
      "Episode 85, Total Reward = 63.0\n",
      "Episode 86, Total Reward = 59.0\n",
      "Episode 87, Total Reward = 76.0\n",
      "Episode 88, Total Reward = 80.0\n",
      "Episode 89, Total Reward = 322.0\n",
      "Episode 90, Total Reward = 149.0\n",
      "Episode 91, Total Reward = 69.0\n",
      "Episode 92, Total Reward = 10.0\n",
      "Episode 93, Total Reward = 125.0\n",
      "Episode 94, Total Reward = 151.0\n",
      "Episode 95, Total Reward = 87.0\n",
      "Episode 96, Total Reward = 70.0\n",
      "Episode 97, Total Reward = 46.0\n",
      "Episode 98, Total Reward = 72.0\n",
      "Episode 99, Total Reward = 73.0\n",
      "Episode 100, Total Reward = 77.0\n",
      "Running experiment with lr=0.0003, epsilon=0.2, k_epochs=10, batch_size=64\n",
      "Episode 1, Total Reward = 9.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 7.0\n",
      "Episode 5, Total Reward = 6.0\n",
      "Episode 6, Total Reward = 30.0\n",
      "Episode 7, Total Reward = 14.0\n",
      "Episode 8, Total Reward = 3.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 10.0\n",
      "Episode 11, Total Reward = 8.0\n",
      "Episode 12, Total Reward = 4.0\n",
      "Episode 13, Total Reward = 6.0\n",
      "Episode 14, Total Reward = 12.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 10.0\n",
      "Episode 17, Total Reward = 5.0\n",
      "Episode 18, Total Reward = 9.0\n",
      "Episode 19, Total Reward = 14.0\n",
      "Episode 20, Total Reward = 11.0\n",
      "Episode 21, Total Reward = 4.0\n",
      "Episode 22, Total Reward = 9.0\n",
      "Episode 23, Total Reward = 11.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 7.0\n",
      "Episode 26, Total Reward = 5.0\n",
      "Episode 27, Total Reward = 6.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 13.0\n",
      "Episode 31, Total Reward = 30.0\n",
      "Episode 32, Total Reward = 10.0\n",
      "Episode 33, Total Reward = 6.0\n",
      "Episode 34, Total Reward = 18.0\n",
      "Episode 35, Total Reward = 29.0\n",
      "Episode 36, Total Reward = 41.0\n",
      "Episode 37, Total Reward = 32.0\n",
      "Episode 38, Total Reward = 5.0\n",
      "Episode 39, Total Reward = 6.0\n",
      "Episode 40, Total Reward = 14.0\n",
      "Episode 41, Total Reward = 15.0\n",
      "Episode 42, Total Reward = 8.0\n",
      "Episode 43, Total Reward = 7.0\n",
      "Episode 44, Total Reward = 7.0\n",
      "Episode 45, Total Reward = 30.0\n",
      "Episode 46, Total Reward = 11.0\n",
      "Episode 47, Total Reward = 29.0\n",
      "Episode 48, Total Reward = 12.0\n",
      "Episode 49, Total Reward = 25.0\n",
      "Episode 50, Total Reward = 11.0\n",
      "Episode 51, Total Reward = 19.0\n",
      "Episode 52, Total Reward = 12.0\n",
      "Episode 53, Total Reward = 13.0\n",
      "Episode 54, Total Reward = 13.0\n",
      "Episode 55, Total Reward = 22.0\n",
      "Episode 56, Total Reward = 31.0\n",
      "Episode 57, Total Reward = 20.0\n",
      "Episode 58, Total Reward = 16.0\n",
      "Episode 59, Total Reward = 38.0\n",
      "Episode 60, Total Reward = 22.0\n",
      "Episode 61, Total Reward = 24.0\n",
      "Episode 62, Total Reward = 38.0\n",
      "Episode 63, Total Reward = 17.0\n",
      "Episode 64, Total Reward = 15.0\n",
      "Episode 65, Total Reward = 42.0\n",
      "Episode 66, Total Reward = 31.0\n",
      "Episode 67, Total Reward = 27.0\n",
      "Episode 68, Total Reward = 52.0\n",
      "Episode 69, Total Reward = 24.0\n",
      "Episode 70, Total Reward = 64.0\n",
      "Episode 71, Total Reward = 16.0\n",
      "Episode 72, Total Reward = 23.0\n",
      "Episode 73, Total Reward = 53.0\n",
      "Episode 74, Total Reward = 44.0\n",
      "Episode 75, Total Reward = 72.0\n",
      "Episode 76, Total Reward = 26.0\n",
      "Episode 77, Total Reward = 5.0\n",
      "Episode 78, Total Reward = 48.0\n",
      "Episode 79, Total Reward = 16.0\n",
      "Episode 80, Total Reward = 10.0\n",
      "Episode 81, Total Reward = 61.0\n",
      "Episode 82, Total Reward = 45.0\n",
      "Episode 83, Total Reward = 42.0\n",
      "Episode 84, Total Reward = 22.0\n",
      "Episode 85, Total Reward = 31.0\n",
      "Episode 86, Total Reward = 40.0\n",
      "Episode 87, Total Reward = 17.0\n",
      "Episode 88, Total Reward = 44.0\n",
      "Episode 89, Total Reward = 45.0\n",
      "Episode 90, Total Reward = 23.0\n",
      "Episode 91, Total Reward = 41.0\n",
      "Episode 92, Total Reward = 83.0\n",
      "Episode 93, Total Reward = 13.0\n",
      "Episode 94, Total Reward = 69.0\n",
      "Episode 95, Total Reward = 82.0\n",
      "Episode 96, Total Reward = 115.0\n",
      "Episode 97, Total Reward = 72.0\n",
      "Episode 98, Total Reward = 40.0\n",
      "Episode 99, Total Reward = 110.0\n",
      "Episode 100, Total Reward = 48.0\n",
      "Running experiment with lr=0.0003, epsilon=0.2, k_epochs=10, batch_size=128\n",
      "Episode 1, Total Reward = 5.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 7.0\n",
      "Episode 4, Total Reward = 13.0\n",
      "Episode 5, Total Reward = 13.0\n",
      "Episode 6, Total Reward = 10.0\n",
      "Episode 7, Total Reward = 9.0\n",
      "Episode 8, Total Reward = 32.0\n",
      "Episode 9, Total Reward = 10.0\n",
      "Episode 10, Total Reward = 14.0\n",
      "Episode 11, Total Reward = 23.0\n",
      "Episode 12, Total Reward = 9.0\n",
      "Episode 13, Total Reward = 11.0\n",
      "Episode 14, Total Reward = 4.0\n",
      "Episode 15, Total Reward = 11.0\n",
      "Episode 16, Total Reward = 12.0\n",
      "Episode 17, Total Reward = 11.0\n",
      "Episode 18, Total Reward = 32.0\n",
      "Episode 19, Total Reward = 10.0\n",
      "Episode 20, Total Reward = 7.0\n",
      "Episode 21, Total Reward = 8.0\n",
      "Episode 22, Total Reward = 5.0\n",
      "Episode 23, Total Reward = 4.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 11.0\n",
      "Episode 26, Total Reward = 5.0\n",
      "Episode 27, Total Reward = 6.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 7.0\n",
      "Episode 30, Total Reward = 6.0\n",
      "Episode 31, Total Reward = 4.0\n",
      "Episode 32, Total Reward = 38.0\n",
      "Episode 33, Total Reward = 46.0\n",
      "Episode 34, Total Reward = 10.0\n",
      "Episode 35, Total Reward = 17.0\n",
      "Episode 36, Total Reward = 11.0\n",
      "Episode 37, Total Reward = 5.0\n",
      "Episode 38, Total Reward = 16.0\n",
      "Episode 39, Total Reward = 14.0\n",
      "Episode 40, Total Reward = 17.0\n",
      "Episode 41, Total Reward = 27.0\n",
      "Episode 42, Total Reward = 9.0\n",
      "Episode 43, Total Reward = 4.0\n",
      "Episode 44, Total Reward = 22.0\n",
      "Episode 45, Total Reward = 19.0\n",
      "Episode 46, Total Reward = 35.0\n",
      "Episode 47, Total Reward = 16.0\n",
      "Episode 48, Total Reward = 6.0\n",
      "Episode 49, Total Reward = 8.0\n",
      "Episode 50, Total Reward = 38.0\n",
      "Episode 51, Total Reward = 17.0\n",
      "Episode 52, Total Reward = 45.0\n",
      "Episode 53, Total Reward = 16.0\n",
      "Episode 54, Total Reward = 33.0\n",
      "Episode 55, Total Reward = 35.0\n",
      "Episode 56, Total Reward = 36.0\n",
      "Episode 57, Total Reward = 19.0\n",
      "Episode 58, Total Reward = 18.0\n",
      "Episode 59, Total Reward = 45.0\n",
      "Episode 60, Total Reward = 24.0\n",
      "Episode 61, Total Reward = 40.0\n",
      "Episode 62, Total Reward = 31.0\n",
      "Episode 63, Total Reward = 27.0\n",
      "Episode 64, Total Reward = 38.0\n",
      "Episode 65, Total Reward = 12.0\n",
      "Episode 66, Total Reward = 42.0\n",
      "Episode 67, Total Reward = 40.0\n",
      "Episode 68, Total Reward = 66.0\n",
      "Episode 69, Total Reward = 42.0\n",
      "Episode 70, Total Reward = 45.0\n",
      "Episode 71, Total Reward = 35.0\n",
      "Episode 72, Total Reward = 15.0\n",
      "Episode 73, Total Reward = 66.0\n",
      "Episode 74, Total Reward = 36.0\n",
      "Episode 75, Total Reward = 27.0\n",
      "Episode 76, Total Reward = 63.0\n",
      "Episode 77, Total Reward = 73.0\n",
      "Episode 78, Total Reward = 51.0\n",
      "Episode 79, Total Reward = 62.0\n",
      "Episode 80, Total Reward = 11.0\n",
      "Episode 81, Total Reward = 48.0\n",
      "Episode 82, Total Reward = 46.0\n",
      "Episode 83, Total Reward = 55.0\n",
      "Episode 84, Total Reward = 55.0\n",
      "Episode 85, Total Reward = 113.0\n",
      "Episode 86, Total Reward = 23.0\n",
      "Episode 87, Total Reward = 84.0\n",
      "Episode 88, Total Reward = 30.0\n",
      "Episode 89, Total Reward = 68.0\n",
      "Episode 90, Total Reward = 63.0\n",
      "Episode 91, Total Reward = 81.0\n",
      "Episode 92, Total Reward = 30.0\n",
      "Episode 93, Total Reward = 58.0\n",
      "Episode 94, Total Reward = 47.0\n",
      "Episode 95, Total Reward = 59.0\n",
      "Episode 96, Total Reward = 60.0\n",
      "Episode 97, Total Reward = 80.0\n",
      "Episode 98, Total Reward = 57.0\n",
      "Episode 99, Total Reward = 49.0\n",
      "Episode 100, Total Reward = 82.0\n",
      "Running experiment with lr=0.0003, epsilon=0.2, k_epochs=20, batch_size=32\n",
      "Episode 1, Total Reward = 4.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 3.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 7.0\n",
      "Episode 7, Total Reward = 7.0\n",
      "Episode 8, Total Reward = 5.0\n",
      "Episode 9, Total Reward = 10.0\n",
      "Episode 10, Total Reward = 6.0\n",
      "Episode 11, Total Reward = 9.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 4.0\n",
      "Episode 14, Total Reward = 4.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 7.0\n",
      "Episode 17, Total Reward = 5.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 7.0\n",
      "Episode 20, Total Reward = 13.0\n",
      "Episode 21, Total Reward = 7.0\n",
      "Episode 22, Total Reward = 8.0\n",
      "Episode 23, Total Reward = 10.0\n",
      "Episode 24, Total Reward = 4.0\n",
      "Episode 25, Total Reward = 6.0\n",
      "Episode 26, Total Reward = 4.0\n",
      "Episode 27, Total Reward = 13.0\n",
      "Episode 28, Total Reward = 12.0\n",
      "Episode 29, Total Reward = 4.0\n",
      "Episode 30, Total Reward = 10.0\n",
      "Episode 31, Total Reward = 19.0\n",
      "Episode 32, Total Reward = 7.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 9.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 6.0\n",
      "Episode 37, Total Reward = 18.0\n",
      "Episode 38, Total Reward = 25.0\n",
      "Episode 39, Total Reward = 9.0\n",
      "Episode 40, Total Reward = 18.0\n",
      "Episode 41, Total Reward = 30.0\n",
      "Episode 42, Total Reward = 15.0\n",
      "Episode 43, Total Reward = 9.0\n",
      "Episode 44, Total Reward = 6.0\n",
      "Episode 45, Total Reward = 8.0\n",
      "Episode 46, Total Reward = 43.0\n",
      "Episode 47, Total Reward = 5.0\n",
      "Episode 48, Total Reward = 19.0\n",
      "Episode 49, Total Reward = 49.0\n",
      "Episode 50, Total Reward = 32.0\n",
      "Episode 51, Total Reward = 23.0\n",
      "Episode 52, Total Reward = 29.0\n",
      "Episode 53, Total Reward = 29.0\n",
      "Episode 54, Total Reward = 59.0\n",
      "Episode 55, Total Reward = 33.0\n",
      "Episode 56, Total Reward = 38.0\n",
      "Episode 57, Total Reward = 35.0\n",
      "Episode 58, Total Reward = 50.0\n",
      "Episode 59, Total Reward = 19.0\n",
      "Episode 60, Total Reward = 69.0\n",
      "Episode 61, Total Reward = 114.0\n",
      "Episode 62, Total Reward = 51.0\n",
      "Episode 63, Total Reward = 89.0\n",
      "Episode 64, Total Reward = 9.0\n",
      "Episode 65, Total Reward = 100.0\n",
      "Episode 66, Total Reward = 52.0\n",
      "Episode 67, Total Reward = 85.0\n",
      "Episode 68, Total Reward = 62.0\n",
      "Episode 69, Total Reward = 107.0\n",
      "Episode 70, Total Reward = 129.0\n",
      "Episode 71, Total Reward = 64.0\n",
      "Episode 72, Total Reward = 111.0\n",
      "Episode 73, Total Reward = 58.0\n",
      "Episode 74, Total Reward = 54.0\n",
      "Episode 75, Total Reward = 61.0\n",
      "Episode 76, Total Reward = 63.0\n",
      "Episode 77, Total Reward = 68.0\n",
      "Episode 78, Total Reward = 215.0\n",
      "Episode 79, Total Reward = 46.0\n",
      "Episode 80, Total Reward = 155.0\n",
      "Episode 81, Total Reward = 254.0\n",
      "Episode 82, Total Reward = 124.0\n",
      "Episode 83, Total Reward = 157.0\n",
      "Episode 84, Total Reward = 38.0\n",
      "Episode 85, Total Reward = 40.0\n",
      "Episode 86, Total Reward = 148.0\n",
      "Episode 87, Total Reward = 84.0\n",
      "Episode 88, Total Reward = 102.0\n",
      "Episode 89, Total Reward = 47.0\n",
      "Episode 90, Total Reward = 209.0\n",
      "Episode 91, Total Reward = 86.0\n",
      "Episode 92, Total Reward = 192.0\n",
      "Episode 93, Total Reward = 69.0\n",
      "Episode 94, Total Reward = 249.0\n",
      "Episode 95, Total Reward = 50.0\n",
      "Episode 96, Total Reward = 79.0\n",
      "Episode 97, Total Reward = 183.0\n",
      "Episode 98, Total Reward = 98.0\n",
      "Episode 99, Total Reward = 136.0\n",
      "Episode 100, Total Reward = 322.0\n",
      "Running experiment with lr=0.0003, epsilon=0.2, k_epochs=20, batch_size=64\n",
      "Episode 1, Total Reward = 6.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 12.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 9.0\n",
      "Episode 8, Total Reward = 12.0\n",
      "Episode 9, Total Reward = 3.0\n",
      "Episode 10, Total Reward = 15.0\n",
      "Episode 11, Total Reward = 6.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 9.0\n",
      "Episode 15, Total Reward = 17.0\n",
      "Episode 16, Total Reward = 3.0\n",
      "Episode 17, Total Reward = 17.0\n",
      "Episode 18, Total Reward = 6.0\n",
      "Episode 19, Total Reward = 10.0\n",
      "Episode 20, Total Reward = 3.0\n",
      "Episode 21, Total Reward = 7.0\n",
      "Episode 22, Total Reward = 8.0\n",
      "Episode 23, Total Reward = 5.0\n",
      "Episode 24, Total Reward = 12.0\n",
      "Episode 25, Total Reward = 17.0\n",
      "Episode 26, Total Reward = 8.0\n",
      "Episode 27, Total Reward = 8.0\n",
      "Episode 28, Total Reward = 15.0\n",
      "Episode 29, Total Reward = 9.0\n",
      "Episode 30, Total Reward = 9.0\n",
      "Episode 31, Total Reward = 25.0\n",
      "Episode 32, Total Reward = 39.0\n",
      "Episode 33, Total Reward = 35.0\n",
      "Episode 34, Total Reward = 10.0\n",
      "Episode 35, Total Reward = 15.0\n",
      "Episode 36, Total Reward = 4.0\n",
      "Episode 37, Total Reward = 9.0\n",
      "Episode 38, Total Reward = 18.0\n",
      "Episode 39, Total Reward = 13.0\n",
      "Episode 40, Total Reward = 10.0\n",
      "Episode 41, Total Reward = 39.0\n",
      "Episode 42, Total Reward = 14.0\n",
      "Episode 43, Total Reward = 38.0\n",
      "Episode 44, Total Reward = 9.0\n",
      "Episode 45, Total Reward = 35.0\n",
      "Episode 46, Total Reward = 13.0\n",
      "Episode 47, Total Reward = 41.0\n",
      "Episode 48, Total Reward = 20.0\n",
      "Episode 49, Total Reward = 27.0\n",
      "Episode 50, Total Reward = 8.0\n",
      "Episode 51, Total Reward = 87.0\n",
      "Episode 52, Total Reward = 73.0\n",
      "Episode 53, Total Reward = 41.0\n",
      "Episode 54, Total Reward = 15.0\n",
      "Episode 55, Total Reward = 49.0\n",
      "Episode 56, Total Reward = 58.0\n",
      "Episode 57, Total Reward = 52.0\n",
      "Episode 58, Total Reward = 37.0\n",
      "Episode 59, Total Reward = 34.0\n",
      "Episode 60, Total Reward = 56.0\n",
      "Episode 61, Total Reward = 42.0\n",
      "Episode 62, Total Reward = 16.0\n",
      "Episode 63, Total Reward = 11.0\n",
      "Episode 64, Total Reward = 47.0\n",
      "Episode 65, Total Reward = 104.0\n",
      "Episode 66, Total Reward = 105.0\n",
      "Episode 67, Total Reward = 77.0\n",
      "Episode 68, Total Reward = 78.0\n",
      "Episode 69, Total Reward = 28.0\n",
      "Episode 70, Total Reward = 40.0\n",
      "Episode 71, Total Reward = 16.0\n",
      "Episode 72, Total Reward = 138.0\n",
      "Episode 73, Total Reward = 64.0\n",
      "Episode 74, Total Reward = 44.0\n",
      "Episode 75, Total Reward = 71.0\n",
      "Episode 76, Total Reward = 90.0\n",
      "Episode 77, Total Reward = 28.0\n",
      "Episode 78, Total Reward = 69.0\n",
      "Episode 79, Total Reward = 90.0\n",
      "Episode 80, Total Reward = 54.0\n",
      "Episode 81, Total Reward = 57.0\n",
      "Episode 82, Total Reward = 43.0\n",
      "Episode 83, Total Reward = 56.0\n",
      "Episode 84, Total Reward = 82.0\n",
      "Episode 85, Total Reward = 66.0\n",
      "Episode 86, Total Reward = 45.0\n",
      "Episode 87, Total Reward = 53.0\n",
      "Episode 88, Total Reward = 16.0\n",
      "Episode 89, Total Reward = 51.0\n",
      "Episode 90, Total Reward = 45.0\n",
      "Episode 91, Total Reward = 54.0\n",
      "Episode 92, Total Reward = 56.0\n",
      "Episode 93, Total Reward = 91.0\n",
      "Episode 94, Total Reward = 46.0\n",
      "Episode 95, Total Reward = 45.0\n",
      "Episode 96, Total Reward = 63.0\n",
      "Episode 97, Total Reward = 39.0\n",
      "Episode 98, Total Reward = 63.0\n",
      "Episode 99, Total Reward = 96.0\n",
      "Episode 100, Total Reward = 42.0\n",
      "Running experiment with lr=0.0003, epsilon=0.2, k_epochs=20, batch_size=128\n",
      "Episode 1, Total Reward = 9.0\n",
      "Episode 2, Total Reward = 12.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 9.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 4.0\n",
      "Episode 7, Total Reward = 6.0\n",
      "Episode 8, Total Reward = 19.0\n",
      "Episode 9, Total Reward = 9.0\n",
      "Episode 10, Total Reward = 16.0\n",
      "Episode 11, Total Reward = 23.0\n",
      "Episode 12, Total Reward = 6.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 12.0\n",
      "Episode 15, Total Reward = 15.0\n",
      "Episode 16, Total Reward = 16.0\n",
      "Episode 17, Total Reward = 8.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 19.0\n",
      "Episode 20, Total Reward = 17.0\n",
      "Episode 21, Total Reward = 8.0\n",
      "Episode 22, Total Reward = 13.0\n",
      "Episode 23, Total Reward = 8.0\n",
      "Episode 24, Total Reward = 22.0\n",
      "Episode 25, Total Reward = 8.0\n",
      "Episode 26, Total Reward = 14.0\n",
      "Episode 27, Total Reward = 12.0\n",
      "Episode 28, Total Reward = 11.0\n",
      "Episode 29, Total Reward = 9.0\n",
      "Episode 30, Total Reward = 17.0\n",
      "Episode 31, Total Reward = 14.0\n",
      "Episode 32, Total Reward = 7.0\n",
      "Episode 33, Total Reward = 7.0\n",
      "Episode 34, Total Reward = 5.0\n",
      "Episode 35, Total Reward = 30.0\n",
      "Episode 36, Total Reward = 20.0\n",
      "Episode 37, Total Reward = 31.0\n",
      "Episode 38, Total Reward = 46.0\n",
      "Episode 39, Total Reward = 17.0\n",
      "Episode 40, Total Reward = 53.0\n",
      "Episode 41, Total Reward = 26.0\n",
      "Episode 42, Total Reward = 37.0\n",
      "Episode 43, Total Reward = 8.0\n",
      "Episode 44, Total Reward = 37.0\n",
      "Episode 45, Total Reward = 11.0\n",
      "Episode 46, Total Reward = 29.0\n",
      "Episode 47, Total Reward = 47.0\n",
      "Episode 48, Total Reward = 40.0\n",
      "Episode 49, Total Reward = 75.0\n",
      "Episode 50, Total Reward = 39.0\n",
      "Episode 51, Total Reward = 83.0\n",
      "Episode 52, Total Reward = 25.0\n",
      "Episode 53, Total Reward = 154.0\n",
      "Episode 54, Total Reward = 49.0\n",
      "Episode 55, Total Reward = 49.0\n",
      "Episode 56, Total Reward = 42.0\n",
      "Episode 57, Total Reward = 40.0\n",
      "Episode 58, Total Reward = 24.0\n",
      "Episode 59, Total Reward = 63.0\n",
      "Episode 60, Total Reward = 61.0\n",
      "Episode 61, Total Reward = 84.0\n",
      "Episode 62, Total Reward = 88.0\n",
      "Episode 63, Total Reward = 56.0\n",
      "Episode 64, Total Reward = 54.0\n",
      "Episode 65, Total Reward = 103.0\n",
      "Episode 66, Total Reward = 61.0\n",
      "Episode 67, Total Reward = 75.0\n",
      "Episode 68, Total Reward = 116.0\n",
      "Episode 69, Total Reward = 108.0\n",
      "Episode 70, Total Reward = 42.0\n",
      "Episode 71, Total Reward = 45.0\n",
      "Episode 72, Total Reward = 56.0\n",
      "Episode 73, Total Reward = 59.0\n",
      "Episode 74, Total Reward = 93.0\n",
      "Episode 75, Total Reward = 82.0\n",
      "Episode 76, Total Reward = 148.0\n",
      "Episode 77, Total Reward = 7.0\n",
      "Episode 78, Total Reward = 49.0\n",
      "Episode 79, Total Reward = 39.0\n",
      "Episode 80, Total Reward = 79.0\n",
      "Episode 81, Total Reward = 59.0\n",
      "Episode 82, Total Reward = 104.0\n",
      "Episode 83, Total Reward = 28.0\n",
      "Episode 84, Total Reward = 66.0\n",
      "Episode 85, Total Reward = 83.0\n",
      "Episode 86, Total Reward = 136.0\n",
      "Episode 87, Total Reward = 90.0\n",
      "Episode 88, Total Reward = 106.0\n",
      "Episode 89, Total Reward = 75.0\n",
      "Episode 90, Total Reward = 93.0\n",
      "Episode 91, Total Reward = 57.0\n",
      "Episode 92, Total Reward = 11.0\n",
      "Episode 93, Total Reward = 57.0\n",
      "Episode 94, Total Reward = 89.0\n",
      "Episode 95, Total Reward = 66.0\n",
      "Episode 96, Total Reward = 79.0\n",
      "Episode 97, Total Reward = 98.0\n",
      "Episode 98, Total Reward = 47.0\n",
      "Episode 99, Total Reward = 138.0\n",
      "Episode 100, Total Reward = 240.0\n",
      "Running experiment with lr=0.0003, epsilon=0.3, k_epochs=5, batch_size=32\n",
      "Episode 1, Total Reward = 12.0\n",
      "Episode 2, Total Reward = 10.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 4.0\n",
      "Episode 5, Total Reward = 3.0\n",
      "Episode 6, Total Reward = 7.0\n",
      "Episode 7, Total Reward = 23.0\n",
      "Episode 8, Total Reward = 7.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 4.0\n",
      "Episode 11, Total Reward = 13.0\n",
      "Episode 12, Total Reward = 9.0\n",
      "Episode 13, Total Reward = 7.0\n",
      "Episode 14, Total Reward = 5.0\n",
      "Episode 15, Total Reward = 4.0\n",
      "Episode 16, Total Reward = 6.0\n",
      "Episode 17, Total Reward = 11.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 6.0\n",
      "Episode 20, Total Reward = 7.0\n",
      "Episode 21, Total Reward = 8.0\n",
      "Episode 22, Total Reward = 6.0\n",
      "Episode 23, Total Reward = 8.0\n",
      "Episode 24, Total Reward = 7.0\n",
      "Episode 25, Total Reward = 26.0\n",
      "Episode 26, Total Reward = 9.0\n",
      "Episode 27, Total Reward = 5.0\n",
      "Episode 28, Total Reward = 25.0\n",
      "Episode 29, Total Reward = 12.0\n",
      "Episode 30, Total Reward = 47.0\n",
      "Episode 31, Total Reward = 6.0\n",
      "Episode 32, Total Reward = 5.0\n",
      "Episode 33, Total Reward = 21.0\n",
      "Episode 34, Total Reward = 63.0\n",
      "Episode 35, Total Reward = 14.0\n",
      "Episode 36, Total Reward = 21.0\n",
      "Episode 37, Total Reward = 10.0\n",
      "Episode 38, Total Reward = 23.0\n",
      "Episode 39, Total Reward = 27.0\n",
      "Episode 40, Total Reward = 27.0\n",
      "Episode 41, Total Reward = 8.0\n",
      "Episode 42, Total Reward = 7.0\n",
      "Episode 43, Total Reward = 23.0\n",
      "Episode 44, Total Reward = 35.0\n",
      "Episode 45, Total Reward = 9.0\n",
      "Episode 46, Total Reward = 45.0\n",
      "Episode 47, Total Reward = 16.0\n",
      "Episode 48, Total Reward = 37.0\n",
      "Episode 49, Total Reward = 35.0\n",
      "Episode 50, Total Reward = 18.0\n",
      "Episode 51, Total Reward = 16.0\n",
      "Episode 52, Total Reward = 39.0\n",
      "Episode 53, Total Reward = 16.0\n",
      "Episode 54, Total Reward = 9.0\n",
      "Episode 55, Total Reward = 50.0\n",
      "Episode 56, Total Reward = 22.0\n",
      "Episode 57, Total Reward = 24.0\n",
      "Episode 58, Total Reward = 28.0\n",
      "Episode 59, Total Reward = 64.0\n",
      "Episode 60, Total Reward = 27.0\n",
      "Episode 61, Total Reward = 72.0\n",
      "Episode 62, Total Reward = 11.0\n",
      "Episode 63, Total Reward = 18.0\n",
      "Episode 64, Total Reward = 7.0\n",
      "Episode 65, Total Reward = 85.0\n",
      "Episode 66, Total Reward = 47.0\n",
      "Episode 67, Total Reward = 42.0\n",
      "Episode 68, Total Reward = 27.0\n",
      "Episode 69, Total Reward = 14.0\n",
      "Episode 70, Total Reward = 74.0\n",
      "Episode 71, Total Reward = 45.0\n",
      "Episode 72, Total Reward = 103.0\n",
      "Episode 73, Total Reward = 62.0\n",
      "Episode 74, Total Reward = 38.0\n",
      "Episode 75, Total Reward = 56.0\n",
      "Episode 76, Total Reward = 41.0\n",
      "Episode 77, Total Reward = 50.0\n",
      "Episode 78, Total Reward = 40.0\n",
      "Episode 79, Total Reward = 27.0\n",
      "Episode 80, Total Reward = 152.0\n",
      "Episode 81, Total Reward = 10.0\n",
      "Episode 82, Total Reward = 178.0\n",
      "Episode 83, Total Reward = 49.0\n",
      "Episode 84, Total Reward = 47.0\n",
      "Episode 85, Total Reward = 55.0\n",
      "Episode 86, Total Reward = 74.0\n",
      "Episode 87, Total Reward = 45.0\n",
      "Episode 88, Total Reward = 67.0\n",
      "Episode 89, Total Reward = 66.0\n",
      "Episode 90, Total Reward = 54.0\n",
      "Episode 91, Total Reward = 45.0\n",
      "Episode 92, Total Reward = 38.0\n",
      "Episode 93, Total Reward = 39.0\n",
      "Episode 94, Total Reward = 69.0\n",
      "Episode 95, Total Reward = 53.0\n",
      "Episode 96, Total Reward = 139.0\n",
      "Episode 97, Total Reward = 72.0\n",
      "Episode 98, Total Reward = 48.0\n",
      "Episode 99, Total Reward = 48.0\n",
      "Episode 100, Total Reward = 18.0\n",
      "Running experiment with lr=0.0003, epsilon=0.3, k_epochs=5, batch_size=64\n",
      "Episode 1, Total Reward = 15.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 7.0\n",
      "Episode 5, Total Reward = 9.0\n",
      "Episode 6, Total Reward = 4.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 7.0\n",
      "Episode 9, Total Reward = 12.0\n",
      "Episode 10, Total Reward = 8.0\n",
      "Episode 11, Total Reward = 17.0\n",
      "Episode 12, Total Reward = 11.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 18.0\n",
      "Episode 15, Total Reward = 17.0\n",
      "Episode 16, Total Reward = 4.0\n",
      "Episode 17, Total Reward = 16.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 5.0\n",
      "Episode 20, Total Reward = 16.0\n",
      "Episode 21, Total Reward = 11.0\n",
      "Episode 22, Total Reward = 9.0\n",
      "Episode 23, Total Reward = 13.0\n",
      "Episode 24, Total Reward = 17.0\n",
      "Episode 25, Total Reward = 5.0\n",
      "Episode 26, Total Reward = 26.0\n",
      "Episode 27, Total Reward = 6.0\n",
      "Episode 28, Total Reward = 11.0\n",
      "Episode 29, Total Reward = 7.0\n",
      "Episode 30, Total Reward = 5.0\n",
      "Episode 31, Total Reward = 20.0\n",
      "Episode 32, Total Reward = 7.0\n",
      "Episode 33, Total Reward = 9.0\n",
      "Episode 34, Total Reward = 7.0\n",
      "Episode 35, Total Reward = 12.0\n",
      "Episode 36, Total Reward = 12.0\n",
      "Episode 37, Total Reward = 7.0\n",
      "Episode 38, Total Reward = 22.0\n",
      "Episode 39, Total Reward = 11.0\n",
      "Episode 40, Total Reward = 7.0\n",
      "Episode 41, Total Reward = 6.0\n",
      "Episode 42, Total Reward = 8.0\n",
      "Episode 43, Total Reward = 7.0\n",
      "Episode 44, Total Reward = 8.0\n",
      "Episode 45, Total Reward = 11.0\n",
      "Episode 46, Total Reward = 5.0\n",
      "Episode 47, Total Reward = 5.0\n",
      "Episode 48, Total Reward = 16.0\n",
      "Episode 49, Total Reward = 5.0\n",
      "Episode 50, Total Reward = 25.0\n",
      "Episode 51, Total Reward = 11.0\n",
      "Episode 52, Total Reward = 9.0\n",
      "Episode 53, Total Reward = 11.0\n",
      "Episode 54, Total Reward = 13.0\n",
      "Episode 55, Total Reward = 11.0\n",
      "Episode 56, Total Reward = 9.0\n",
      "Episode 57, Total Reward = 8.0\n",
      "Episode 58, Total Reward = 11.0\n",
      "Episode 59, Total Reward = 14.0\n",
      "Episode 60, Total Reward = 24.0\n",
      "Episode 61, Total Reward = 18.0\n",
      "Episode 62, Total Reward = 24.0\n",
      "Episode 63, Total Reward = 27.0\n",
      "Episode 64, Total Reward = 14.0\n",
      "Episode 65, Total Reward = 20.0\n",
      "Episode 66, Total Reward = 25.0\n",
      "Episode 67, Total Reward = 52.0\n",
      "Episode 68, Total Reward = 14.0\n",
      "Episode 69, Total Reward = 24.0\n",
      "Episode 70, Total Reward = 27.0\n",
      "Episode 71, Total Reward = 12.0\n",
      "Episode 72, Total Reward = 8.0\n",
      "Episode 73, Total Reward = 23.0\n",
      "Episode 74, Total Reward = 18.0\n",
      "Episode 75, Total Reward = 5.0\n",
      "Episode 76, Total Reward = 43.0\n",
      "Episode 77, Total Reward = 31.0\n",
      "Episode 78, Total Reward = 24.0\n",
      "Episode 79, Total Reward = 7.0\n",
      "Episode 80, Total Reward = 18.0\n",
      "Episode 81, Total Reward = 16.0\n",
      "Episode 82, Total Reward = 33.0\n",
      "Episode 83, Total Reward = 8.0\n",
      "Episode 84, Total Reward = 41.0\n",
      "Episode 85, Total Reward = 18.0\n",
      "Episode 86, Total Reward = 22.0\n",
      "Episode 87, Total Reward = 38.0\n",
      "Episode 88, Total Reward = 30.0\n",
      "Episode 89, Total Reward = 59.0\n",
      "Episode 90, Total Reward = 55.0\n",
      "Episode 91, Total Reward = 39.0\n",
      "Episode 92, Total Reward = 26.0\n",
      "Episode 93, Total Reward = 57.0\n",
      "Episode 94, Total Reward = 50.0\n",
      "Episode 95, Total Reward = 103.0\n",
      "Episode 96, Total Reward = 18.0\n",
      "Episode 97, Total Reward = 39.0\n",
      "Episode 98, Total Reward = 20.0\n",
      "Episode 99, Total Reward = 16.0\n",
      "Episode 100, Total Reward = 64.0\n",
      "Running experiment with lr=0.0003, epsilon=0.3, k_epochs=5, batch_size=128\n",
      "Episode 1, Total Reward = 9.0\n",
      "Episode 2, Total Reward = 7.0\n",
      "Episode 3, Total Reward = 10.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 5.0\n",
      "Episode 6, Total Reward = 5.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 6.0\n",
      "Episode 9, Total Reward = 10.0\n",
      "Episode 10, Total Reward = 14.0\n",
      "Episode 11, Total Reward = 4.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 10.0\n",
      "Episode 16, Total Reward = 10.0\n",
      "Episode 17, Total Reward = 9.0\n",
      "Episode 18, Total Reward = 9.0\n",
      "Episode 19, Total Reward = 5.0\n",
      "Episode 20, Total Reward = 5.0\n",
      "Episode 21, Total Reward = 6.0\n",
      "Episode 22, Total Reward = 10.0\n",
      "Episode 23, Total Reward = 7.0\n",
      "Episode 24, Total Reward = 4.0\n",
      "Episode 25, Total Reward = 4.0\n",
      "Episode 26, Total Reward = 8.0\n",
      "Episode 27, Total Reward = 25.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 6.0\n",
      "Episode 31, Total Reward = 9.0\n",
      "Episode 32, Total Reward = 6.0\n",
      "Episode 33, Total Reward = 19.0\n",
      "Episode 34, Total Reward = 8.0\n",
      "Episode 35, Total Reward = 4.0\n",
      "Episode 36, Total Reward = 5.0\n",
      "Episode 37, Total Reward = 5.0\n",
      "Episode 38, Total Reward = 4.0\n",
      "Episode 39, Total Reward = 4.0\n",
      "Episode 40, Total Reward = 26.0\n",
      "Episode 41, Total Reward = 4.0\n",
      "Episode 42, Total Reward = 5.0\n",
      "Episode 43, Total Reward = 7.0\n",
      "Episode 44, Total Reward = 7.0\n",
      "Episode 45, Total Reward = 16.0\n",
      "Episode 46, Total Reward = 5.0\n",
      "Episode 47, Total Reward = 6.0\n",
      "Episode 48, Total Reward = 4.0\n",
      "Episode 49, Total Reward = 6.0\n",
      "Episode 50, Total Reward = 11.0\n",
      "Episode 51, Total Reward = 12.0\n",
      "Episode 52, Total Reward = 8.0\n",
      "Episode 53, Total Reward = 27.0\n",
      "Episode 54, Total Reward = 10.0\n",
      "Episode 55, Total Reward = 5.0\n",
      "Episode 56, Total Reward = 7.0\n",
      "Episode 57, Total Reward = 13.0\n",
      "Episode 58, Total Reward = 11.0\n",
      "Episode 59, Total Reward = 12.0\n",
      "Episode 60, Total Reward = 9.0\n",
      "Episode 61, Total Reward = 12.0\n",
      "Episode 62, Total Reward = 12.0\n",
      "Episode 63, Total Reward = 7.0\n",
      "Episode 64, Total Reward = 8.0\n",
      "Episode 65, Total Reward = 12.0\n",
      "Episode 66, Total Reward = 12.0\n",
      "Episode 67, Total Reward = 13.0\n",
      "Episode 68, Total Reward = 6.0\n",
      "Episode 69, Total Reward = 13.0\n",
      "Episode 70, Total Reward = 8.0\n",
      "Episode 71, Total Reward = 5.0\n",
      "Episode 72, Total Reward = 33.0\n",
      "Episode 73, Total Reward = 14.0\n",
      "Episode 74, Total Reward = 9.0\n",
      "Episode 75, Total Reward = 15.0\n",
      "Episode 76, Total Reward = 7.0\n",
      "Episode 77, Total Reward = 20.0\n",
      "Episode 78, Total Reward = 16.0\n",
      "Episode 79, Total Reward = 18.0\n",
      "Episode 80, Total Reward = 29.0\n",
      "Episode 81, Total Reward = 6.0\n",
      "Episode 82, Total Reward = 6.0\n",
      "Episode 83, Total Reward = 6.0\n",
      "Episode 84, Total Reward = 16.0\n",
      "Episode 85, Total Reward = 21.0\n",
      "Episode 86, Total Reward = 19.0\n",
      "Episode 87, Total Reward = 7.0\n",
      "Episode 88, Total Reward = 4.0\n",
      "Episode 89, Total Reward = 10.0\n",
      "Episode 90, Total Reward = 10.0\n",
      "Episode 91, Total Reward = 9.0\n",
      "Episode 92, Total Reward = 27.0\n",
      "Episode 93, Total Reward = 19.0\n",
      "Episode 94, Total Reward = 10.0\n",
      "Episode 95, Total Reward = 11.0\n",
      "Episode 96, Total Reward = 15.0\n",
      "Episode 97, Total Reward = 18.0\n",
      "Episode 98, Total Reward = 8.0\n",
      "Episode 99, Total Reward = 13.0\n",
      "Episode 100, Total Reward = 8.0\n",
      "Running experiment with lr=0.0003, epsilon=0.3, k_epochs=10, batch_size=32\n",
      "Episode 1, Total Reward = 14.0\n",
      "Episode 2, Total Reward = 9.0\n",
      "Episode 3, Total Reward = 7.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 20.0\n",
      "Episode 6, Total Reward = 12.0\n",
      "Episode 7, Total Reward = 9.0\n",
      "Episode 8, Total Reward = 16.0\n",
      "Episode 9, Total Reward = 18.0\n",
      "Episode 10, Total Reward = 7.0\n",
      "Episode 11, Total Reward = 5.0\n",
      "Episode 12, Total Reward = 33.0\n",
      "Episode 13, Total Reward = 34.0\n",
      "Episode 14, Total Reward = 50.0\n",
      "Episode 15, Total Reward = 33.0\n",
      "Episode 16, Total Reward = 26.0\n",
      "Episode 17, Total Reward = 8.0\n",
      "Episode 18, Total Reward = 12.0\n",
      "Episode 19, Total Reward = 42.0\n",
      "Episode 20, Total Reward = 17.0\n",
      "Episode 21, Total Reward = 10.0\n",
      "Episode 22, Total Reward = 28.0\n",
      "Episode 23, Total Reward = 19.0\n",
      "Episode 24, Total Reward = 21.0\n",
      "Episode 25, Total Reward = 19.0\n",
      "Episode 26, Total Reward = 14.0\n",
      "Episode 27, Total Reward = 33.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 12.0\n",
      "Episode 30, Total Reward = 21.0\n",
      "Episode 31, Total Reward = 6.0\n",
      "Episode 32, Total Reward = 46.0\n",
      "Episode 33, Total Reward = 27.0\n",
      "Episode 34, Total Reward = 56.0\n",
      "Episode 35, Total Reward = 39.0\n",
      "Episode 36, Total Reward = 16.0\n",
      "Episode 37, Total Reward = 13.0\n",
      "Episode 38, Total Reward = 42.0\n",
      "Episode 39, Total Reward = 38.0\n",
      "Episode 40, Total Reward = 54.0\n",
      "Episode 41, Total Reward = 65.0\n",
      "Episode 42, Total Reward = 35.0\n",
      "Episode 43, Total Reward = 49.0\n",
      "Episode 44, Total Reward = 46.0\n",
      "Episode 45, Total Reward = 23.0\n",
      "Episode 46, Total Reward = 72.0\n",
      "Episode 47, Total Reward = 51.0\n",
      "Episode 48, Total Reward = 102.0\n",
      "Episode 49, Total Reward = 28.0\n",
      "Episode 50, Total Reward = 23.0\n",
      "Episode 51, Total Reward = 97.0\n",
      "Episode 52, Total Reward = 73.0\n",
      "Episode 53, Total Reward = 103.0\n",
      "Episode 54, Total Reward = 90.0\n",
      "Episode 55, Total Reward = 45.0\n",
      "Episode 56, Total Reward = 97.0\n",
      "Episode 57, Total Reward = 75.0\n",
      "Episode 58, Total Reward = 48.0\n",
      "Episode 59, Total Reward = 28.0\n",
      "Episode 60, Total Reward = 36.0\n",
      "Episode 61, Total Reward = 53.0\n",
      "Episode 62, Total Reward = 65.0\n",
      "Episode 63, Total Reward = 81.0\n",
      "Episode 64, Total Reward = 67.0\n",
      "Episode 65, Total Reward = 72.0\n",
      "Episode 66, Total Reward = 81.0\n",
      "Episode 67, Total Reward = 76.0\n",
      "Episode 68, Total Reward = 58.0\n",
      "Episode 69, Total Reward = 256.0\n",
      "Episode 70, Total Reward = 119.0\n",
      "Episode 71, Total Reward = 80.0\n",
      "Episode 72, Total Reward = 192.0\n",
      "Episode 73, Total Reward = 138.0\n",
      "Episode 74, Total Reward = 65.0\n",
      "Episode 75, Total Reward = 54.0\n",
      "Episode 76, Total Reward = 81.0\n",
      "Episode 77, Total Reward = 67.0\n",
      "Episode 78, Total Reward = 58.0\n",
      "Episode 79, Total Reward = 35.0\n",
      "Episode 80, Total Reward = 19.0\n",
      "Episode 81, Total Reward = 9.0\n",
      "Episode 82, Total Reward = 43.0\n",
      "Episode 83, Total Reward = 17.0\n",
      "Episode 84, Total Reward = 52.0\n",
      "Episode 85, Total Reward = 98.0\n",
      "Episode 86, Total Reward = 67.0\n",
      "Episode 87, Total Reward = 125.0\n",
      "Episode 88, Total Reward = 108.0\n",
      "Episode 89, Total Reward = 80.0\n",
      "Episode 90, Total Reward = 89.0\n",
      "Episode 91, Total Reward = 251.0\n",
      "Episode 92, Total Reward = 70.0\n",
      "Episode 93, Total Reward = 31.0\n",
      "Episode 94, Total Reward = 66.0\n",
      "Episode 95, Total Reward = 95.0\n",
      "Episode 96, Total Reward = 72.0\n",
      "Episode 97, Total Reward = 56.0\n",
      "Episode 98, Total Reward = 65.0\n",
      "Episode 99, Total Reward = 112.0\n",
      "Episode 100, Total Reward = 91.0\n",
      "Running experiment with lr=0.0003, epsilon=0.3, k_epochs=10, batch_size=64\n",
      "Episode 1, Total Reward = 20.0\n",
      "Episode 2, Total Reward = 20.0\n",
      "Episode 3, Total Reward = 17.0\n",
      "Episode 4, Total Reward = 14.0\n",
      "Episode 5, Total Reward = 11.0\n",
      "Episode 6, Total Reward = 9.0\n",
      "Episode 7, Total Reward = 14.0\n",
      "Episode 8, Total Reward = 5.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 13.0\n",
      "Episode 11, Total Reward = 9.0\n",
      "Episode 12, Total Reward = 4.0\n",
      "Episode 13, Total Reward = 12.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 13.0\n",
      "Episode 16, Total Reward = 11.0\n",
      "Episode 17, Total Reward = 19.0\n",
      "Episode 18, Total Reward = 6.0\n",
      "Episode 19, Total Reward = 16.0\n",
      "Episode 20, Total Reward = 7.0\n",
      "Episode 21, Total Reward = 20.0\n",
      "Episode 22, Total Reward = 7.0\n",
      "Episode 23, Total Reward = 6.0\n",
      "Episode 24, Total Reward = 8.0\n",
      "Episode 25, Total Reward = 22.0\n",
      "Episode 26, Total Reward = 7.0\n",
      "Episode 27, Total Reward = 11.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 59.0\n",
      "Episode 30, Total Reward = 13.0\n",
      "Episode 31, Total Reward = 72.0\n",
      "Episode 32, Total Reward = 13.0\n",
      "Episode 33, Total Reward = 11.0\n",
      "Episode 34, Total Reward = 19.0\n",
      "Episode 35, Total Reward = 11.0\n",
      "Episode 36, Total Reward = 46.0\n",
      "Episode 37, Total Reward = 11.0\n",
      "Episode 38, Total Reward = 15.0\n",
      "Episode 39, Total Reward = 20.0\n",
      "Episode 40, Total Reward = 25.0\n",
      "Episode 41, Total Reward = 16.0\n",
      "Episode 42, Total Reward = 24.0\n",
      "Episode 43, Total Reward = 18.0\n",
      "Episode 44, Total Reward = 17.0\n",
      "Episode 45, Total Reward = 6.0\n",
      "Episode 46, Total Reward = 13.0\n",
      "Episode 47, Total Reward = 78.0\n",
      "Episode 48, Total Reward = 26.0\n",
      "Episode 49, Total Reward = 19.0\n",
      "Episode 50, Total Reward = 52.0\n",
      "Episode 51, Total Reward = 20.0\n",
      "Episode 52, Total Reward = 26.0\n",
      "Episode 53, Total Reward = 36.0\n",
      "Episode 54, Total Reward = 69.0\n",
      "Episode 55, Total Reward = 11.0\n",
      "Episode 56, Total Reward = 14.0\n",
      "Episode 57, Total Reward = 12.0\n",
      "Episode 58, Total Reward = 35.0\n",
      "Episode 59, Total Reward = 19.0\n",
      "Episode 60, Total Reward = 74.0\n",
      "Episode 61, Total Reward = 69.0\n",
      "Episode 62, Total Reward = 22.0\n",
      "Episode 63, Total Reward = 27.0\n",
      "Episode 64, Total Reward = 20.0\n",
      "Episode 65, Total Reward = 39.0\n",
      "Episode 66, Total Reward = 44.0\n",
      "Episode 67, Total Reward = 15.0\n",
      "Episode 68, Total Reward = 54.0\n",
      "Episode 69, Total Reward = 62.0\n",
      "Episode 70, Total Reward = 41.0\n",
      "Episode 71, Total Reward = 62.0\n",
      "Episode 72, Total Reward = 47.0\n",
      "Episode 73, Total Reward = 26.0\n",
      "Episode 74, Total Reward = 65.0\n",
      "Episode 75, Total Reward = 53.0\n",
      "Episode 76, Total Reward = 59.0\n",
      "Episode 77, Total Reward = 104.0\n",
      "Episode 78, Total Reward = 42.0\n",
      "Episode 79, Total Reward = 59.0\n",
      "Episode 80, Total Reward = 21.0\n",
      "Episode 81, Total Reward = 61.0\n",
      "Episode 82, Total Reward = 51.0\n",
      "Episode 83, Total Reward = 48.0\n",
      "Episode 84, Total Reward = 80.0\n",
      "Episode 85, Total Reward = 85.0\n",
      "Episode 86, Total Reward = 83.0\n",
      "Episode 87, Total Reward = 76.0\n",
      "Episode 88, Total Reward = 117.0\n",
      "Episode 89, Total Reward = 78.0\n",
      "Episode 90, Total Reward = 46.0\n",
      "Episode 91, Total Reward = 56.0\n",
      "Episode 92, Total Reward = 23.0\n",
      "Episode 93, Total Reward = 77.0\n",
      "Episode 94, Total Reward = 58.0\n",
      "Episode 95, Total Reward = 176.0\n",
      "Episode 96, Total Reward = 75.0\n",
      "Episode 97, Total Reward = 93.0\n",
      "Episode 98, Total Reward = 67.0\n",
      "Episode 99, Total Reward = 98.0\n",
      "Episode 100, Total Reward = 55.0\n",
      "Running experiment with lr=0.0003, epsilon=0.3, k_epochs=10, batch_size=128\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 4.0\n",
      "Episode 3, Total Reward = 5.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 3.0\n",
      "Episode 6, Total Reward = 9.0\n",
      "Episode 7, Total Reward = 6.0\n",
      "Episode 8, Total Reward = 16.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 7.0\n",
      "Episode 11, Total Reward = 13.0\n",
      "Episode 12, Total Reward = 11.0\n",
      "Episode 13, Total Reward = 6.0\n",
      "Episode 14, Total Reward = 15.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 4.0\n",
      "Episode 17, Total Reward = 9.0\n",
      "Episode 18, Total Reward = 8.0\n",
      "Episode 19, Total Reward = 7.0\n",
      "Episode 20, Total Reward = 9.0\n",
      "Episode 21, Total Reward = 17.0\n",
      "Episode 22, Total Reward = 13.0\n",
      "Episode 23, Total Reward = 22.0\n",
      "Episode 24, Total Reward = 4.0\n",
      "Episode 25, Total Reward = 8.0\n",
      "Episode 26, Total Reward = 36.0\n",
      "Episode 27, Total Reward = 10.0\n",
      "Episode 28, Total Reward = 19.0\n",
      "Episode 29, Total Reward = 9.0\n",
      "Episode 30, Total Reward = 21.0\n",
      "Episode 31, Total Reward = 5.0\n",
      "Episode 32, Total Reward = 14.0\n",
      "Episode 33, Total Reward = 20.0\n",
      "Episode 34, Total Reward = 8.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 22.0\n",
      "Episode 37, Total Reward = 19.0\n",
      "Episode 38, Total Reward = 14.0\n",
      "Episode 39, Total Reward = 8.0\n",
      "Episode 40, Total Reward = 14.0\n",
      "Episode 41, Total Reward = 11.0\n",
      "Episode 42, Total Reward = 21.0\n",
      "Episode 43, Total Reward = 74.0\n",
      "Episode 44, Total Reward = 18.0\n",
      "Episode 45, Total Reward = 16.0\n",
      "Episode 46, Total Reward = 18.0\n",
      "Episode 47, Total Reward = 34.0\n",
      "Episode 48, Total Reward = 19.0\n",
      "Episode 49, Total Reward = 8.0\n",
      "Episode 50, Total Reward = 17.0\n",
      "Episode 51, Total Reward = 10.0\n",
      "Episode 52, Total Reward = 47.0\n",
      "Episode 53, Total Reward = 41.0\n",
      "Episode 54, Total Reward = 49.0\n",
      "Episode 55, Total Reward = 43.0\n",
      "Episode 56, Total Reward = 13.0\n",
      "Episode 57, Total Reward = 15.0\n",
      "Episode 58, Total Reward = 24.0\n",
      "Episode 59, Total Reward = 10.0\n",
      "Episode 60, Total Reward = 12.0\n",
      "Episode 61, Total Reward = 24.0\n",
      "Episode 62, Total Reward = 9.0\n",
      "Episode 63, Total Reward = 42.0\n",
      "Episode 64, Total Reward = 24.0\n",
      "Episode 65, Total Reward = 17.0\n",
      "Episode 66, Total Reward = 58.0\n",
      "Episode 67, Total Reward = 47.0\n",
      "Episode 68, Total Reward = 33.0\n",
      "Episode 69, Total Reward = 6.0\n",
      "Episode 70, Total Reward = 26.0\n",
      "Episode 71, Total Reward = 27.0\n",
      "Episode 72, Total Reward = 26.0\n",
      "Episode 73, Total Reward = 18.0\n",
      "Episode 74, Total Reward = 26.0\n",
      "Episode 75, Total Reward = 91.0\n",
      "Episode 76, Total Reward = 39.0\n",
      "Episode 77, Total Reward = 55.0\n",
      "Episode 78, Total Reward = 34.0\n",
      "Episode 79, Total Reward = 18.0\n",
      "Episode 80, Total Reward = 20.0\n",
      "Episode 81, Total Reward = 54.0\n",
      "Episode 82, Total Reward = 35.0\n",
      "Episode 83, Total Reward = 70.0\n",
      "Episode 84, Total Reward = 38.0\n",
      "Episode 85, Total Reward = 75.0\n",
      "Episode 86, Total Reward = 69.0\n",
      "Episode 87, Total Reward = 50.0\n",
      "Episode 88, Total Reward = 15.0\n",
      "Episode 89, Total Reward = 11.0\n",
      "Episode 90, Total Reward = 8.0\n",
      "Episode 91, Total Reward = 56.0\n",
      "Episode 92, Total Reward = 70.0\n",
      "Episode 93, Total Reward = 21.0\n",
      "Episode 94, Total Reward = 11.0\n",
      "Episode 95, Total Reward = 20.0\n",
      "Episode 96, Total Reward = 44.0\n",
      "Episode 97, Total Reward = 45.0\n",
      "Episode 98, Total Reward = 48.0\n",
      "Episode 99, Total Reward = 46.0\n",
      "Episode 100, Total Reward = 76.0\n",
      "Running experiment with lr=0.0003, epsilon=0.3, k_epochs=20, batch_size=32\n",
      "Episode 1, Total Reward = 6.0\n",
      "Episode 2, Total Reward = 8.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 8.0\n",
      "Episode 5, Total Reward = 3.0\n",
      "Episode 6, Total Reward = 6.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 10.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 4.0\n",
      "Episode 11, Total Reward = 7.0\n",
      "Episode 12, Total Reward = 9.0\n",
      "Episode 13, Total Reward = 3.0\n",
      "Episode 14, Total Reward = 4.0\n",
      "Episode 15, Total Reward = 9.0\n",
      "Episode 16, Total Reward = 18.0\n",
      "Episode 17, Total Reward = 10.0\n",
      "Episode 18, Total Reward = 7.0\n",
      "Episode 19, Total Reward = 6.0\n",
      "Episode 20, Total Reward = 13.0\n",
      "Episode 21, Total Reward = 7.0\n",
      "Episode 22, Total Reward = 11.0\n",
      "Episode 23, Total Reward = 9.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 27.0\n",
      "Episode 26, Total Reward = 15.0\n",
      "Episode 27, Total Reward = 21.0\n",
      "Episode 28, Total Reward = 22.0\n",
      "Episode 29, Total Reward = 10.0\n",
      "Episode 30, Total Reward = 17.0\n",
      "Episode 31, Total Reward = 47.0\n",
      "Episode 32, Total Reward = 12.0\n",
      "Episode 33, Total Reward = 43.0\n",
      "Episode 34, Total Reward = 8.0\n",
      "Episode 35, Total Reward = 16.0\n",
      "Episode 36, Total Reward = 30.0\n",
      "Episode 37, Total Reward = 16.0\n",
      "Episode 38, Total Reward = 19.0\n",
      "Episode 39, Total Reward = 29.0\n",
      "Episode 40, Total Reward = 46.0\n",
      "Episode 41, Total Reward = 99.0\n",
      "Episode 42, Total Reward = 49.0\n",
      "Episode 43, Total Reward = 46.0\n",
      "Episode 44, Total Reward = 14.0\n",
      "Episode 45, Total Reward = 16.0\n",
      "Episode 46, Total Reward = 54.0\n",
      "Episode 47, Total Reward = 27.0\n",
      "Episode 48, Total Reward = 92.0\n",
      "Episode 49, Total Reward = 41.0\n",
      "Episode 50, Total Reward = 114.0\n",
      "Episode 51, Total Reward = 65.0\n",
      "Episode 52, Total Reward = 95.0\n",
      "Episode 53, Total Reward = 66.0\n",
      "Episode 54, Total Reward = 56.0\n",
      "Episode 55, Total Reward = 113.0\n",
      "Episode 56, Total Reward = 88.0\n",
      "Episode 57, Total Reward = 154.0\n",
      "Episode 58, Total Reward = 67.0\n",
      "Episode 59, Total Reward = 125.0\n",
      "Episode 60, Total Reward = 38.0\n",
      "Episode 61, Total Reward = 68.0\n",
      "Episode 62, Total Reward = 58.0\n",
      "Episode 63, Total Reward = 9.0\n",
      "Episode 64, Total Reward = 44.0\n",
      "Episode 65, Total Reward = 51.0\n",
      "Episode 66, Total Reward = 104.0\n",
      "Episode 67, Total Reward = 79.0\n",
      "Episode 68, Total Reward = 34.0\n",
      "Episode 69, Total Reward = 61.0\n",
      "Episode 70, Total Reward = 86.0\n",
      "Episode 71, Total Reward = 147.0\n",
      "Episode 72, Total Reward = 94.0\n",
      "Episode 73, Total Reward = 77.0\n",
      "Episode 74, Total Reward = 64.0\n",
      "Episode 75, Total Reward = 50.0\n",
      "Episode 76, Total Reward = 79.0\n",
      "Episode 77, Total Reward = 157.0\n",
      "Episode 78, Total Reward = 90.0\n",
      "Episode 79, Total Reward = 113.0\n",
      "Episode 80, Total Reward = 100.0\n",
      "Episode 81, Total Reward = 132.0\n",
      "Episode 82, Total Reward = 38.0\n",
      "Episode 83, Total Reward = 91.0\n",
      "Episode 84, Total Reward = 75.0\n",
      "Episode 85, Total Reward = 41.0\n",
      "Episode 86, Total Reward = 50.0\n",
      "Episode 87, Total Reward = 14.0\n",
      "Episode 88, Total Reward = 53.0\n",
      "Episode 89, Total Reward = 44.0\n",
      "Episode 90, Total Reward = 55.0\n",
      "Episode 91, Total Reward = 9.0\n",
      "Episode 92, Total Reward = 85.0\n",
      "Episode 93, Total Reward = 72.0\n",
      "Episode 94, Total Reward = 26.0\n",
      "Episode 95, Total Reward = 81.0\n",
      "Episode 96, Total Reward = 79.0\n",
      "Episode 97, Total Reward = 64.0\n",
      "Episode 98, Total Reward = 91.0\n",
      "Episode 99, Total Reward = 91.0\n",
      "Episode 100, Total Reward = 19.0\n",
      "Running experiment with lr=0.0003, epsilon=0.3, k_epochs=20, batch_size=64\n",
      "Episode 1, Total Reward = 18.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 12.0\n",
      "Episode 4, Total Reward = 15.0\n",
      "Episode 5, Total Reward = 8.0\n",
      "Episode 6, Total Reward = 12.0\n",
      "Episode 7, Total Reward = 12.0\n",
      "Episode 8, Total Reward = 5.0\n",
      "Episode 9, Total Reward = 5.0\n",
      "Episode 10, Total Reward = 31.0\n",
      "Episode 11, Total Reward = 6.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 6.0\n",
      "Episode 14, Total Reward = 9.0\n",
      "Episode 15, Total Reward = 16.0\n",
      "Episode 16, Total Reward = 8.0\n",
      "Episode 17, Total Reward = 21.0\n",
      "Episode 18, Total Reward = 9.0\n",
      "Episode 19, Total Reward = 19.0\n",
      "Episode 20, Total Reward = 33.0\n",
      "Episode 21, Total Reward = 26.0\n",
      "Episode 22, Total Reward = 57.0\n",
      "Episode 23, Total Reward = 35.0\n",
      "Episode 24, Total Reward = 33.0\n",
      "Episode 25, Total Reward = 32.0\n",
      "Episode 26, Total Reward = 16.0\n",
      "Episode 27, Total Reward = 17.0\n",
      "Episode 28, Total Reward = 19.0\n",
      "Episode 29, Total Reward = 19.0\n",
      "Episode 30, Total Reward = 15.0\n",
      "Episode 31, Total Reward = 23.0\n",
      "Episode 32, Total Reward = 52.0\n",
      "Episode 33, Total Reward = 51.0\n",
      "Episode 34, Total Reward = 25.0\n",
      "Episode 35, Total Reward = 37.0\n",
      "Episode 36, Total Reward = 14.0\n",
      "Episode 37, Total Reward = 50.0\n",
      "Episode 38, Total Reward = 45.0\n",
      "Episode 39, Total Reward = 10.0\n",
      "Episode 40, Total Reward = 106.0\n",
      "Episode 41, Total Reward = 84.0\n",
      "Episode 42, Total Reward = 45.0\n",
      "Episode 43, Total Reward = 86.0\n",
      "Episode 44, Total Reward = 48.0\n",
      "Episode 45, Total Reward = 42.0\n",
      "Episode 46, Total Reward = 44.0\n",
      "Episode 47, Total Reward = 14.0\n",
      "Episode 48, Total Reward = 13.0\n",
      "Episode 49, Total Reward = 15.0\n",
      "Episode 50, Total Reward = 50.0\n",
      "Episode 51, Total Reward = 67.0\n",
      "Episode 52, Total Reward = 50.0\n",
      "Episode 53, Total Reward = 34.0\n",
      "Episode 54, Total Reward = 59.0\n",
      "Episode 55, Total Reward = 78.0\n",
      "Episode 56, Total Reward = 70.0\n",
      "Episode 57, Total Reward = 55.0\n",
      "Episode 58, Total Reward = 49.0\n",
      "Episode 59, Total Reward = 75.0\n",
      "Episode 60, Total Reward = 134.0\n",
      "Episode 61, Total Reward = 66.0\n",
      "Episode 62, Total Reward = 124.0\n",
      "Episode 63, Total Reward = 165.0\n",
      "Episode 64, Total Reward = 50.0\n",
      "Episode 65, Total Reward = 91.0\n",
      "Episode 66, Total Reward = 289.0\n",
      "Episode 67, Total Reward = 51.0\n",
      "Episode 68, Total Reward = 11.0\n",
      "Episode 69, Total Reward = 117.0\n",
      "Episode 70, Total Reward = 74.0\n",
      "Episode 71, Total Reward = 56.0\n",
      "Episode 72, Total Reward = 157.0\n",
      "Episode 73, Total Reward = 115.0\n",
      "Episode 74, Total Reward = 86.0\n",
      "Episode 75, Total Reward = 96.0\n",
      "Episode 76, Total Reward = 66.0\n",
      "Episode 77, Total Reward = 14.0\n",
      "Episode 78, Total Reward = 67.0\n",
      "Episode 79, Total Reward = 158.0\n",
      "Episode 80, Total Reward = 48.0\n",
      "Episode 81, Total Reward = 87.0\n",
      "Episode 82, Total Reward = 107.0\n",
      "Episode 83, Total Reward = 80.0\n",
      "Episode 84, Total Reward = 156.0\n",
      "Episode 85, Total Reward = 138.0\n",
      "Episode 86, Total Reward = 86.0\n",
      "Episode 87, Total Reward = 105.0\n",
      "Episode 88, Total Reward = 124.0\n",
      "Episode 89, Total Reward = 100.0\n",
      "Episode 90, Total Reward = 93.0\n",
      "Episode 91, Total Reward = 124.0\n",
      "Episode 92, Total Reward = 124.0\n",
      "Episode 93, Total Reward = 139.0\n",
      "Episode 94, Total Reward = 126.0\n",
      "Episode 95, Total Reward = 108.0\n",
      "Episode 96, Total Reward = 115.0\n",
      "Episode 97, Total Reward = 85.0\n",
      "Episode 98, Total Reward = 114.0\n",
      "Episode 99, Total Reward = 114.0\n",
      "Episode 100, Total Reward = 95.0\n",
      "Running experiment with lr=0.0003, epsilon=0.3, k_epochs=20, batch_size=128\n",
      "Episode 1, Total Reward = 8.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 7.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 9.0\n",
      "Episode 6, Total Reward = 8.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 6.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 7.0\n",
      "Episode 11, Total Reward = 3.0\n",
      "Episode 12, Total Reward = 18.0\n",
      "Episode 13, Total Reward = 29.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 4.0\n",
      "Episode 16, Total Reward = 20.0\n",
      "Episode 17, Total Reward = 12.0\n",
      "Episode 18, Total Reward = 19.0\n",
      "Episode 19, Total Reward = 6.0\n",
      "Episode 20, Total Reward = 8.0\n",
      "Episode 21, Total Reward = 10.0\n",
      "Episode 22, Total Reward = 10.0\n",
      "Episode 23, Total Reward = 41.0\n",
      "Episode 24, Total Reward = 19.0\n",
      "Episode 25, Total Reward = 8.0\n",
      "Episode 26, Total Reward = 47.0\n",
      "Episode 27, Total Reward = 23.0\n",
      "Episode 28, Total Reward = 47.0\n",
      "Episode 29, Total Reward = 12.0\n",
      "Episode 30, Total Reward = 16.0\n",
      "Episode 31, Total Reward = 46.0\n",
      "Episode 32, Total Reward = 32.0\n",
      "Episode 33, Total Reward = 43.0\n",
      "Episode 34, Total Reward = 20.0\n",
      "Episode 35, Total Reward = 11.0\n",
      "Episode 36, Total Reward = 51.0\n",
      "Episode 37, Total Reward = 36.0\n",
      "Episode 38, Total Reward = 78.0\n",
      "Episode 39, Total Reward = 46.0\n",
      "Episode 40, Total Reward = 83.0\n",
      "Episode 41, Total Reward = 42.0\n",
      "Episode 42, Total Reward = 48.0\n",
      "Episode 43, Total Reward = 38.0\n",
      "Episode 44, Total Reward = 15.0\n",
      "Episode 45, Total Reward = 45.0\n",
      "Episode 46, Total Reward = 45.0\n",
      "Episode 47, Total Reward = 16.0\n",
      "Episode 48, Total Reward = 33.0\n",
      "Episode 49, Total Reward = 8.0\n",
      "Episode 50, Total Reward = 41.0\n",
      "Episode 51, Total Reward = 50.0\n",
      "Episode 52, Total Reward = 90.0\n",
      "Episode 53, Total Reward = 16.0\n",
      "Episode 54, Total Reward = 58.0\n",
      "Episode 55, Total Reward = 101.0\n",
      "Episode 56, Total Reward = 54.0\n",
      "Episode 57, Total Reward = 67.0\n",
      "Episode 58, Total Reward = 45.0\n",
      "Episode 59, Total Reward = 27.0\n",
      "Episode 60, Total Reward = 69.0\n",
      "Episode 61, Total Reward = 83.0\n",
      "Episode 62, Total Reward = 37.0\n",
      "Episode 63, Total Reward = 54.0\n",
      "Episode 64, Total Reward = 58.0\n",
      "Episode 65, Total Reward = 57.0\n",
      "Episode 66, Total Reward = 24.0\n",
      "Episode 67, Total Reward = 99.0\n",
      "Episode 68, Total Reward = 25.0\n",
      "Episode 69, Total Reward = 22.0\n",
      "Episode 70, Total Reward = 19.0\n",
      "Episode 71, Total Reward = 6.0\n",
      "Episode 72, Total Reward = 45.0\n",
      "Episode 73, Total Reward = 83.0\n",
      "Episode 74, Total Reward = 12.0\n",
      "Episode 75, Total Reward = 49.0\n",
      "Episode 76, Total Reward = 81.0\n",
      "Episode 77, Total Reward = 48.0\n",
      "Episode 78, Total Reward = 13.0\n",
      "Episode 79, Total Reward = 129.0\n",
      "Episode 80, Total Reward = 148.0\n",
      "Episode 81, Total Reward = 77.0\n",
      "Episode 82, Total Reward = 56.0\n",
      "Episode 83, Total Reward = 11.0\n",
      "Episode 84, Total Reward = 63.0\n",
      "Episode 85, Total Reward = 42.0\n",
      "Episode 86, Total Reward = 54.0\n",
      "Episode 87, Total Reward = 57.0\n",
      "Episode 88, Total Reward = 54.0\n",
      "Episode 89, Total Reward = 46.0\n",
      "Episode 90, Total Reward = 54.0\n",
      "Episode 91, Total Reward = 61.0\n",
      "Episode 92, Total Reward = 52.0\n",
      "Episode 93, Total Reward = 44.0\n",
      "Episode 94, Total Reward = 22.0\n",
      "Episode 95, Total Reward = 39.0\n",
      "Episode 96, Total Reward = 42.0\n",
      "Episode 97, Total Reward = 42.0\n",
      "Episode 98, Total Reward = 41.0\n",
      "Episode 99, Total Reward = 42.0\n",
      "Episode 100, Total Reward = 41.0\n",
      "Running experiment with lr=0.001, epsilon=0.1, k_epochs=5, batch_size=32\n",
      "Episode 1, Total Reward = 10.0\n",
      "Episode 2, Total Reward = 7.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 8.0\n",
      "Episode 6, Total Reward = 10.0\n",
      "Episode 7, Total Reward = 7.0\n",
      "Episode 8, Total Reward = 7.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 9.0\n",
      "Episode 11, Total Reward = 11.0\n",
      "Episode 12, Total Reward = 19.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 4.0\n",
      "Episode 16, Total Reward = 31.0\n",
      "Episode 17, Total Reward = 9.0\n",
      "Episode 18, Total Reward = 26.0\n",
      "Episode 19, Total Reward = 13.0\n",
      "Episode 20, Total Reward = 7.0\n",
      "Episode 21, Total Reward = 15.0\n",
      "Episode 22, Total Reward = 9.0\n",
      "Episode 23, Total Reward = 4.0\n",
      "Episode 24, Total Reward = 17.0\n",
      "Episode 25, Total Reward = 13.0\n",
      "Episode 26, Total Reward = 17.0\n",
      "Episode 27, Total Reward = 15.0\n",
      "Episode 28, Total Reward = 28.0\n",
      "Episode 29, Total Reward = 7.0\n",
      "Episode 30, Total Reward = 9.0\n",
      "Episode 31, Total Reward = 7.0\n",
      "Episode 32, Total Reward = 5.0\n",
      "Episode 33, Total Reward = 14.0\n",
      "Episode 34, Total Reward = 6.0\n",
      "Episode 35, Total Reward = 14.0\n",
      "Episode 36, Total Reward = 15.0\n",
      "Episode 37, Total Reward = 5.0\n",
      "Episode 38, Total Reward = 13.0\n",
      "Episode 39, Total Reward = 10.0\n",
      "Episode 40, Total Reward = 26.0\n",
      "Episode 41, Total Reward = 30.0\n",
      "Episode 42, Total Reward = 11.0\n",
      "Episode 43, Total Reward = 33.0\n",
      "Episode 44, Total Reward = 6.0\n",
      "Episode 45, Total Reward = 19.0\n",
      "Episode 46, Total Reward = 6.0\n",
      "Episode 47, Total Reward = 19.0\n",
      "Episode 48, Total Reward = 10.0\n",
      "Episode 49, Total Reward = 10.0\n",
      "Episode 50, Total Reward = 19.0\n",
      "Episode 51, Total Reward = 7.0\n",
      "Episode 52, Total Reward = 18.0\n",
      "Episode 53, Total Reward = 61.0\n",
      "Episode 54, Total Reward = 11.0\n",
      "Episode 55, Total Reward = 34.0\n",
      "Episode 56, Total Reward = 19.0\n",
      "Episode 57, Total Reward = 42.0\n",
      "Episode 58, Total Reward = 71.0\n",
      "Episode 59, Total Reward = 15.0\n",
      "Episode 60, Total Reward = 53.0\n",
      "Episode 61, Total Reward = 38.0\n",
      "Episode 62, Total Reward = 76.0\n",
      "Episode 63, Total Reward = 133.0\n",
      "Episode 64, Total Reward = 57.0\n",
      "Episode 65, Total Reward = 25.0\n",
      "Episode 66, Total Reward = 11.0\n",
      "Episode 67, Total Reward = 30.0\n",
      "Episode 68, Total Reward = 50.0\n",
      "Episode 69, Total Reward = 52.0\n",
      "Episode 70, Total Reward = 75.0\n",
      "Episode 71, Total Reward = 67.0\n",
      "Episode 72, Total Reward = 57.0\n",
      "Episode 73, Total Reward = 32.0\n",
      "Episode 74, Total Reward = 53.0\n",
      "Episode 75, Total Reward = 49.0\n",
      "Episode 76, Total Reward = 11.0\n",
      "Episode 77, Total Reward = 53.0\n",
      "Episode 78, Total Reward = 17.0\n",
      "Episode 79, Total Reward = 13.0\n",
      "Episode 80, Total Reward = 92.0\n",
      "Episode 81, Total Reward = 37.0\n",
      "Episode 82, Total Reward = 41.0\n",
      "Episode 83, Total Reward = 52.0\n",
      "Episode 84, Total Reward = 45.0\n",
      "Episode 85, Total Reward = 64.0\n",
      "Episode 86, Total Reward = 56.0\n",
      "Episode 87, Total Reward = 48.0\n",
      "Episode 88, Total Reward = 66.0\n",
      "Episode 89, Total Reward = 67.0\n",
      "Episode 90, Total Reward = 49.0\n",
      "Episode 91, Total Reward = 14.0\n",
      "Episode 92, Total Reward = 54.0\n",
      "Episode 93, Total Reward = 79.0\n",
      "Episode 94, Total Reward = 136.0\n",
      "Episode 95, Total Reward = 67.0\n",
      "Episode 96, Total Reward = 106.0\n",
      "Episode 97, Total Reward = 75.0\n",
      "Episode 98, Total Reward = 114.0\n",
      "Episode 99, Total Reward = 70.0\n",
      "Episode 100, Total Reward = 52.0\n",
      "Running experiment with lr=0.001, epsilon=0.1, k_epochs=5, batch_size=64\n",
      "Episode 1, Total Reward = 11.0\n",
      "Episode 2, Total Reward = 15.0\n",
      "Episode 3, Total Reward = 13.0\n",
      "Episode 4, Total Reward = 18.0\n",
      "Episode 5, Total Reward = 7.0\n",
      "Episode 6, Total Reward = 7.0\n",
      "Episode 7, Total Reward = 6.0\n",
      "Episode 8, Total Reward = 5.0\n",
      "Episode 9, Total Reward = 11.0\n",
      "Episode 10, Total Reward = 4.0\n",
      "Episode 11, Total Reward = 10.0\n",
      "Episode 12, Total Reward = 23.0\n",
      "Episode 13, Total Reward = 4.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 5.0\n",
      "Episode 16, Total Reward = 5.0\n",
      "Episode 17, Total Reward = 9.0\n",
      "Episode 18, Total Reward = 18.0\n",
      "Episode 19, Total Reward = 8.0\n",
      "Episode 20, Total Reward = 6.0\n",
      "Episode 21, Total Reward = 26.0\n",
      "Episode 22, Total Reward = 5.0\n",
      "Episode 23, Total Reward = 21.0\n",
      "Episode 24, Total Reward = 9.0\n",
      "Episode 25, Total Reward = 18.0\n",
      "Episode 26, Total Reward = 45.0\n",
      "Episode 27, Total Reward = 61.0\n",
      "Episode 28, Total Reward = 7.0\n",
      "Episode 29, Total Reward = 13.0\n",
      "Episode 30, Total Reward = 20.0\n",
      "Episode 31, Total Reward = 15.0\n",
      "Episode 32, Total Reward = 7.0\n",
      "Episode 33, Total Reward = 19.0\n",
      "Episode 34, Total Reward = 8.0\n",
      "Episode 35, Total Reward = 5.0\n",
      "Episode 36, Total Reward = 15.0\n",
      "Episode 37, Total Reward = 24.0\n",
      "Episode 38, Total Reward = 9.0\n",
      "Episode 39, Total Reward = 13.0\n",
      "Episode 40, Total Reward = 28.0\n",
      "Episode 41, Total Reward = 23.0\n",
      "Episode 42, Total Reward = 22.0\n",
      "Episode 43, Total Reward = 12.0\n",
      "Episode 44, Total Reward = 14.0\n",
      "Episode 45, Total Reward = 19.0\n",
      "Episode 46, Total Reward = 38.0\n",
      "Episode 47, Total Reward = 21.0\n",
      "Episode 48, Total Reward = 16.0\n",
      "Episode 49, Total Reward = 10.0\n",
      "Episode 50, Total Reward = 11.0\n",
      "Episode 51, Total Reward = 15.0\n",
      "Episode 52, Total Reward = 41.0\n",
      "Episode 53, Total Reward = 12.0\n",
      "Episode 54, Total Reward = 17.0\n",
      "Episode 55, Total Reward = 11.0\n",
      "Episode 56, Total Reward = 28.0\n",
      "Episode 57, Total Reward = 8.0\n",
      "Episode 58, Total Reward = 36.0\n",
      "Episode 59, Total Reward = 38.0\n",
      "Episode 60, Total Reward = 29.0\n",
      "Episode 61, Total Reward = 9.0\n",
      "Episode 62, Total Reward = 29.0\n",
      "Episode 63, Total Reward = 18.0\n",
      "Episode 64, Total Reward = 22.0\n",
      "Episode 65, Total Reward = 13.0\n",
      "Episode 66, Total Reward = 15.0\n",
      "Episode 67, Total Reward = 35.0\n",
      "Episode 68, Total Reward = 10.0\n",
      "Episode 69, Total Reward = 29.0\n",
      "Episode 70, Total Reward = 37.0\n",
      "Episode 71, Total Reward = 36.0\n",
      "Episode 72, Total Reward = 15.0\n",
      "Episode 73, Total Reward = 23.0\n",
      "Episode 74, Total Reward = 53.0\n",
      "Episode 75, Total Reward = 51.0\n",
      "Episode 76, Total Reward = 26.0\n",
      "Episode 77, Total Reward = 43.0\n",
      "Episode 78, Total Reward = 13.0\n",
      "Episode 79, Total Reward = 9.0\n",
      "Episode 80, Total Reward = 10.0\n",
      "Episode 81, Total Reward = 30.0\n",
      "Episode 82, Total Reward = 13.0\n",
      "Episode 83, Total Reward = 57.0\n",
      "Episode 84, Total Reward = 8.0\n",
      "Episode 85, Total Reward = 14.0\n",
      "Episode 86, Total Reward = 13.0\n",
      "Episode 87, Total Reward = 27.0\n",
      "Episode 88, Total Reward = 26.0\n",
      "Episode 89, Total Reward = 14.0\n",
      "Episode 90, Total Reward = 43.0\n",
      "Episode 91, Total Reward = 62.0\n",
      "Episode 92, Total Reward = 35.0\n",
      "Episode 93, Total Reward = 47.0\n",
      "Episode 94, Total Reward = 51.0\n",
      "Episode 95, Total Reward = 37.0\n",
      "Episode 96, Total Reward = 69.0\n",
      "Episode 97, Total Reward = 82.0\n",
      "Episode 98, Total Reward = 69.0\n",
      "Episode 99, Total Reward = 72.0\n",
      "Episode 100, Total Reward = 60.0\n",
      "Running experiment with lr=0.001, epsilon=0.1, k_epochs=5, batch_size=128\n",
      "Episode 1, Total Reward = 5.0\n",
      "Episode 2, Total Reward = 6.0\n",
      "Episode 3, Total Reward = 5.0\n",
      "Episode 4, Total Reward = 10.0\n",
      "Episode 5, Total Reward = 9.0\n",
      "Episode 6, Total Reward = 8.0\n",
      "Episode 7, Total Reward = 9.0\n",
      "Episode 8, Total Reward = 6.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 15.0\n",
      "Episode 11, Total Reward = 11.0\n",
      "Episode 12, Total Reward = 10.0\n",
      "Episode 13, Total Reward = 4.0\n",
      "Episode 14, Total Reward = 11.0\n",
      "Episode 15, Total Reward = 15.0\n",
      "Episode 16, Total Reward = 29.0\n",
      "Episode 17, Total Reward = 13.0\n",
      "Episode 18, Total Reward = 10.0\n",
      "Episode 19, Total Reward = 11.0\n",
      "Episode 20, Total Reward = 8.0\n",
      "Episode 21, Total Reward = 20.0\n",
      "Episode 22, Total Reward = 18.0\n",
      "Episode 23, Total Reward = 6.0\n",
      "Episode 24, Total Reward = 5.0\n",
      "Episode 25, Total Reward = 15.0\n",
      "Episode 26, Total Reward = 5.0\n",
      "Episode 27, Total Reward = 22.0\n",
      "Episode 28, Total Reward = 5.0\n",
      "Episode 29, Total Reward = 12.0\n",
      "Episode 30, Total Reward = 9.0\n",
      "Episode 31, Total Reward = 80.0\n",
      "Episode 32, Total Reward = 16.0\n",
      "Episode 33, Total Reward = 21.0\n",
      "Episode 34, Total Reward = 49.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 10.0\n",
      "Episode 37, Total Reward = 27.0\n",
      "Episode 38, Total Reward = 14.0\n",
      "Episode 39, Total Reward = 38.0\n",
      "Episode 40, Total Reward = 19.0\n",
      "Episode 41, Total Reward = 16.0\n",
      "Episode 42, Total Reward = 30.0\n",
      "Episode 43, Total Reward = 63.0\n",
      "Episode 44, Total Reward = 6.0\n",
      "Episode 45, Total Reward = 27.0\n",
      "Episode 46, Total Reward = 37.0\n",
      "Episode 47, Total Reward = 46.0\n",
      "Episode 48, Total Reward = 23.0\n",
      "Episode 49, Total Reward = 22.0\n",
      "Episode 50, Total Reward = 52.0\n",
      "Episode 51, Total Reward = 35.0\n",
      "Episode 52, Total Reward = 87.0\n",
      "Episode 53, Total Reward = 49.0\n",
      "Episode 54, Total Reward = 47.0\n",
      "Episode 55, Total Reward = 15.0\n",
      "Episode 56, Total Reward = 12.0\n",
      "Episode 57, Total Reward = 58.0\n",
      "Episode 58, Total Reward = 59.0\n",
      "Episode 59, Total Reward = 56.0\n",
      "Episode 60, Total Reward = 29.0\n",
      "Episode 61, Total Reward = 139.0\n",
      "Episode 62, Total Reward = 42.0\n",
      "Episode 63, Total Reward = 59.0\n",
      "Episode 64, Total Reward = 63.0\n",
      "Episode 65, Total Reward = 49.0\n",
      "Episode 66, Total Reward = 55.0\n",
      "Episode 67, Total Reward = 74.0\n",
      "Episode 68, Total Reward = 106.0\n",
      "Episode 69, Total Reward = 76.0\n",
      "Episode 70, Total Reward = 65.0\n",
      "Episode 71, Total Reward = 44.0\n",
      "Episode 72, Total Reward = 89.0\n",
      "Episode 73, Total Reward = 108.0\n",
      "Episode 74, Total Reward = 78.0\n",
      "Episode 75, Total Reward = 41.0\n",
      "Episode 76, Total Reward = 103.0\n",
      "Episode 77, Total Reward = 25.0\n",
      "Episode 78, Total Reward = 235.0\n",
      "Episode 79, Total Reward = 191.0\n",
      "Episode 80, Total Reward = 77.0\n",
      "Episode 81, Total Reward = 50.0\n",
      "Episode 82, Total Reward = 12.0\n",
      "Episode 83, Total Reward = 142.0\n",
      "Episode 84, Total Reward = 51.0\n",
      "Episode 85, Total Reward = 181.0\n",
      "Episode 86, Total Reward = 48.0\n",
      "Episode 87, Total Reward = 65.0\n",
      "Episode 88, Total Reward = 53.0\n",
      "Episode 89, Total Reward = 46.0\n",
      "Episode 90, Total Reward = 58.0\n",
      "Episode 91, Total Reward = 96.0\n",
      "Episode 92, Total Reward = 115.0\n",
      "Episode 93, Total Reward = 184.0\n",
      "Episode 94, Total Reward = 114.0\n",
      "Episode 95, Total Reward = 71.0\n",
      "Episode 96, Total Reward = 59.0\n",
      "Episode 97, Total Reward = 74.0\n",
      "Episode 98, Total Reward = 154.0\n",
      "Episode 99, Total Reward = 84.0\n",
      "Episode 100, Total Reward = 201.0\n",
      "Running experiment with lr=0.001, epsilon=0.1, k_epochs=10, batch_size=32\n",
      "Episode 1, Total Reward = 5.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 23.0\n",
      "Episode 5, Total Reward = 12.0\n",
      "Episode 6, Total Reward = 10.0\n",
      "Episode 7, Total Reward = 10.0\n",
      "Episode 8, Total Reward = 17.0\n",
      "Episode 9, Total Reward = 10.0\n",
      "Episode 10, Total Reward = 17.0\n",
      "Episode 11, Total Reward = 13.0\n",
      "Episode 12, Total Reward = 14.0\n",
      "Episode 13, Total Reward = 9.0\n",
      "Episode 14, Total Reward = 16.0\n",
      "Episode 15, Total Reward = 12.0\n",
      "Episode 16, Total Reward = 6.0\n",
      "Episode 17, Total Reward = 12.0\n",
      "Episode 18, Total Reward = 6.0\n",
      "Episode 19, Total Reward = 13.0\n",
      "Episode 20, Total Reward = 7.0\n",
      "Episode 21, Total Reward = 9.0\n",
      "Episode 22, Total Reward = 17.0\n",
      "Episode 23, Total Reward = 21.0\n",
      "Episode 24, Total Reward = 20.0\n",
      "Episode 25, Total Reward = 27.0\n",
      "Episode 26, Total Reward = 4.0\n",
      "Episode 27, Total Reward = 27.0\n",
      "Episode 28, Total Reward = 12.0\n",
      "Episode 29, Total Reward = 5.0\n",
      "Episode 30, Total Reward = 70.0\n",
      "Episode 31, Total Reward = 19.0\n",
      "Episode 32, Total Reward = 25.0\n",
      "Episode 33, Total Reward = 72.0\n",
      "Episode 34, Total Reward = 22.0\n",
      "Episode 35, Total Reward = 9.0\n",
      "Episode 36, Total Reward = 19.0\n",
      "Episode 37, Total Reward = 90.0\n",
      "Episode 38, Total Reward = 48.0\n",
      "Episode 39, Total Reward = 22.0\n",
      "Episode 40, Total Reward = 61.0\n",
      "Episode 41, Total Reward = 72.0\n",
      "Episode 42, Total Reward = 58.0\n",
      "Episode 43, Total Reward = 51.0\n",
      "Episode 44, Total Reward = 61.0\n",
      "Episode 45, Total Reward = 56.0\n",
      "Episode 46, Total Reward = 20.0\n",
      "Episode 47, Total Reward = 92.0\n",
      "Episode 48, Total Reward = 152.0\n",
      "Episode 49, Total Reward = 85.0\n",
      "Episode 50, Total Reward = 182.0\n",
      "Episode 51, Total Reward = 42.0\n",
      "Episode 52, Total Reward = 114.0\n",
      "Episode 53, Total Reward = 90.0\n",
      "Episode 54, Total Reward = 62.0\n",
      "Episode 55, Total Reward = 48.0\n",
      "Episode 56, Total Reward = 34.0\n",
      "Episode 57, Total Reward = 76.0\n",
      "Episode 58, Total Reward = 105.0\n",
      "Episode 59, Total Reward = 63.0\n",
      "Episode 60, Total Reward = 46.0\n",
      "Episode 61, Total Reward = 78.0\n",
      "Episode 62, Total Reward = 139.0\n",
      "Episode 63, Total Reward = 81.0\n",
      "Episode 64, Total Reward = 68.0\n",
      "Episode 65, Total Reward = 124.0\n",
      "Episode 66, Total Reward = 76.0\n",
      "Episode 67, Total Reward = 87.0\n",
      "Episode 68, Total Reward = 86.0\n",
      "Episode 69, Total Reward = 168.0\n",
      "Episode 70, Total Reward = 227.0\n",
      "Episode 71, Total Reward = 152.0\n",
      "Episode 72, Total Reward = 184.0\n",
      "Episode 73, Total Reward = 256.0\n",
      "Episode 74, Total Reward = 104.0\n",
      "Episode 75, Total Reward = 248.0\n",
      "Episode 76, Total Reward = 25.0\n",
      "Episode 77, Total Reward = 105.0\n",
      "Episode 78, Total Reward = 254.0\n",
      "Episode 79, Total Reward = 24.0\n",
      "Episode 80, Total Reward = 316.0\n",
      "Episode 81, Total Reward = 193.0\n",
      "Episode 82, Total Reward = 118.0\n",
      "Episode 83, Total Reward = 117.0\n",
      "Episode 84, Total Reward = 100.0\n",
      "Episode 85, Total Reward = 75.0\n",
      "Episode 86, Total Reward = 29.0\n",
      "Episode 87, Total Reward = 97.0\n",
      "Episode 88, Total Reward = 127.0\n",
      "Episode 89, Total Reward = 147.0\n",
      "Episode 90, Total Reward = 124.0\n",
      "Episode 91, Total Reward = 198.0\n",
      "Episode 92, Total Reward = 89.0\n",
      "Episode 93, Total Reward = 150.0\n",
      "Episode 94, Total Reward = 167.0\n",
      "Episode 95, Total Reward = 96.0\n",
      "Episode 96, Total Reward = 16.0\n",
      "Episode 97, Total Reward = 235.0\n",
      "Episode 98, Total Reward = 523.0\n",
      "Episode 99, Total Reward = 113.0\n",
      "Episode 100, Total Reward = 41.0\n",
      "Running experiment with lr=0.001, epsilon=0.1, k_epochs=10, batch_size=64\n",
      "Episode 1, Total Reward = 4.0\n",
      "Episode 2, Total Reward = 7.0\n",
      "Episode 3, Total Reward = 5.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 7.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 6.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 7.0\n",
      "Episode 11, Total Reward = 7.0\n",
      "Episode 12, Total Reward = 6.0\n",
      "Episode 13, Total Reward = 5.0\n",
      "Episode 14, Total Reward = 6.0\n",
      "Episode 15, Total Reward = 5.0\n",
      "Episode 16, Total Reward = 7.0\n",
      "Episode 17, Total Reward = 6.0\n",
      "Episode 18, Total Reward = 5.0\n",
      "Episode 19, Total Reward = 6.0\n",
      "Episode 20, Total Reward = 15.0\n",
      "Episode 21, Total Reward = 11.0\n",
      "Episode 22, Total Reward = 6.0\n",
      "Episode 23, Total Reward = 7.0\n",
      "Episode 24, Total Reward = 6.0\n",
      "Episode 25, Total Reward = 11.0\n",
      "Episode 26, Total Reward = 14.0\n",
      "Episode 27, Total Reward = 21.0\n",
      "Episode 28, Total Reward = 9.0\n",
      "Episode 29, Total Reward = 6.0\n",
      "Episode 30, Total Reward = 12.0\n",
      "Episode 31, Total Reward = 10.0\n",
      "Episode 32, Total Reward = 4.0\n",
      "Episode 33, Total Reward = 16.0\n",
      "Episode 34, Total Reward = 29.0\n",
      "Episode 35, Total Reward = 36.0\n",
      "Episode 36, Total Reward = 11.0\n",
      "Episode 37, Total Reward = 14.0\n",
      "Episode 38, Total Reward = 6.0\n",
      "Episode 39, Total Reward = 12.0\n",
      "Episode 40, Total Reward = 5.0\n",
      "Episode 41, Total Reward = 28.0\n",
      "Episode 42, Total Reward = 14.0\n",
      "Episode 43, Total Reward = 19.0\n",
      "Episode 44, Total Reward = 16.0\n",
      "Episode 45, Total Reward = 16.0\n",
      "Episode 46, Total Reward = 63.0\n",
      "Episode 47, Total Reward = 13.0\n",
      "Episode 48, Total Reward = 11.0\n",
      "Episode 49, Total Reward = 23.0\n",
      "Episode 50, Total Reward = 20.0\n",
      "Episode 51, Total Reward = 34.0\n",
      "Episode 52, Total Reward = 58.0\n",
      "Episode 53, Total Reward = 12.0\n",
      "Episode 54, Total Reward = 18.0\n",
      "Episode 55, Total Reward = 9.0\n",
      "Episode 56, Total Reward = 10.0\n",
      "Episode 57, Total Reward = 15.0\n",
      "Episode 58, Total Reward = 16.0\n",
      "Episode 59, Total Reward = 70.0\n",
      "Episode 60, Total Reward = 54.0\n",
      "Episode 61, Total Reward = 9.0\n",
      "Episode 62, Total Reward = 73.0\n",
      "Episode 63, Total Reward = 62.0\n",
      "Episode 64, Total Reward = 120.0\n",
      "Episode 65, Total Reward = 25.0\n",
      "Episode 66, Total Reward = 8.0\n",
      "Episode 67, Total Reward = 7.0\n",
      "Episode 68, Total Reward = 8.0\n",
      "Episode 69, Total Reward = 5.0\n",
      "Episode 70, Total Reward = 10.0\n",
      "Episode 71, Total Reward = 27.0\n",
      "Episode 72, Total Reward = 38.0\n",
      "Episode 73, Total Reward = 82.0\n",
      "Episode 74, Total Reward = 29.0\n",
      "Episode 75, Total Reward = 98.0\n",
      "Episode 76, Total Reward = 18.0\n",
      "Episode 77, Total Reward = 49.0\n",
      "Episode 78, Total Reward = 36.0\n",
      "Episode 79, Total Reward = 176.0\n",
      "Episode 80, Total Reward = 171.0\n",
      "Episode 81, Total Reward = 132.0\n",
      "Episode 82, Total Reward = 179.0\n",
      "Episode 83, Total Reward = 61.0\n",
      "Episode 84, Total Reward = 74.0\n",
      "Episode 85, Total Reward = 72.0\n",
      "Episode 86, Total Reward = 70.0\n",
      "Episode 87, Total Reward = 16.0\n",
      "Episode 88, Total Reward = 59.0\n",
      "Episode 89, Total Reward = 106.0\n",
      "Episode 90, Total Reward = 60.0\n",
      "Episode 91, Total Reward = 71.0\n",
      "Episode 92, Total Reward = 107.0\n",
      "Episode 93, Total Reward = 133.0\n",
      "Episode 94, Total Reward = 37.0\n",
      "Episode 95, Total Reward = 539.0\n",
      "Episode 96, Total Reward = 42.0\n",
      "Episode 97, Total Reward = 11.0\n",
      "Episode 98, Total Reward = 22.0\n",
      "Episode 99, Total Reward = 48.0\n",
      "Episode 100, Total Reward = 79.0\n",
      "Running experiment with lr=0.001, epsilon=0.1, k_epochs=10, batch_size=128\n",
      "Episode 1, Total Reward = 8.0\n",
      "Episode 2, Total Reward = 16.0\n",
      "Episode 3, Total Reward = 8.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 5.0\n",
      "Episode 6, Total Reward = 5.0\n",
      "Episode 7, Total Reward = 10.0\n",
      "Episode 8, Total Reward = 10.0\n",
      "Episode 9, Total Reward = 16.0\n",
      "Episode 10, Total Reward = 8.0\n",
      "Episode 11, Total Reward = 9.0\n",
      "Episode 12, Total Reward = 15.0\n",
      "Episode 13, Total Reward = 7.0\n",
      "Episode 14, Total Reward = 15.0\n",
      "Episode 15, Total Reward = 11.0\n",
      "Episode 16, Total Reward = 9.0\n",
      "Episode 17, Total Reward = 29.0\n",
      "Episode 18, Total Reward = 17.0\n",
      "Episode 19, Total Reward = 11.0\n",
      "Episode 20, Total Reward = 4.0\n",
      "Episode 21, Total Reward = 20.0\n",
      "Episode 22, Total Reward = 7.0\n",
      "Episode 23, Total Reward = 44.0\n",
      "Episode 24, Total Reward = 22.0\n",
      "Episode 25, Total Reward = 16.0\n",
      "Episode 26, Total Reward = 24.0\n",
      "Episode 27, Total Reward = 70.0\n",
      "Episode 28, Total Reward = 9.0\n",
      "Episode 29, Total Reward = 8.0\n",
      "Episode 30, Total Reward = 23.0\n",
      "Episode 31, Total Reward = 31.0\n",
      "Episode 32, Total Reward = 11.0\n",
      "Episode 33, Total Reward = 31.0\n",
      "Episode 34, Total Reward = 11.0\n",
      "Episode 35, Total Reward = 35.0\n",
      "Episode 36, Total Reward = 38.0\n",
      "Episode 37, Total Reward = 28.0\n",
      "Episode 38, Total Reward = 20.0\n",
      "Episode 39, Total Reward = 91.0\n",
      "Episode 40, Total Reward = 30.0\n",
      "Episode 41, Total Reward = 21.0\n",
      "Episode 42, Total Reward = 46.0\n",
      "Episode 43, Total Reward = 51.0\n",
      "Episode 44, Total Reward = 26.0\n",
      "Episode 45, Total Reward = 19.0\n",
      "Episode 46, Total Reward = 8.0\n",
      "Episode 47, Total Reward = 48.0\n",
      "Episode 48, Total Reward = 44.0\n",
      "Episode 49, Total Reward = 21.0\n",
      "Episode 50, Total Reward = 19.0\n",
      "Episode 51, Total Reward = 72.0\n",
      "Episode 52, Total Reward = 59.0\n",
      "Episode 53, Total Reward = 90.0\n",
      "Episode 54, Total Reward = 94.0\n",
      "Episode 55, Total Reward = 56.0\n",
      "Episode 56, Total Reward = 108.0\n",
      "Episode 57, Total Reward = 125.0\n",
      "Episode 58, Total Reward = 88.0\n",
      "Episode 59, Total Reward = 114.0\n",
      "Episode 60, Total Reward = 96.0\n",
      "Episode 61, Total Reward = 114.0\n",
      "Episode 62, Total Reward = 78.0\n",
      "Episode 63, Total Reward = 95.0\n",
      "Episode 64, Total Reward = 226.0\n",
      "Episode 65, Total Reward = 159.0\n",
      "Episode 66, Total Reward = 210.0\n",
      "Episode 67, Total Reward = 103.0\n",
      "Episode 68, Total Reward = 126.0\n",
      "Episode 69, Total Reward = 32.0\n",
      "Episode 70, Total Reward = 117.0\n",
      "Episode 71, Total Reward = 85.0\n",
      "Episode 72, Total Reward = 113.0\n",
      "Episode 73, Total Reward = 116.0\n",
      "Episode 74, Total Reward = 123.0\n",
      "Episode 75, Total Reward = 55.0\n",
      "Episode 76, Total Reward = 68.0\n",
      "Episode 77, Total Reward = 105.0\n",
      "Episode 78, Total Reward = 125.0\n",
      "Episode 79, Total Reward = 107.0\n",
      "Episode 80, Total Reward = 76.0\n",
      "Episode 81, Total Reward = 121.0\n",
      "Episode 82, Total Reward = 100.0\n",
      "Episode 83, Total Reward = 134.0\n",
      "Episode 84, Total Reward = 107.0\n",
      "Episode 85, Total Reward = 86.0\n",
      "Episode 86, Total Reward = 115.0\n",
      "Episode 87, Total Reward = 94.0\n",
      "Episode 88, Total Reward = 89.0\n",
      "Episode 89, Total Reward = 41.0\n",
      "Episode 90, Total Reward = 71.0\n",
      "Episode 91, Total Reward = 22.0\n",
      "Episode 92, Total Reward = 66.0\n",
      "Episode 93, Total Reward = 94.0\n",
      "Episode 94, Total Reward = 114.0\n",
      "Episode 95, Total Reward = 146.0\n",
      "Episode 96, Total Reward = 147.0\n",
      "Episode 97, Total Reward = 174.0\n",
      "Episode 98, Total Reward = 144.0\n",
      "Episode 99, Total Reward = 334.0\n",
      "Episode 100, Total Reward = 39.0\n",
      "Running experiment with lr=0.001, epsilon=0.1, k_epochs=20, batch_size=32\n",
      "Episode 1, Total Reward = 14.0\n",
      "Episode 2, Total Reward = 5.0\n",
      "Episode 3, Total Reward = 11.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 4.0\n",
      "Episode 6, Total Reward = 16.0\n",
      "Episode 7, Total Reward = 20.0\n",
      "Episode 8, Total Reward = 8.0\n",
      "Episode 9, Total Reward = 7.0\n",
      "Episode 10, Total Reward = 7.0\n",
      "Episode 11, Total Reward = 8.0\n",
      "Episode 12, Total Reward = 5.0\n",
      "Episode 13, Total Reward = 11.0\n",
      "Episode 14, Total Reward = 17.0\n",
      "Episode 15, Total Reward = 10.0\n",
      "Episode 16, Total Reward = 18.0\n",
      "Episode 17, Total Reward = 8.0\n",
      "Episode 18, Total Reward = 13.0\n",
      "Episode 19, Total Reward = 4.0\n",
      "Episode 20, Total Reward = 7.0\n",
      "Episode 21, Total Reward = 4.0\n",
      "Episode 22, Total Reward = 12.0\n",
      "Episode 23, Total Reward = 4.0\n",
      "Episode 24, Total Reward = 9.0\n",
      "Episode 25, Total Reward = 9.0\n",
      "Episode 26, Total Reward = 4.0\n",
      "Episode 27, Total Reward = 13.0\n",
      "Episode 28, Total Reward = 20.0\n",
      "Episode 29, Total Reward = 16.0\n",
      "Episode 30, Total Reward = 20.0\n",
      "Episode 31, Total Reward = 6.0\n",
      "Episode 32, Total Reward = 15.0\n",
      "Episode 33, Total Reward = 24.0\n",
      "Episode 34, Total Reward = 27.0\n",
      "Episode 35, Total Reward = 47.0\n",
      "Episode 36, Total Reward = 9.0\n",
      "Episode 37, Total Reward = 19.0\n",
      "Episode 38, Total Reward = 10.0\n",
      "Episode 39, Total Reward = 28.0\n",
      "Episode 40, Total Reward = 50.0\n",
      "Episode 41, Total Reward = 47.0\n",
      "Episode 42, Total Reward = 9.0\n",
      "Episode 43, Total Reward = 82.0\n",
      "Episode 44, Total Reward = 61.0\n",
      "Episode 45, Total Reward = 15.0\n",
      "Episode 46, Total Reward = 44.0\n",
      "Episode 47, Total Reward = 66.0\n",
      "Episode 48, Total Reward = 63.0\n",
      "Episode 49, Total Reward = 90.0\n",
      "Episode 50, Total Reward = 78.0\n",
      "Episode 51, Total Reward = 57.0\n",
      "Episode 52, Total Reward = 42.0\n",
      "Episode 53, Total Reward = 17.0\n",
      "Episode 54, Total Reward = 91.0\n",
      "Episode 55, Total Reward = 35.0\n",
      "Episode 56, Total Reward = 153.0\n",
      "Episode 57, Total Reward = 361.0\n",
      "Episode 58, Total Reward = 158.0\n",
      "Episode 59, Total Reward = 392.0\n",
      "Episode 60, Total Reward = 556.0\n",
      "Episode 61, Total Reward = 223.0\n",
      "Episode 62, Total Reward = 134.0\n",
      "Episode 63, Total Reward = 1000.0\n",
      "Episode 64, Total Reward = 636.0\n",
      "Episode 65, Total Reward = 43.0\n",
      "Episode 66, Total Reward = 182.0\n",
      "Episode 67, Total Reward = 294.0\n",
      "Episode 68, Total Reward = 1000.0\n",
      "Episode 69, Total Reward = 343.0\n",
      "Episode 70, Total Reward = 1000.0\n",
      "Episode 71, Total Reward = 1000.0\n",
      "Episode 72, Total Reward = 971.0\n",
      "Episode 73, Total Reward = 162.0\n",
      "Episode 74, Total Reward = 63.0\n",
      "Episode 75, Total Reward = 62.0\n",
      "Episode 76, Total Reward = 99.0\n",
      "Episode 77, Total Reward = 40.0\n",
      "Episode 78, Total Reward = 169.0\n",
      "Episode 79, Total Reward = 74.0\n",
      "Episode 80, Total Reward = 163.0\n",
      "Episode 81, Total Reward = 80.0\n",
      "Episode 82, Total Reward = 521.0\n",
      "Episode 83, Total Reward = 157.0\n",
      "Episode 84, Total Reward = 71.0\n",
      "Episode 85, Total Reward = 77.0\n",
      "Episode 86, Total Reward = 168.0\n",
      "Episode 87, Total Reward = 46.0\n",
      "Episode 88, Total Reward = 529.0\n",
      "Episode 89, Total Reward = 42.0\n",
      "Episode 90, Total Reward = 1000.0\n",
      "Episode 91, Total Reward = 1000.0\n",
      "Episode 92, Total Reward = 1000.0\n",
      "Episode 93, Total Reward = 124.0\n",
      "Episode 94, Total Reward = 236.0\n",
      "Episode 95, Total Reward = 109.0\n",
      "Episode 96, Total Reward = 91.0\n",
      "Episode 97, Total Reward = 56.0\n",
      "Episode 98, Total Reward = 94.0\n",
      "Episode 99, Total Reward = 201.0\n",
      "Episode 100, Total Reward = 98.0\n",
      "Running experiment with lr=0.001, epsilon=0.1, k_epochs=20, batch_size=64\n",
      "Episode 1, Total Reward = 7.0\n",
      "Episode 2, Total Reward = 8.0\n",
      "Episode 3, Total Reward = 6.0\n",
      "Episode 4, Total Reward = 6.0\n",
      "Episode 5, Total Reward = 15.0\n",
      "Episode 6, Total Reward = 8.0\n",
      "Episode 7, Total Reward = 7.0\n",
      "Episode 8, Total Reward = 4.0\n",
      "Episode 9, Total Reward = 13.0\n",
      "Episode 10, Total Reward = 15.0\n",
      "Episode 11, Total Reward = 9.0\n",
      "Episode 12, Total Reward = 18.0\n",
      "Episode 13, Total Reward = 11.0\n",
      "Episode 14, Total Reward = 13.0\n",
      "Episode 15, Total Reward = 6.0\n",
      "Episode 16, Total Reward = 5.0\n",
      "Episode 17, Total Reward = 12.0\n",
      "Episode 18, Total Reward = 8.0\n",
      "Episode 19, Total Reward = 34.0\n",
      "Episode 20, Total Reward = 10.0\n",
      "Episode 21, Total Reward = 5.0\n",
      "Episode 22, Total Reward = 15.0\n",
      "Episode 23, Total Reward = 12.0\n",
      "Episode 24, Total Reward = 8.0\n",
      "Episode 25, Total Reward = 4.0\n",
      "Episode 26, Total Reward = 12.0\n",
      "Episode 27, Total Reward = 34.0\n",
      "Episode 28, Total Reward = 17.0\n",
      "Episode 29, Total Reward = 11.0\n",
      "Episode 30, Total Reward = 15.0\n",
      "Episode 31, Total Reward = 20.0\n",
      "Episode 32, Total Reward = 21.0\n",
      "Episode 33, Total Reward = 9.0\n",
      "Episode 34, Total Reward = 30.0\n",
      "Episode 35, Total Reward = 6.0\n",
      "Episode 36, Total Reward = 58.0\n",
      "Episode 37, Total Reward = 15.0\n",
      "Episode 38, Total Reward = 70.0\n",
      "Episode 39, Total Reward = 13.0\n",
      "Episode 40, Total Reward = 49.0\n",
      "Episode 41, Total Reward = 58.0\n",
      "Episode 42, Total Reward = 19.0\n",
      "Episode 43, Total Reward = 18.0\n",
      "Episode 44, Total Reward = 62.0\n",
      "Episode 45, Total Reward = 38.0\n",
      "Episode 46, Total Reward = 49.0\n",
      "Episode 47, Total Reward = 50.0\n",
      "Episode 48, Total Reward = 49.0\n",
      "Episode 49, Total Reward = 37.0\n",
      "Episode 50, Total Reward = 74.0\n",
      "Episode 51, Total Reward = 45.0\n",
      "Episode 52, Total Reward = 102.0\n",
      "Episode 53, Total Reward = 75.0\n",
      "Episode 54, Total Reward = 44.0\n",
      "Episode 55, Total Reward = 55.0\n",
      "Episode 56, Total Reward = 35.0\n",
      "Episode 57, Total Reward = 36.0\n",
      "Episode 58, Total Reward = 26.0\n",
      "Episode 59, Total Reward = 9.0\n",
      "Episode 60, Total Reward = 25.0\n",
      "Episode 61, Total Reward = 71.0\n",
      "Episode 62, Total Reward = 99.0\n",
      "Episode 63, Total Reward = 72.0\n",
      "Episode 64, Total Reward = 91.0\n",
      "Episode 65, Total Reward = 17.0\n",
      "Episode 66, Total Reward = 8.0\n",
      "Episode 67, Total Reward = 39.0\n",
      "Episode 68, Total Reward = 19.0\n",
      "Episode 69, Total Reward = 21.0\n",
      "Episode 70, Total Reward = 54.0\n",
      "Episode 71, Total Reward = 91.0\n",
      "Episode 72, Total Reward = 63.0\n",
      "Episode 73, Total Reward = 82.0\n",
      "Episode 74, Total Reward = 14.0\n",
      "Episode 75, Total Reward = 39.0\n",
      "Episode 76, Total Reward = 37.0\n",
      "Episode 77, Total Reward = 66.0\n",
      "Episode 78, Total Reward = 58.0\n",
      "Episode 79, Total Reward = 85.0\n",
      "Episode 80, Total Reward = 130.0\n",
      "Episode 81, Total Reward = 75.0\n",
      "Episode 82, Total Reward = 60.0\n",
      "Episode 83, Total Reward = 38.0\n",
      "Episode 84, Total Reward = 96.0\n",
      "Episode 85, Total Reward = 6.0\n",
      "Episode 86, Total Reward = 137.0\n",
      "Episode 87, Total Reward = 152.0\n",
      "Episode 88, Total Reward = 31.0\n",
      "Episode 89, Total Reward = 42.0\n",
      "Episode 90, Total Reward = 57.0\n",
      "Episode 91, Total Reward = 95.0\n",
      "Episode 92, Total Reward = 99.0\n",
      "Episode 93, Total Reward = 123.0\n",
      "Episode 94, Total Reward = 75.0\n",
      "Episode 95, Total Reward = 129.0\n",
      "Episode 96, Total Reward = 207.0\n",
      "Episode 97, Total Reward = 67.0\n",
      "Episode 98, Total Reward = 106.0\n",
      "Episode 99, Total Reward = 119.0\n",
      "Episode 100, Total Reward = 85.0\n",
      "Running experiment with lr=0.001, epsilon=0.1, k_epochs=20, batch_size=128\n",
      "Episode 1, Total Reward = 4.0\n",
      "Episode 2, Total Reward = 4.0\n",
      "Episode 3, Total Reward = 7.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 5.0\n",
      "Episode 6, Total Reward = 5.0\n",
      "Episode 7, Total Reward = 5.0\n",
      "Episode 8, Total Reward = 5.0\n",
      "Episode 9, Total Reward = 6.0\n",
      "Episode 10, Total Reward = 11.0\n",
      "Episode 11, Total Reward = 7.0\n",
      "Episode 12, Total Reward = 16.0\n",
      "Episode 13, Total Reward = 6.0\n",
      "Episode 14, Total Reward = 7.0\n",
      "Episode 15, Total Reward = 7.0\n",
      "Episode 16, Total Reward = 5.0\n",
      "Episode 17, Total Reward = 4.0\n",
      "Episode 18, Total Reward = 6.0\n",
      "Episode 19, Total Reward = 13.0\n",
      "Episode 20, Total Reward = 6.0\n",
      "Episode 21, Total Reward = 22.0\n",
      "Episode 22, Total Reward = 4.0\n",
      "Episode 23, Total Reward = 18.0\n",
      "Episode 24, Total Reward = 8.0\n",
      "Episode 25, Total Reward = 9.0\n",
      "Episode 26, Total Reward = 4.0\n",
      "Episode 27, Total Reward = 5.0\n",
      "Episode 28, Total Reward = 6.0\n",
      "Episode 29, Total Reward = 10.0\n",
      "Episode 30, Total Reward = 5.0\n",
      "Episode 31, Total Reward = 5.0\n",
      "Episode 32, Total Reward = 6.0\n",
      "Episode 33, Total Reward = 11.0\n",
      "Episode 34, Total Reward = 15.0\n",
      "Episode 35, Total Reward = 10.0\n",
      "Episode 36, Total Reward = 5.0\n",
      "Episode 37, Total Reward = 8.0\n",
      "Episode 38, Total Reward = 11.0\n",
      "Episode 39, Total Reward = 4.0\n",
      "Episode 40, Total Reward = 7.0\n",
      "Episode 41, Total Reward = 8.0\n",
      "Episode 42, Total Reward = 9.0\n",
      "Episode 43, Total Reward = 15.0\n",
      "Episode 44, Total Reward = 23.0\n",
      "Episode 45, Total Reward = 26.0\n",
      "Episode 46, Total Reward = 16.0\n",
      "Episode 47, Total Reward = 10.0\n",
      "Episode 48, Total Reward = 24.0\n",
      "Episode 49, Total Reward = 7.0\n",
      "Episode 50, Total Reward = 7.0\n",
      "Episode 51, Total Reward = 53.0\n",
      "Episode 52, Total Reward = 39.0\n",
      "Episode 53, Total Reward = 57.0\n",
      "Episode 54, Total Reward = 28.0\n",
      "Episode 55, Total Reward = 28.0\n",
      "Episode 56, Total Reward = 46.0\n",
      "Episode 57, Total Reward = 13.0\n",
      "Episode 58, Total Reward = 42.0\n",
      "Episode 59, Total Reward = 47.0\n",
      "Episode 60, Total Reward = 39.0\n",
      "Episode 61, Total Reward = 39.0\n",
      "Episode 62, Total Reward = 55.0\n",
      "Episode 63, Total Reward = 54.0\n",
      "Episode 64, Total Reward = 61.0\n",
      "Episode 65, Total Reward = 76.0\n",
      "Episode 66, Total Reward = 57.0\n",
      "Episode 67, Total Reward = 57.0\n",
      "Episode 68, Total Reward = 102.0\n",
      "Episode 69, Total Reward = 111.0\n",
      "Episode 70, Total Reward = 43.0\n",
      "Episode 71, Total Reward = 85.0\n",
      "Episode 72, Total Reward = 50.0\n",
      "Episode 73, Total Reward = 56.0\n",
      "Episode 74, Total Reward = 57.0\n",
      "Episode 75, Total Reward = 49.0\n",
      "Episode 76, Total Reward = 19.0\n",
      "Episode 77, Total Reward = 85.0\n",
      "Episode 78, Total Reward = 61.0\n",
      "Episode 79, Total Reward = 55.0\n",
      "Episode 80, Total Reward = 62.0\n",
      "Episode 81, Total Reward = 64.0\n",
      "Episode 82, Total Reward = 61.0\n",
      "Episode 83, Total Reward = 75.0\n",
      "Episode 84, Total Reward = 76.0\n",
      "Episode 85, Total Reward = 66.0\n",
      "Episode 86, Total Reward = 65.0\n",
      "Episode 87, Total Reward = 95.0\n",
      "Episode 88, Total Reward = 72.0\n",
      "Episode 89, Total Reward = 59.0\n",
      "Episode 90, Total Reward = 43.0\n",
      "Episode 91, Total Reward = 130.0\n",
      "Episode 92, Total Reward = 99.0\n",
      "Episode 93, Total Reward = 76.0\n",
      "Episode 94, Total Reward = 64.0\n",
      "Episode 95, Total Reward = 79.0\n",
      "Episode 96, Total Reward = 72.0\n",
      "Episode 97, Total Reward = 67.0\n",
      "Episode 98, Total Reward = 70.0\n",
      "Episode 99, Total Reward = 56.0\n",
      "Episode 100, Total Reward = 63.0\n",
      "Running experiment with lr=0.001, epsilon=0.2, k_epochs=5, batch_size=32\n",
      "Episode 1, Total Reward = 5.0\n",
      "Episode 2, Total Reward = 7.0\n",
      "Episode 3, Total Reward = 4.0\n",
      "Episode 4, Total Reward = 5.0\n",
      "Episode 5, Total Reward = 5.0\n",
      "Episode 6, Total Reward = 4.0\n",
      "Episode 7, Total Reward = 7.0\n",
      "Episode 8, Total Reward = 9.0\n",
      "Episode 9, Total Reward = 12.0\n",
      "Episode 10, Total Reward = 5.0\n",
      "Episode 11, Total Reward = 13.0\n",
      "Episode 12, Total Reward = 19.0\n",
      "Episode 13, Total Reward = 12.0\n",
      "Episode 14, Total Reward = 13.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 217\u001b[0m\n\u001b[0;32m    214\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvertedPendulum-v4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiments with the ActorCritic network architecture:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 217\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 187\u001b[0m, in \u001b[0;36mrun_experiments\u001b[1;34m(env, param_grid)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning experiment with lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, epsilon=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepsilon\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, k_epochs=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch_size=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    186\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPOAgent(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, action_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lr\u001b[38;5;241m=\u001b[39mlr, epsilon\u001b[38;5;241m=\u001b[39mepsilon, k_epochs\u001b[38;5;241m=\u001b[39mk_epochs, minibatch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m--> 187\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Reduce the number of episodes for quicker experimentation\u001b[39;00m\n\u001b[0;32m    188\u001b[0m avg_reward \u001b[38;5;241m=\u001b[39m ppo\u001b[38;5;241m.\u001b[39mevaluate(env)\n\u001b[0;32m    190\u001b[0m result \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: lr,\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m: epsilon,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m: rewards\n\u001b[0;32m    197\u001b[0m }\n",
      "Cell \u001b[1;32mIn[18], line 118\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[1;34m(self, env, total_episodes, horizon, batch_size)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m    117\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m--> 118\u001b[0m     mu, std, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    120\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(next_state, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m next_state  \u001b[38;5;66;03m# Extract next_state from tuple if necessary\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 33\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(x)\n\u001b[0;32m     32\u001b[0m std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mexpand_as(mu)\n\u001b[1;32m---> 33\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mu, std, value\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import pickle\n",
    "from itertools import product\n",
    "\n",
    "# Define the ActorCritic class with separate actor and critic networks\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        value = self.critic(x)\n",
    "        return mu, std, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, k_epochs=10, minibatch_size=64, gae_lambda=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.actor_critic = ActorCritic(input_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        mu, std, _ = self.actor_critic(state)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            indices = np.arange(states.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, states.shape[0], self.minibatch_size):\n",
    "                end = start + self.minibatch_size\n",
    "                minibatch_indices = indices[start:end]\n",
    "                \n",
    "                minibatch_states = states[minibatch_indices]\n",
    "                minibatch_actions = actions[minibatch_indices]\n",
    "                minibatch_log_probs_old = log_probs_old[minibatch_indices]\n",
    "                minibatch_returns = returns[minibatch_indices]\n",
    "                minibatch_advantages = advantages[minibatch_indices]\n",
    "\n",
    "                mu, std, values = self.actor_critic(minibatch_states)\n",
    "                dist = torch.distributions.Normal(mu, std)\n",
    "                log_probs_new = dist.log_prob(minibatch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratios = torch.exp(log_probs_new - minibatch_log_probs_old)\n",
    "                surr1 = ratios * minibatch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * minibatch_advantages\n",
    "\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy\n",
    "                critic_loss = self.mse_loss(values, minibatch_returns)\n",
    "\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train(self, env, total_episodes=1000, horizon=2048, batch_size=64):\n",
    "        all_rewards = []\n",
    "\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        for episode in range(total_episodes):\n",
    "            state = env.reset()\n",
    "            state = state[0] if isinstance(state, tuple) else state  # Extract state from tuple if necessary\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                mu, std, value = self.actor_critic(torch.from_numpy(state).float())\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                next_state = next_state[0] if isinstance(next_state, tuple) else next_state  # Extract next_state from tuple if necessary\n",
    "\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value.item())\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "\n",
    "                if len(states) >= batch_size:\n",
    "                    next_value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {episode + 1}, Total Reward = {episode_rewards}\")\n",
    "\n",
    "        return all_rewards\n",
    "    \n",
    "    def evaluate(self, env, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state = state[0] if isinstance(state, tuple) else state\n",
    "                \n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state array to tensor\n",
    "               \n",
    "                mu, std, values = self.actor_critic(state_tensor)\n",
    "                dist = torch.distributions.Normal(mu, std)\n",
    "                action = dist.sample()\n",
    "               \n",
    "                # Ensure action remains a 1-dimensional array with a single element\n",
    "                action_numpy = action.squeeze().detach().numpy()  # Squeeze to potentially reduce dimensions\n",
    "                if action_numpy.ndim == 0:  # If the result is a scalar, convert it back to an array\n",
    "                    action_numpy = np.array([action_numpy])\n",
    "                \n",
    "                next_state, reward, done, _, _ = env.step(action_numpy)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "def run_experiments(env, param_grid):\n",
    "    results = []\n",
    "\n",
    "    for params in param_grid:\n",
    "        lr, epsilon, k_epochs, batch_size = params\n",
    "        print(f\"Running experiment with lr={lr}, epsilon={epsilon}, k_epochs={k_epochs}, batch_size={batch_size}\")\n",
    "\n",
    "        ppo = PPOAgent(input_dim=4, action_dim=1, lr=lr, epsilon=epsilon, k_epochs=k_epochs, minibatch_size=batch_size)\n",
    "        rewards = ppo.train(env, total_episodes=100)  # Reduce the number of episodes for quicker experimentation\n",
    "        avg_reward = ppo.evaluate(env)\n",
    "        \n",
    "        result = {\n",
    "            'lr': lr,\n",
    "            'epsilon': epsilon,\n",
    "            'k_epochs': k_epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'average_reward': avg_reward,\n",
    "            'rewards': rewards\n",
    "        }\n",
    "        results.append(result)\n",
    "\n",
    "        with open('experiment_results_trial1.pkl', 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = list(product(\n",
    "    [1e-4, 3e-4, 1e-3],  # Learning rates\n",
    "    [0.1, 0.2, 0.3],     # Epsilon values\n",
    "    [5, 10, 20],         # Epochs\n",
    "    [32, 64, 128]        # Batch sizes\n",
    "))\n",
    "\n",
    "# Run experiments with the ActorCritic network architecture\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "\n",
    "print(\"Running experiments with the ActorCritic network architecture:\")\n",
    "results = run_experiments(env, param_grid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horizon (T): This refers to the number of timesteps collected before performing a policy update.\n",
    "1 Million Timestep Benchmark: This means the total number of timesteps for training across all episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward = 19.0, Total Steps = 19\n",
      "Episode 2, Total Reward = 9.0, Total Steps = 28\n",
      "Episode 3, Total Reward = 6.0, Total Steps = 34\n",
      "Episode 4, Total Reward = 9.0, Total Steps = 43\n",
      "Episode 5, Total Reward = 16.0, Total Steps = 59\n",
      "Episode 6, Total Reward = 21.0, Total Steps = 80\n",
      "Episode 7, Total Reward = 8.0, Total Steps = 88\n",
      "Episode 8, Total Reward = 21.0, Total Steps = 109\n",
      "Episode 9, Total Reward = 8.0, Total Steps = 117\n",
      "Episode 10, Total Reward = 16.0, Total Steps = 133\n",
      "Episode 11, Total Reward = 4.0, Total Steps = 137\n",
      "Episode 12, Total Reward = 13.0, Total Steps = 150\n",
      "Episode 13, Total Reward = 4.0, Total Steps = 154\n",
      "Episode 14, Total Reward = 18.0, Total Steps = 172\n",
      "Episode 15, Total Reward = 7.0, Total Steps = 179\n",
      "Episode 16, Total Reward = 33.0, Total Steps = 212\n",
      "Episode 17, Total Reward = 10.0, Total Steps = 222\n",
      "Episode 18, Total Reward = 11.0, Total Steps = 233\n",
      "Episode 19, Total Reward = 8.0, Total Steps = 241\n",
      "Episode 20, Total Reward = 6.0, Total Steps = 247\n",
      "Episode 21, Total Reward = 17.0, Total Steps = 264\n",
      "Episode 22, Total Reward = 8.0, Total Steps = 272\n",
      "Episode 23, Total Reward = 9.0, Total Steps = 281\n",
      "Episode 24, Total Reward = 17.0, Total Steps = 298\n",
      "Episode 25, Total Reward = 17.0, Total Steps = 315\n",
      "Episode 26, Total Reward = 12.0, Total Steps = 327\n",
      "Episode 27, Total Reward = 5.0, Total Steps = 332\n",
      "Episode 28, Total Reward = 12.0, Total Steps = 344\n",
      "Episode 29, Total Reward = 28.0, Total Steps = 372\n",
      "Episode 30, Total Reward = 11.0, Total Steps = 383\n",
      "Episode 31, Total Reward = 13.0, Total Steps = 396\n",
      "Episode 32, Total Reward = 16.0, Total Steps = 412\n",
      "Episode 33, Total Reward = 16.0, Total Steps = 428\n",
      "Episode 34, Total Reward = 6.0, Total Steps = 434\n",
      "Episode 35, Total Reward = 7.0, Total Steps = 441\n",
      "Episode 36, Total Reward = 13.0, Total Steps = 454\n",
      "Episode 37, Total Reward = 6.0, Total Steps = 460\n",
      "Episode 38, Total Reward = 20.0, Total Steps = 480\n",
      "Episode 39, Total Reward = 11.0, Total Steps = 491\n",
      "Episode 40, Total Reward = 9.0, Total Steps = 500\n",
      "Episode 41, Total Reward = 10.0, Total Steps = 510\n",
      "Episode 42, Total Reward = 11.0, Total Steps = 521\n",
      "Episode 43, Total Reward = 13.0, Total Steps = 534\n",
      "Episode 44, Total Reward = 8.0, Total Steps = 542\n",
      "Episode 45, Total Reward = 42.0, Total Steps = 584\n",
      "Episode 46, Total Reward = 17.0, Total Steps = 601\n",
      "Episode 47, Total Reward = 13.0, Total Steps = 614\n",
      "Episode 48, Total Reward = 39.0, Total Steps = 653\n",
      "Episode 49, Total Reward = 7.0, Total Steps = 660\n",
      "Episode 50, Total Reward = 10.0, Total Steps = 670\n",
      "Episode 51, Total Reward = 10.0, Total Steps = 680\n",
      "Episode 52, Total Reward = 6.0, Total Steps = 686\n",
      "Episode 53, Total Reward = 19.0, Total Steps = 705\n",
      "Episode 54, Total Reward = 4.0, Total Steps = 709\n",
      "Episode 55, Total Reward = 22.0, Total Steps = 731\n",
      "Episode 56, Total Reward = 10.0, Total Steps = 741\n",
      "Episode 57, Total Reward = 19.0, Total Steps = 760\n",
      "Episode 58, Total Reward = 20.0, Total Steps = 780\n",
      "Episode 59, Total Reward = 5.0, Total Steps = 785\n",
      "Episode 60, Total Reward = 5.0, Total Steps = 790\n",
      "Episode 61, Total Reward = 8.0, Total Steps = 798\n",
      "Episode 62, Total Reward = 4.0, Total Steps = 802\n",
      "Episode 63, Total Reward = 26.0, Total Steps = 828\n",
      "Episode 64, Total Reward = 12.0, Total Steps = 840\n",
      "Episode 65, Total Reward = 8.0, Total Steps = 848\n",
      "Episode 66, Total Reward = 6.0, Total Steps = 854\n",
      "Episode 67, Total Reward = 5.0, Total Steps = 859\n",
      "Episode 68, Total Reward = 6.0, Total Steps = 865\n",
      "Episode 69, Total Reward = 6.0, Total Steps = 871\n",
      "Episode 70, Total Reward = 4.0, Total Steps = 875\n",
      "Episode 71, Total Reward = 12.0, Total Steps = 887\n",
      "Episode 72, Total Reward = 42.0, Total Steps = 929\n",
      "Episode 73, Total Reward = 8.0, Total Steps = 937\n",
      "Episode 74, Total Reward = 6.0, Total Steps = 943\n",
      "Episode 75, Total Reward = 5.0, Total Steps = 948\n",
      "Episode 76, Total Reward = 5.0, Total Steps = 953\n",
      "Episode 77, Total Reward = 39.0, Total Steps = 992\n",
      "Episode 78, Total Reward = 15.0, Total Steps = 1007\n",
      "Episode 79, Total Reward = 11.0, Total Steps = 1018\n",
      "Episode 80, Total Reward = 8.0, Total Steps = 1026\n",
      "Episode 81, Total Reward = 25.0, Total Steps = 1051\n",
      "Episode 82, Total Reward = 9.0, Total Steps = 1060\n",
      "Episode 83, Total Reward = 7.0, Total Steps = 1067\n",
      "Episode 84, Total Reward = 29.0, Total Steps = 1096\n",
      "Episode 85, Total Reward = 14.0, Total Steps = 1110\n",
      "Episode 86, Total Reward = 7.0, Total Steps = 1117\n",
      "Episode 87, Total Reward = 12.0, Total Steps = 1129\n",
      "Episode 88, Total Reward = 17.0, Total Steps = 1146\n",
      "Episode 89, Total Reward = 9.0, Total Steps = 1155\n",
      "Episode 90, Total Reward = 8.0, Total Steps = 1163\n",
      "Episode 91, Total Reward = 12.0, Total Steps = 1175\n",
      "Episode 92, Total Reward = 5.0, Total Steps = 1180\n",
      "Episode 93, Total Reward = 10.0, Total Steps = 1190\n",
      "Episode 94, Total Reward = 25.0, Total Steps = 1215\n",
      "Episode 95, Total Reward = 8.0, Total Steps = 1223\n",
      "Episode 96, Total Reward = 7.0, Total Steps = 1230\n",
      "Episode 97, Total Reward = 6.0, Total Steps = 1236\n",
      "Episode 98, Total Reward = 6.0, Total Steps = 1242\n",
      "Episode 99, Total Reward = 6.0, Total Steps = 1248\n",
      "Episode 100, Total Reward = 14.0, Total Steps = 1262\n",
      "Episode 101, Total Reward = 23.0, Total Steps = 1285\n",
      "Episode 102, Total Reward = 20.0, Total Steps = 1305\n",
      "Episode 103, Total Reward = 9.0, Total Steps = 1314\n",
      "Episode 104, Total Reward = 7.0, Total Steps = 1321\n",
      "Episode 105, Total Reward = 13.0, Total Steps = 1334\n",
      "Episode 106, Total Reward = 4.0, Total Steps = 1338\n",
      "Episode 107, Total Reward = 5.0, Total Steps = 1343\n",
      "Episode 108, Total Reward = 6.0, Total Steps = 1349\n",
      "Episode 109, Total Reward = 7.0, Total Steps = 1356\n",
      "Episode 110, Total Reward = 6.0, Total Steps = 1362\n",
      "Episode 111, Total Reward = 6.0, Total Steps = 1368\n",
      "Episode 112, Total Reward = 8.0, Total Steps = 1376\n",
      "Episode 113, Total Reward = 14.0, Total Steps = 1390\n",
      "Episode 114, Total Reward = 13.0, Total Steps = 1403\n",
      "Episode 115, Total Reward = 11.0, Total Steps = 1414\n",
      "Episode 116, Total Reward = 26.0, Total Steps = 1440\n",
      "Episode 117, Total Reward = 21.0, Total Steps = 1461\n",
      "Episode 118, Total Reward = 8.0, Total Steps = 1469\n",
      "Episode 119, Total Reward = 8.0, Total Steps = 1477\n",
      "Episode 120, Total Reward = 6.0, Total Steps = 1483\n",
      "Episode 121, Total Reward = 10.0, Total Steps = 1493\n",
      "Episode 122, Total Reward = 8.0, Total Steps = 1501\n",
      "Episode 123, Total Reward = 13.0, Total Steps = 1514\n",
      "Episode 124, Total Reward = 7.0, Total Steps = 1521\n",
      "Episode 125, Total Reward = 15.0, Total Steps = 1536\n",
      "Episode 126, Total Reward = 8.0, Total Steps = 1544\n",
      "Episode 127, Total Reward = 8.0, Total Steps = 1552\n",
      "Episode 128, Total Reward = 6.0, Total Steps = 1558\n",
      "Episode 129, Total Reward = 6.0, Total Steps = 1564\n",
      "Episode 130, Total Reward = 7.0, Total Steps = 1571\n",
      "Episode 131, Total Reward = 9.0, Total Steps = 1580\n",
      "Episode 132, Total Reward = 14.0, Total Steps = 1594\n",
      "Episode 133, Total Reward = 4.0, Total Steps = 1598\n",
      "Episode 134, Total Reward = 19.0, Total Steps = 1617\n",
      "Episode 135, Total Reward = 6.0, Total Steps = 1623\n",
      "Episode 136, Total Reward = 6.0, Total Steps = 1629\n",
      "Episode 137, Total Reward = 5.0, Total Steps = 1634\n",
      "Episode 138, Total Reward = 5.0, Total Steps = 1639\n",
      "Episode 139, Total Reward = 5.0, Total Steps = 1644\n",
      "Episode 140, Total Reward = 22.0, Total Steps = 1666\n",
      "Episode 141, Total Reward = 7.0, Total Steps = 1673\n",
      "Episode 142, Total Reward = 8.0, Total Steps = 1681\n",
      "Episode 143, Total Reward = 13.0, Total Steps = 1694\n",
      "Episode 144, Total Reward = 12.0, Total Steps = 1706\n",
      "Episode 145, Total Reward = 19.0, Total Steps = 1725\n",
      "Episode 146, Total Reward = 9.0, Total Steps = 1734\n",
      "Episode 147, Total Reward = 4.0, Total Steps = 1738\n",
      "Episode 148, Total Reward = 9.0, Total Steps = 1747\n",
      "Episode 149, Total Reward = 7.0, Total Steps = 1754\n",
      "Episode 150, Total Reward = 12.0, Total Steps = 1766\n",
      "Episode 151, Total Reward = 11.0, Total Steps = 1777\n",
      "Episode 152, Total Reward = 21.0, Total Steps = 1798\n",
      "Episode 153, Total Reward = 11.0, Total Steps = 1809\n",
      "Episode 154, Total Reward = 9.0, Total Steps = 1818\n",
      "Episode 155, Total Reward = 17.0, Total Steps = 1835\n",
      "Episode 156, Total Reward = 10.0, Total Steps = 1845\n",
      "Episode 157, Total Reward = 12.0, Total Steps = 1857\n",
      "Episode 158, Total Reward = 20.0, Total Steps = 1877\n",
      "Episode 159, Total Reward = 18.0, Total Steps = 1895\n",
      "Episode 160, Total Reward = 16.0, Total Steps = 1911\n",
      "Episode 161, Total Reward = 10.0, Total Steps = 1921\n",
      "Episode 162, Total Reward = 15.0, Total Steps = 1936\n",
      "Episode 163, Total Reward = 17.0, Total Steps = 1953\n",
      "Episode 164, Total Reward = 30.0, Total Steps = 1983\n",
      "Episode 165, Total Reward = 48.0, Total Steps = 2031\n",
      "Episode 166, Total Reward = 8.0, Total Steps = 2039\n",
      "Episode 167, Total Reward = 27.0, Total Steps = 2066\n",
      "Episode 168, Total Reward = 5.0, Total Steps = 2071\n",
      "Episode 169, Total Reward = 5.0, Total Steps = 2076\n",
      "Episode 170, Total Reward = 8.0, Total Steps = 2084\n",
      "Episode 171, Total Reward = 6.0, Total Steps = 2090\n",
      "Episode 172, Total Reward = 13.0, Total Steps = 2103\n",
      "Episode 173, Total Reward = 6.0, Total Steps = 2109\n",
      "Episode 174, Total Reward = 20.0, Total Steps = 2129\n",
      "Episode 175, Total Reward = 5.0, Total Steps = 2134\n",
      "Episode 176, Total Reward = 7.0, Total Steps = 2141\n",
      "Episode 177, Total Reward = 6.0, Total Steps = 2147\n",
      "Episode 178, Total Reward = 14.0, Total Steps = 2161\n",
      "Episode 179, Total Reward = 7.0, Total Steps = 2168\n",
      "Episode 180, Total Reward = 7.0, Total Steps = 2175\n",
      "Episode 181, Total Reward = 15.0, Total Steps = 2190\n",
      "Episode 182, Total Reward = 13.0, Total Steps = 2203\n",
      "Episode 183, Total Reward = 9.0, Total Steps = 2212\n",
      "Episode 184, Total Reward = 10.0, Total Steps = 2222\n",
      "Episode 185, Total Reward = 17.0, Total Steps = 2239\n",
      "Episode 186, Total Reward = 9.0, Total Steps = 2248\n",
      "Episode 187, Total Reward = 21.0, Total Steps = 2269\n",
      "Episode 188, Total Reward = 7.0, Total Steps = 2276\n",
      "Episode 189, Total Reward = 11.0, Total Steps = 2287\n",
      "Episode 190, Total Reward = 15.0, Total Steps = 2302\n",
      "Episode 191, Total Reward = 9.0, Total Steps = 2311\n",
      "Episode 192, Total Reward = 4.0, Total Steps = 2315\n",
      "Episode 193, Total Reward = 6.0, Total Steps = 2321\n",
      "Episode 194, Total Reward = 15.0, Total Steps = 2336\n",
      "Episode 195, Total Reward = 12.0, Total Steps = 2348\n",
      "Episode 196, Total Reward = 11.0, Total Steps = 2359\n",
      "Episode 197, Total Reward = 6.0, Total Steps = 2365\n",
      "Episode 198, Total Reward = 6.0, Total Steps = 2371\n",
      "Episode 199, Total Reward = 29.0, Total Steps = 2400\n",
      "Episode 200, Total Reward = 6.0, Total Steps = 2406\n",
      "Episode 201, Total Reward = 10.0, Total Steps = 2416\n",
      "Episode 202, Total Reward = 11.0, Total Steps = 2427\n",
      "Episode 203, Total Reward = 12.0, Total Steps = 2439\n",
      "Episode 204, Total Reward = 14.0, Total Steps = 2453\n",
      "Episode 205, Total Reward = 11.0, Total Steps = 2464\n",
      "Episode 206, Total Reward = 11.0, Total Steps = 2475\n",
      "Episode 207, Total Reward = 22.0, Total Steps = 2497\n",
      "Episode 208, Total Reward = 11.0, Total Steps = 2508\n",
      "Episode 209, Total Reward = 12.0, Total Steps = 2520\n",
      "Episode 210, Total Reward = 13.0, Total Steps = 2533\n",
      "Episode 211, Total Reward = 23.0, Total Steps = 2556\n",
      "Episode 212, Total Reward = 8.0, Total Steps = 2564\n",
      "Episode 213, Total Reward = 8.0, Total Steps = 2572\n",
      "Episode 214, Total Reward = 16.0, Total Steps = 2588\n",
      "Episode 215, Total Reward = 30.0, Total Steps = 2618\n",
      "Episode 216, Total Reward = 7.0, Total Steps = 2625\n",
      "Episode 217, Total Reward = 13.0, Total Steps = 2638\n",
      "Episode 218, Total Reward = 6.0, Total Steps = 2644\n",
      "Episode 219, Total Reward = 11.0, Total Steps = 2655\n",
      "Episode 220, Total Reward = 16.0, Total Steps = 2671\n",
      "Episode 221, Total Reward = 7.0, Total Steps = 2678\n",
      "Episode 222, Total Reward = 8.0, Total Steps = 2686\n",
      "Episode 223, Total Reward = 7.0, Total Steps = 2693\n",
      "Episode 224, Total Reward = 26.0, Total Steps = 2719\n",
      "Episode 225, Total Reward = 50.0, Total Steps = 2769\n",
      "Episode 226, Total Reward = 18.0, Total Steps = 2787\n",
      "Episode 227, Total Reward = 13.0, Total Steps = 2800\n",
      "Episode 228, Total Reward = 14.0, Total Steps = 2814\n",
      "Episode 229, Total Reward = 9.0, Total Steps = 2823\n",
      "Episode 230, Total Reward = 10.0, Total Steps = 2833\n",
      "Episode 231, Total Reward = 4.0, Total Steps = 2837\n",
      "Episode 232, Total Reward = 4.0, Total Steps = 2841\n",
      "Episode 233, Total Reward = 20.0, Total Steps = 2861\n",
      "Episode 234, Total Reward = 6.0, Total Steps = 2867\n",
      "Episode 235, Total Reward = 6.0, Total Steps = 2873\n",
      "Episode 236, Total Reward = 5.0, Total Steps = 2878\n",
      "Episode 237, Total Reward = 10.0, Total Steps = 2888\n",
      "Episode 238, Total Reward = 20.0, Total Steps = 2908\n",
      "Episode 239, Total Reward = 10.0, Total Steps = 2918\n",
      "Episode 240, Total Reward = 12.0, Total Steps = 2930\n",
      "Episode 241, Total Reward = 10.0, Total Steps = 2940\n",
      "Episode 242, Total Reward = 49.0, Total Steps = 2989\n",
      "Episode 243, Total Reward = 23.0, Total Steps = 3012\n",
      "Episode 244, Total Reward = 8.0, Total Steps = 3020\n",
      "Episode 245, Total Reward = 9.0, Total Steps = 3029\n",
      "Episode 246, Total Reward = 9.0, Total Steps = 3038\n",
      "Episode 247, Total Reward = 21.0, Total Steps = 3059\n",
      "Episode 248, Total Reward = 15.0, Total Steps = 3074\n",
      "Episode 249, Total Reward = 15.0, Total Steps = 3089\n",
      "Episode 250, Total Reward = 27.0, Total Steps = 3116\n",
      "Episode 251, Total Reward = 14.0, Total Steps = 3130\n",
      "Episode 252, Total Reward = 6.0, Total Steps = 3136\n",
      "Episode 253, Total Reward = 9.0, Total Steps = 3145\n",
      "Episode 254, Total Reward = 5.0, Total Steps = 3150\n",
      "Episode 255, Total Reward = 19.0, Total Steps = 3169\n",
      "Episode 256, Total Reward = 38.0, Total Steps = 3207\n",
      "Episode 257, Total Reward = 6.0, Total Steps = 3213\n",
      "Episode 258, Total Reward = 11.0, Total Steps = 3224\n",
      "Episode 259, Total Reward = 10.0, Total Steps = 3234\n",
      "Episode 260, Total Reward = 9.0, Total Steps = 3243\n",
      "Episode 261, Total Reward = 28.0, Total Steps = 3271\n",
      "Episode 262, Total Reward = 7.0, Total Steps = 3278\n",
      "Episode 263, Total Reward = 6.0, Total Steps = 3284\n",
      "Episode 264, Total Reward = 19.0, Total Steps = 3303\n",
      "Episode 265, Total Reward = 22.0, Total Steps = 3325\n",
      "Episode 266, Total Reward = 7.0, Total Steps = 3332\n",
      "Episode 267, Total Reward = 17.0, Total Steps = 3349\n",
      "Episode 268, Total Reward = 12.0, Total Steps = 3361\n",
      "Episode 269, Total Reward = 16.0, Total Steps = 3377\n",
      "Episode 270, Total Reward = 15.0, Total Steps = 3392\n",
      "Episode 271, Total Reward = 7.0, Total Steps = 3399\n",
      "Episode 272, Total Reward = 14.0, Total Steps = 3413\n",
      "Episode 273, Total Reward = 6.0, Total Steps = 3419\n",
      "Episode 274, Total Reward = 10.0, Total Steps = 3429\n",
      "Episode 275, Total Reward = 30.0, Total Steps = 3459\n",
      "Episode 276, Total Reward = 34.0, Total Steps = 3493\n",
      "Episode 277, Total Reward = 33.0, Total Steps = 3526\n",
      "Episode 278, Total Reward = 5.0, Total Steps = 3531\n",
      "Episode 279, Total Reward = 4.0, Total Steps = 3535\n",
      "Episode 280, Total Reward = 24.0, Total Steps = 3559\n",
      "Episode 281, Total Reward = 5.0, Total Steps = 3564\n",
      "Episode 282, Total Reward = 31.0, Total Steps = 3595\n",
      "Episode 283, Total Reward = 15.0, Total Steps = 3610\n",
      "Episode 284, Total Reward = 15.0, Total Steps = 3625\n",
      "Episode 285, Total Reward = 15.0, Total Steps = 3640\n",
      "Episode 286, Total Reward = 11.0, Total Steps = 3651\n",
      "Episode 287, Total Reward = 7.0, Total Steps = 3658\n",
      "Episode 288, Total Reward = 12.0, Total Steps = 3670\n",
      "Episode 289, Total Reward = 25.0, Total Steps = 3695\n",
      "Episode 290, Total Reward = 8.0, Total Steps = 3703\n",
      "Episode 291, Total Reward = 12.0, Total Steps = 3715\n",
      "Episode 292, Total Reward = 10.0, Total Steps = 3725\n",
      "Episode 293, Total Reward = 7.0, Total Steps = 3732\n",
      "Episode 294, Total Reward = 8.0, Total Steps = 3740\n",
      "Episode 295, Total Reward = 19.0, Total Steps = 3759\n",
      "Episode 296, Total Reward = 14.0, Total Steps = 3773\n",
      "Episode 297, Total Reward = 21.0, Total Steps = 3794\n",
      "Episode 298, Total Reward = 9.0, Total Steps = 3803\n",
      "Episode 299, Total Reward = 16.0, Total Steps = 3819\n",
      "Episode 300, Total Reward = 70.0, Total Steps = 3889\n",
      "Episode 301, Total Reward = 8.0, Total Steps = 3897\n",
      "Episode 302, Total Reward = 21.0, Total Steps = 3918\n",
      "Episode 303, Total Reward = 33.0, Total Steps = 3951\n",
      "Episode 304, Total Reward = 17.0, Total Steps = 3968\n",
      "Episode 305, Total Reward = 8.0, Total Steps = 3976\n",
      "Episode 306, Total Reward = 7.0, Total Steps = 3983\n",
      "Episode 307, Total Reward = 18.0, Total Steps = 4001\n",
      "Episode 308, Total Reward = 19.0, Total Steps = 4020\n",
      "Episode 309, Total Reward = 23.0, Total Steps = 4043\n",
      "Episode 310, Total Reward = 6.0, Total Steps = 4049\n",
      "Episode 311, Total Reward = 24.0, Total Steps = 4073\n",
      "Episode 312, Total Reward = 6.0, Total Steps = 4079\n",
      "Episode 313, Total Reward = 16.0, Total Steps = 4095\n",
      "Episode 314, Total Reward = 25.0, Total Steps = 4120\n",
      "Episode 315, Total Reward = 13.0, Total Steps = 4133\n",
      "Episode 316, Total Reward = 32.0, Total Steps = 4165\n",
      "Episode 317, Total Reward = 7.0, Total Steps = 4172\n",
      "Episode 318, Total Reward = 10.0, Total Steps = 4182\n",
      "Episode 319, Total Reward = 23.0, Total Steps = 4205\n",
      "Episode 320, Total Reward = 14.0, Total Steps = 4219\n",
      "Episode 321, Total Reward = 6.0, Total Steps = 4225\n",
      "Episode 322, Total Reward = 6.0, Total Steps = 4231\n",
      "Episode 323, Total Reward = 12.0, Total Steps = 4243\n",
      "Episode 324, Total Reward = 9.0, Total Steps = 4252\n",
      "Episode 325, Total Reward = 12.0, Total Steps = 4264\n",
      "Episode 326, Total Reward = 34.0, Total Steps = 4298\n",
      "Episode 327, Total Reward = 42.0, Total Steps = 4340\n",
      "Episode 328, Total Reward = 15.0, Total Steps = 4355\n",
      "Episode 329, Total Reward = 24.0, Total Steps = 4379\n",
      "Episode 330, Total Reward = 5.0, Total Steps = 4384\n",
      "Episode 331, Total Reward = 13.0, Total Steps = 4397\n",
      "Episode 332, Total Reward = 12.0, Total Steps = 4409\n",
      "Episode 333, Total Reward = 16.0, Total Steps = 4425\n",
      "Episode 334, Total Reward = 25.0, Total Steps = 4450\n",
      "Episode 335, Total Reward = 23.0, Total Steps = 4473\n",
      "Episode 336, Total Reward = 11.0, Total Steps = 4484\n",
      "Episode 337, Total Reward = 14.0, Total Steps = 4498\n",
      "Episode 338, Total Reward = 12.0, Total Steps = 4510\n",
      "Episode 339, Total Reward = 23.0, Total Steps = 4533\n",
      "Episode 340, Total Reward = 4.0, Total Steps = 4537\n",
      "Episode 341, Total Reward = 14.0, Total Steps = 4551\n",
      "Episode 342, Total Reward = 9.0, Total Steps = 4560\n",
      "Episode 343, Total Reward = 10.0, Total Steps = 4570\n",
      "Episode 344, Total Reward = 18.0, Total Steps = 4588\n",
      "Episode 345, Total Reward = 18.0, Total Steps = 4606\n",
      "Episode 346, Total Reward = 6.0, Total Steps = 4612\n",
      "Episode 347, Total Reward = 5.0, Total Steps = 4617\n",
      "Episode 348, Total Reward = 8.0, Total Steps = 4625\n",
      "Episode 349, Total Reward = 8.0, Total Steps = 4633\n",
      "Episode 350, Total Reward = 10.0, Total Steps = 4643\n",
      "Episode 351, Total Reward = 9.0, Total Steps = 4652\n",
      "Episode 352, Total Reward = 10.0, Total Steps = 4662\n",
      "Episode 353, Total Reward = 13.0, Total Steps = 4675\n",
      "Episode 354, Total Reward = 21.0, Total Steps = 4696\n",
      "Episode 355, Total Reward = 13.0, Total Steps = 4709\n",
      "Episode 356, Total Reward = 19.0, Total Steps = 4728\n",
      "Episode 357, Total Reward = 6.0, Total Steps = 4734\n",
      "Episode 358, Total Reward = 28.0, Total Steps = 4762\n",
      "Episode 359, Total Reward = 16.0, Total Steps = 4778\n",
      "Episode 360, Total Reward = 22.0, Total Steps = 4800\n",
      "Episode 361, Total Reward = 54.0, Total Steps = 4854\n",
      "Episode 362, Total Reward = 21.0, Total Steps = 4875\n",
      "Episode 363, Total Reward = 25.0, Total Steps = 4900\n",
      "Episode 364, Total Reward = 18.0, Total Steps = 4918\n",
      "Episode 365, Total Reward = 51.0, Total Steps = 4969\n",
      "Episode 366, Total Reward = 40.0, Total Steps = 5009\n",
      "Episode 367, Total Reward = 8.0, Total Steps = 5017\n",
      "Episode 368, Total Reward = 29.0, Total Steps = 5046\n",
      "Episode 369, Total Reward = 10.0, Total Steps = 5056\n",
      "Episode 370, Total Reward = 9.0, Total Steps = 5065\n",
      "Episode 371, Total Reward = 51.0, Total Steps = 5116\n",
      "Episode 372, Total Reward = 15.0, Total Steps = 5131\n",
      "Episode 373, Total Reward = 10.0, Total Steps = 5141\n",
      "Episode 374, Total Reward = 15.0, Total Steps = 5156\n",
      "Episode 375, Total Reward = 5.0, Total Steps = 5161\n",
      "Episode 376, Total Reward = 13.0, Total Steps = 5174\n",
      "Episode 377, Total Reward = 20.0, Total Steps = 5194\n",
      "Episode 378, Total Reward = 7.0, Total Steps = 5201\n",
      "Episode 379, Total Reward = 32.0, Total Steps = 5233\n",
      "Episode 380, Total Reward = 20.0, Total Steps = 5253\n",
      "Episode 381, Total Reward = 27.0, Total Steps = 5280\n",
      "Episode 382, Total Reward = 17.0, Total Steps = 5297\n",
      "Episode 383, Total Reward = 66.0, Total Steps = 5363\n",
      "Episode 384, Total Reward = 43.0, Total Steps = 5406\n",
      "Episode 385, Total Reward = 5.0, Total Steps = 5411\n",
      "Episode 386, Total Reward = 17.0, Total Steps = 5428\n",
      "Episode 387, Total Reward = 10.0, Total Steps = 5438\n",
      "Episode 388, Total Reward = 23.0, Total Steps = 5461\n",
      "Episode 389, Total Reward = 8.0, Total Steps = 5469\n",
      "Episode 390, Total Reward = 54.0, Total Steps = 5523\n",
      "Episode 391, Total Reward = 15.0, Total Steps = 5538\n",
      "Episode 392, Total Reward = 6.0, Total Steps = 5544\n",
      "Episode 393, Total Reward = 9.0, Total Steps = 5553\n",
      "Episode 394, Total Reward = 12.0, Total Steps = 5565\n",
      "Episode 395, Total Reward = 12.0, Total Steps = 5577\n",
      "Episode 396, Total Reward = 14.0, Total Steps = 5591\n",
      "Episode 397, Total Reward = 21.0, Total Steps = 5612\n",
      "Episode 398, Total Reward = 28.0, Total Steps = 5640\n",
      "Episode 399, Total Reward = 31.0, Total Steps = 5671\n",
      "Episode 400, Total Reward = 19.0, Total Steps = 5690\n",
      "Episode 401, Total Reward = 19.0, Total Steps = 5709\n",
      "Episode 402, Total Reward = 12.0, Total Steps = 5721\n",
      "Episode 403, Total Reward = 20.0, Total Steps = 5741\n",
      "Episode 404, Total Reward = 16.0, Total Steps = 5757\n",
      "Episode 405, Total Reward = 26.0, Total Steps = 5783\n",
      "Episode 406, Total Reward = 26.0, Total Steps = 5809\n",
      "Episode 407, Total Reward = 22.0, Total Steps = 5831\n",
      "Episode 408, Total Reward = 35.0, Total Steps = 5866\n",
      "Episode 409, Total Reward = 29.0, Total Steps = 5895\n",
      "Episode 410, Total Reward = 28.0, Total Steps = 5923\n",
      "Episode 411, Total Reward = 23.0, Total Steps = 5946\n",
      "Episode 412, Total Reward = 6.0, Total Steps = 5952\n",
      "Episode 413, Total Reward = 21.0, Total Steps = 5973\n",
      "Episode 414, Total Reward = 40.0, Total Steps = 6013\n",
      "Episode 415, Total Reward = 5.0, Total Steps = 6018\n",
      "Episode 416, Total Reward = 13.0, Total Steps = 6031\n",
      "Episode 417, Total Reward = 4.0, Total Steps = 6035\n",
      "Episode 418, Total Reward = 20.0, Total Steps = 6055\n",
      "Episode 419, Total Reward = 9.0, Total Steps = 6064\n",
      "Episode 420, Total Reward = 22.0, Total Steps = 6086\n",
      "Episode 421, Total Reward = 11.0, Total Steps = 6097\n",
      "Episode 422, Total Reward = 17.0, Total Steps = 6114\n",
      "Episode 423, Total Reward = 46.0, Total Steps = 6160\n",
      "Episode 424, Total Reward = 20.0, Total Steps = 6180\n",
      "Episode 425, Total Reward = 12.0, Total Steps = 6192\n",
      "Episode 426, Total Reward = 6.0, Total Steps = 6198\n",
      "Episode 427, Total Reward = 20.0, Total Steps = 6218\n",
      "Episode 428, Total Reward = 31.0, Total Steps = 6249\n",
      "Episode 429, Total Reward = 9.0, Total Steps = 6258\n",
      "Episode 430, Total Reward = 13.0, Total Steps = 6271\n",
      "Episode 431, Total Reward = 11.0, Total Steps = 6282\n",
      "Episode 432, Total Reward = 6.0, Total Steps = 6288\n",
      "Episode 433, Total Reward = 31.0, Total Steps = 6319\n",
      "Episode 434, Total Reward = 33.0, Total Steps = 6352\n",
      "Episode 435, Total Reward = 20.0, Total Steps = 6372\n",
      "Episode 436, Total Reward = 34.0, Total Steps = 6406\n",
      "Episode 437, Total Reward = 11.0, Total Steps = 6417\n",
      "Episode 438, Total Reward = 6.0, Total Steps = 6423\n",
      "Episode 439, Total Reward = 18.0, Total Steps = 6441\n",
      "Episode 440, Total Reward = 46.0, Total Steps = 6487\n",
      "Episode 441, Total Reward = 20.0, Total Steps = 6507\n",
      "Episode 442, Total Reward = 13.0, Total Steps = 6520\n",
      "Episode 443, Total Reward = 36.0, Total Steps = 6556\n",
      "Episode 444, Total Reward = 36.0, Total Steps = 6592\n",
      "Episode 445, Total Reward = 92.0, Total Steps = 6684\n",
      "Episode 446, Total Reward = 12.0, Total Steps = 6696\n",
      "Episode 447, Total Reward = 9.0, Total Steps = 6705\n",
      "Episode 448, Total Reward = 8.0, Total Steps = 6713\n",
      "Episode 449, Total Reward = 17.0, Total Steps = 6730\n",
      "Episode 450, Total Reward = 15.0, Total Steps = 6745\n",
      "Episode 451, Total Reward = 7.0, Total Steps = 6752\n",
      "Episode 452, Total Reward = 21.0, Total Steps = 6773\n",
      "Episode 453, Total Reward = 9.0, Total Steps = 6782\n",
      "Episode 454, Total Reward = 10.0, Total Steps = 6792\n",
      "Episode 455, Total Reward = 22.0, Total Steps = 6814\n",
      "Episode 456, Total Reward = 9.0, Total Steps = 6823\n",
      "Episode 457, Total Reward = 19.0, Total Steps = 6842\n",
      "Episode 458, Total Reward = 42.0, Total Steps = 6884\n",
      "Episode 459, Total Reward = 16.0, Total Steps = 6900\n",
      "Episode 460, Total Reward = 42.0, Total Steps = 6942\n",
      "Episode 461, Total Reward = 31.0, Total Steps = 6973\n",
      "Episode 462, Total Reward = 19.0, Total Steps = 6992\n",
      "Episode 463, Total Reward = 17.0, Total Steps = 7009\n",
      "Episode 464, Total Reward = 23.0, Total Steps = 7032\n",
      "Episode 465, Total Reward = 41.0, Total Steps = 7073\n",
      "Episode 466, Total Reward = 17.0, Total Steps = 7090\n",
      "Episode 467, Total Reward = 28.0, Total Steps = 7118\n",
      "Episode 468, Total Reward = 8.0, Total Steps = 7126\n",
      "Episode 469, Total Reward = 5.0, Total Steps = 7131\n",
      "Episode 470, Total Reward = 18.0, Total Steps = 7149\n",
      "Episode 471, Total Reward = 15.0, Total Steps = 7164\n",
      "Episode 472, Total Reward = 34.0, Total Steps = 7198\n",
      "Episode 473, Total Reward = 49.0, Total Steps = 7247\n",
      "Episode 474, Total Reward = 23.0, Total Steps = 7270\n",
      "Episode 475, Total Reward = 10.0, Total Steps = 7280\n",
      "Episode 476, Total Reward = 10.0, Total Steps = 7290\n",
      "Episode 477, Total Reward = 15.0, Total Steps = 7305\n",
      "Episode 478, Total Reward = 20.0, Total Steps = 7325\n",
      "Episode 479, Total Reward = 8.0, Total Steps = 7333\n",
      "Episode 480, Total Reward = 13.0, Total Steps = 7346\n",
      "Episode 481, Total Reward = 15.0, Total Steps = 7361\n",
      "Episode 482, Total Reward = 13.0, Total Steps = 7374\n",
      "Episode 483, Total Reward = 42.0, Total Steps = 7416\n",
      "Episode 484, Total Reward = 35.0, Total Steps = 7451\n",
      "Episode 485, Total Reward = 41.0, Total Steps = 7492\n",
      "Episode 486, Total Reward = 38.0, Total Steps = 7530\n",
      "Episode 487, Total Reward = 9.0, Total Steps = 7539\n",
      "Episode 488, Total Reward = 18.0, Total Steps = 7557\n",
      "Episode 489, Total Reward = 35.0, Total Steps = 7592\n",
      "Episode 490, Total Reward = 12.0, Total Steps = 7604\n",
      "Episode 491, Total Reward = 24.0, Total Steps = 7628\n",
      "Episode 492, Total Reward = 26.0, Total Steps = 7654\n",
      "Episode 493, Total Reward = 14.0, Total Steps = 7668\n",
      "Episode 494, Total Reward = 16.0, Total Steps = 7684\n",
      "Episode 495, Total Reward = 19.0, Total Steps = 7703\n",
      "Episode 496, Total Reward = 5.0, Total Steps = 7708\n",
      "Episode 497, Total Reward = 53.0, Total Steps = 7761\n",
      "Episode 498, Total Reward = 7.0, Total Steps = 7768\n",
      "Episode 499, Total Reward = 25.0, Total Steps = 7793\n",
      "Episode 500, Total Reward = 13.0, Total Steps = 7806\n",
      "Episode 501, Total Reward = 38.0, Total Steps = 7844\n",
      "Episode 502, Total Reward = 9.0, Total Steps = 7853\n",
      "Episode 503, Total Reward = 30.0, Total Steps = 7883\n",
      "Episode 504, Total Reward = 11.0, Total Steps = 7894\n",
      "Episode 505, Total Reward = 15.0, Total Steps = 7909\n",
      "Episode 506, Total Reward = 11.0, Total Steps = 7920\n",
      "Episode 507, Total Reward = 11.0, Total Steps = 7931\n",
      "Episode 508, Total Reward = 5.0, Total Steps = 7936\n",
      "Episode 509, Total Reward = 42.0, Total Steps = 7978\n",
      "Episode 510, Total Reward = 37.0, Total Steps = 8015\n",
      "Episode 511, Total Reward = 35.0, Total Steps = 8050\n",
      "Episode 512, Total Reward = 13.0, Total Steps = 8063\n",
      "Episode 513, Total Reward = 9.0, Total Steps = 8072\n",
      "Episode 514, Total Reward = 11.0, Total Steps = 8083\n",
      "Episode 515, Total Reward = 20.0, Total Steps = 8103\n",
      "Episode 516, Total Reward = 6.0, Total Steps = 8109\n",
      "Episode 517, Total Reward = 20.0, Total Steps = 8129\n",
      "Episode 518, Total Reward = 26.0, Total Steps = 8155\n",
      "Episode 519, Total Reward = 17.0, Total Steps = 8172\n",
      "Episode 520, Total Reward = 40.0, Total Steps = 8212\n",
      "Episode 521, Total Reward = 16.0, Total Steps = 8228\n",
      "Episode 522, Total Reward = 24.0, Total Steps = 8252\n",
      "Episode 523, Total Reward = 82.0, Total Steps = 8334\n",
      "Episode 524, Total Reward = 19.0, Total Steps = 8353\n",
      "Episode 525, Total Reward = 39.0, Total Steps = 8392\n",
      "Episode 526, Total Reward = 22.0, Total Steps = 8414\n",
      "Episode 527, Total Reward = 16.0, Total Steps = 8430\n",
      "Episode 528, Total Reward = 26.0, Total Steps = 8456\n",
      "Episode 529, Total Reward = 16.0, Total Steps = 8472\n",
      "Episode 530, Total Reward = 9.0, Total Steps = 8481\n",
      "Episode 531, Total Reward = 26.0, Total Steps = 8507\n",
      "Episode 532, Total Reward = 21.0, Total Steps = 8528\n",
      "Episode 533, Total Reward = 18.0, Total Steps = 8546\n",
      "Episode 534, Total Reward = 18.0, Total Steps = 8564\n",
      "Episode 535, Total Reward = 38.0, Total Steps = 8602\n",
      "Episode 536, Total Reward = 37.0, Total Steps = 8639\n",
      "Episode 537, Total Reward = 72.0, Total Steps = 8711\n",
      "Episode 538, Total Reward = 15.0, Total Steps = 8726\n",
      "Episode 539, Total Reward = 32.0, Total Steps = 8758\n",
      "Episode 540, Total Reward = 19.0, Total Steps = 8777\n",
      "Episode 541, Total Reward = 8.0, Total Steps = 8785\n",
      "Episode 542, Total Reward = 7.0, Total Steps = 8792\n",
      "Episode 543, Total Reward = 21.0, Total Steps = 8813\n",
      "Episode 544, Total Reward = 23.0, Total Steps = 8836\n",
      "Episode 545, Total Reward = 16.0, Total Steps = 8852\n",
      "Episode 546, Total Reward = 9.0, Total Steps = 8861\n",
      "Episode 547, Total Reward = 15.0, Total Steps = 8876\n",
      "Episode 548, Total Reward = 22.0, Total Steps = 8898\n",
      "Episode 549, Total Reward = 9.0, Total Steps = 8907\n",
      "Episode 550, Total Reward = 94.0, Total Steps = 9001\n",
      "Episode 551, Total Reward = 162.0, Total Steps = 9163\n",
      "Episode 552, Total Reward = 16.0, Total Steps = 9179\n",
      "Episode 553, Total Reward = 7.0, Total Steps = 9186\n",
      "Episode 554, Total Reward = 10.0, Total Steps = 9196\n",
      "Episode 555, Total Reward = 11.0, Total Steps = 9207\n",
      "Episode 556, Total Reward = 5.0, Total Steps = 9212\n",
      "Episode 557, Total Reward = 14.0, Total Steps = 9226\n",
      "Episode 558, Total Reward = 5.0, Total Steps = 9231\n",
      "Episode 559, Total Reward = 8.0, Total Steps = 9239\n",
      "Episode 560, Total Reward = 5.0, Total Steps = 9244\n",
      "Episode 561, Total Reward = 20.0, Total Steps = 9264\n",
      "Episode 562, Total Reward = 22.0, Total Steps = 9286\n",
      "Episode 563, Total Reward = 6.0, Total Steps = 9292\n",
      "Episode 564, Total Reward = 15.0, Total Steps = 9307\n",
      "Episode 565, Total Reward = 8.0, Total Steps = 9315\n",
      "Episode 566, Total Reward = 7.0, Total Steps = 9322\n",
      "Episode 567, Total Reward = 6.0, Total Steps = 9328\n",
      "Episode 568, Total Reward = 15.0, Total Steps = 9343\n",
      "Episode 569, Total Reward = 18.0, Total Steps = 9361\n",
      "Episode 570, Total Reward = 12.0, Total Steps = 9373\n",
      "Episode 571, Total Reward = 22.0, Total Steps = 9395\n",
      "Episode 572, Total Reward = 6.0, Total Steps = 9401\n",
      "Episode 573, Total Reward = 7.0, Total Steps = 9408\n",
      "Episode 574, Total Reward = 21.0, Total Steps = 9429\n",
      "Episode 575, Total Reward = 9.0, Total Steps = 9438\n",
      "Episode 576, Total Reward = 44.0, Total Steps = 9482\n",
      "Episode 577, Total Reward = 14.0, Total Steps = 9496\n",
      "Episode 578, Total Reward = 7.0, Total Steps = 9503\n",
      "Episode 579, Total Reward = 33.0, Total Steps = 9536\n",
      "Episode 580, Total Reward = 7.0, Total Steps = 9543\n",
      "Episode 581, Total Reward = 14.0, Total Steps = 9557\n",
      "Episode 582, Total Reward = 14.0, Total Steps = 9571\n",
      "Episode 583, Total Reward = 29.0, Total Steps = 9600\n",
      "Episode 584, Total Reward = 16.0, Total Steps = 9616\n",
      "Episode 585, Total Reward = 49.0, Total Steps = 9665\n",
      "Episode 586, Total Reward = 14.0, Total Steps = 9679\n",
      "Episode 587, Total Reward = 31.0, Total Steps = 9710\n",
      "Episode 588, Total Reward = 7.0, Total Steps = 9717\n",
      "Episode 589, Total Reward = 41.0, Total Steps = 9758\n",
      "Episode 590, Total Reward = 39.0, Total Steps = 9797\n",
      "Episode 591, Total Reward = 15.0, Total Steps = 9812\n",
      "Episode 592, Total Reward = 19.0, Total Steps = 9831\n",
      "Episode 593, Total Reward = 19.0, Total Steps = 9850\n",
      "Episode 594, Total Reward = 15.0, Total Steps = 9865\n",
      "Episode 595, Total Reward = 21.0, Total Steps = 9886\n",
      "Episode 596, Total Reward = 27.0, Total Steps = 9913\n",
      "Episode 597, Total Reward = 24.0, Total Steps = 9937\n",
      "Episode 598, Total Reward = 39.0, Total Steps = 9976\n",
      "Episode 599, Total Reward = 54.0, Total Steps = 10030\n",
      "Episode 600, Total Reward = 35.0, Total Steps = 10065\n",
      "Episode 601, Total Reward = 22.0, Total Steps = 10087\n",
      "Episode 602, Total Reward = 21.0, Total Steps = 10108\n",
      "Episode 603, Total Reward = 36.0, Total Steps = 10144\n",
      "Episode 604, Total Reward = 14.0, Total Steps = 10158\n",
      "Episode 605, Total Reward = 8.0, Total Steps = 10166\n",
      "Episode 606, Total Reward = 7.0, Total Steps = 10173\n",
      "Episode 607, Total Reward = 17.0, Total Steps = 10190\n",
      "Episode 608, Total Reward = 64.0, Total Steps = 10254\n",
      "Episode 609, Total Reward = 21.0, Total Steps = 10275\n",
      "Episode 610, Total Reward = 11.0, Total Steps = 10286\n",
      "Episode 611, Total Reward = 28.0, Total Steps = 10314\n",
      "Episode 612, Total Reward = 9.0, Total Steps = 10323\n",
      "Episode 613, Total Reward = 25.0, Total Steps = 10348\n",
      "Episode 614, Total Reward = 12.0, Total Steps = 10360\n",
      "Episode 615, Total Reward = 24.0, Total Steps = 10384\n",
      "Episode 616, Total Reward = 21.0, Total Steps = 10405\n",
      "Episode 617, Total Reward = 14.0, Total Steps = 10419\n",
      "Episode 618, Total Reward = 11.0, Total Steps = 10430\n",
      "Episode 619, Total Reward = 49.0, Total Steps = 10479\n",
      "Episode 620, Total Reward = 23.0, Total Steps = 10502\n",
      "Episode 621, Total Reward = 57.0, Total Steps = 10559\n",
      "Episode 622, Total Reward = 44.0, Total Steps = 10603\n",
      "Episode 623, Total Reward = 29.0, Total Steps = 10632\n",
      "Episode 624, Total Reward = 30.0, Total Steps = 10662\n",
      "Episode 625, Total Reward = 26.0, Total Steps = 10688\n",
      "Episode 626, Total Reward = 19.0, Total Steps = 10707\n",
      "Episode 627, Total Reward = 14.0, Total Steps = 10721\n",
      "Episode 628, Total Reward = 4.0, Total Steps = 10725\n",
      "Episode 629, Total Reward = 16.0, Total Steps = 10741\n",
      "Episode 630, Total Reward = 14.0, Total Steps = 10755\n",
      "Episode 631, Total Reward = 51.0, Total Steps = 10806\n",
      "Episode 632, Total Reward = 64.0, Total Steps = 10870\n",
      "Episode 633, Total Reward = 45.0, Total Steps = 10915\n",
      "Episode 634, Total Reward = 20.0, Total Steps = 10935\n",
      "Episode 635, Total Reward = 15.0, Total Steps = 10950\n",
      "Episode 636, Total Reward = 14.0, Total Steps = 10964\n",
      "Episode 637, Total Reward = 41.0, Total Steps = 11005\n",
      "Episode 638, Total Reward = 8.0, Total Steps = 11013\n",
      "Episode 639, Total Reward = 24.0, Total Steps = 11037\n",
      "Episode 640, Total Reward = 14.0, Total Steps = 11051\n",
      "Episode 641, Total Reward = 35.0, Total Steps = 11086\n",
      "Episode 642, Total Reward = 21.0, Total Steps = 11107\n",
      "Episode 643, Total Reward = 37.0, Total Steps = 11144\n",
      "Episode 644, Total Reward = 8.0, Total Steps = 11152\n",
      "Episode 645, Total Reward = 52.0, Total Steps = 11204\n",
      "Episode 646, Total Reward = 7.0, Total Steps = 11211\n",
      "Episode 647, Total Reward = 36.0, Total Steps = 11247\n",
      "Episode 648, Total Reward = 14.0, Total Steps = 11261\n",
      "Episode 649, Total Reward = 10.0, Total Steps = 11271\n",
      "Episode 650, Total Reward = 12.0, Total Steps = 11283\n",
      "Episode 651, Total Reward = 11.0, Total Steps = 11294\n",
      "Episode 652, Total Reward = 16.0, Total Steps = 11310\n",
      "Episode 653, Total Reward = 25.0, Total Steps = 11335\n",
      "Episode 654, Total Reward = 35.0, Total Steps = 11370\n",
      "Episode 655, Total Reward = 24.0, Total Steps = 11394\n",
      "Episode 656, Total Reward = 35.0, Total Steps = 11429\n",
      "Episode 657, Total Reward = 29.0, Total Steps = 11458\n",
      "Episode 658, Total Reward = 10.0, Total Steps = 11468\n",
      "Episode 659, Total Reward = 7.0, Total Steps = 11475\n",
      "Episode 660, Total Reward = 25.0, Total Steps = 11500\n",
      "Episode 661, Total Reward = 35.0, Total Steps = 11535\n",
      "Episode 662, Total Reward = 30.0, Total Steps = 11565\n",
      "Episode 663, Total Reward = 24.0, Total Steps = 11589\n",
      "Episode 664, Total Reward = 44.0, Total Steps = 11633\n",
      "Episode 665, Total Reward = 42.0, Total Steps = 11675\n",
      "Episode 666, Total Reward = 11.0, Total Steps = 11686\n",
      "Episode 667, Total Reward = 30.0, Total Steps = 11716\n",
      "Episode 668, Total Reward = 20.0, Total Steps = 11736\n",
      "Episode 669, Total Reward = 28.0, Total Steps = 11764\n",
      "Episode 670, Total Reward = 13.0, Total Steps = 11777\n",
      "Episode 671, Total Reward = 9.0, Total Steps = 11786\n",
      "Episode 672, Total Reward = 35.0, Total Steps = 11821\n",
      "Episode 673, Total Reward = 39.0, Total Steps = 11860\n",
      "Episode 674, Total Reward = 42.0, Total Steps = 11902\n",
      "Episode 675, Total Reward = 42.0, Total Steps = 11944\n",
      "Episode 676, Total Reward = 38.0, Total Steps = 11982\n",
      "Episode 677, Total Reward = 37.0, Total Steps = 12019\n",
      "Episode 678, Total Reward = 24.0, Total Steps = 12043\n",
      "Episode 679, Total Reward = 48.0, Total Steps = 12091\n",
      "Episode 680, Total Reward = 16.0, Total Steps = 12107\n",
      "Episode 681, Total Reward = 35.0, Total Steps = 12142\n",
      "Episode 682, Total Reward = 19.0, Total Steps = 12161\n",
      "Episode 683, Total Reward = 67.0, Total Steps = 12228\n",
      "Episode 684, Total Reward = 8.0, Total Steps = 12236\n",
      "Episode 685, Total Reward = 56.0, Total Steps = 12292\n",
      "Episode 686, Total Reward = 27.0, Total Steps = 12319\n",
      "Episode 687, Total Reward = 22.0, Total Steps = 12341\n",
      "Episode 688, Total Reward = 10.0, Total Steps = 12351\n",
      "Episode 689, Total Reward = 22.0, Total Steps = 12373\n",
      "Episode 690, Total Reward = 18.0, Total Steps = 12391\n",
      "Episode 691, Total Reward = 35.0, Total Steps = 12426\n",
      "Episode 692, Total Reward = 5.0, Total Steps = 12431\n",
      "Episode 693, Total Reward = 21.0, Total Steps = 12452\n",
      "Episode 694, Total Reward = 17.0, Total Steps = 12469\n",
      "Episode 695, Total Reward = 28.0, Total Steps = 12497\n",
      "Episode 696, Total Reward = 23.0, Total Steps = 12520\n",
      "Episode 697, Total Reward = 32.0, Total Steps = 12552\n",
      "Episode 698, Total Reward = 10.0, Total Steps = 12562\n",
      "Episode 699, Total Reward = 31.0, Total Steps = 12593\n",
      "Episode 700, Total Reward = 60.0, Total Steps = 12653\n",
      "Episode 701, Total Reward = 15.0, Total Steps = 12668\n",
      "Episode 702, Total Reward = 43.0, Total Steps = 12711\n",
      "Episode 703, Total Reward = 66.0, Total Steps = 12777\n",
      "Episode 704, Total Reward = 51.0, Total Steps = 12828\n",
      "Episode 705, Total Reward = 38.0, Total Steps = 12866\n",
      "Episode 706, Total Reward = 14.0, Total Steps = 12880\n",
      "Episode 707, Total Reward = 9.0, Total Steps = 12889\n",
      "Episode 708, Total Reward = 9.0, Total Steps = 12898\n",
      "Episode 709, Total Reward = 15.0, Total Steps = 12913\n",
      "Episode 710, Total Reward = 56.0, Total Steps = 12969\n",
      "Episode 711, Total Reward = 65.0, Total Steps = 13034\n",
      "Episode 712, Total Reward = 6.0, Total Steps = 13040\n",
      "Episode 713, Total Reward = 16.0, Total Steps = 13056\n",
      "Episode 714, Total Reward = 17.0, Total Steps = 13073\n",
      "Episode 715, Total Reward = 43.0, Total Steps = 13116\n",
      "Episode 716, Total Reward = 12.0, Total Steps = 13128\n",
      "Episode 717, Total Reward = 17.0, Total Steps = 13145\n",
      "Episode 718, Total Reward = 9.0, Total Steps = 13154\n",
      "Episode 719, Total Reward = 60.0, Total Steps = 13214\n",
      "Episode 720, Total Reward = 51.0, Total Steps = 13265\n",
      "Episode 721, Total Reward = 53.0, Total Steps = 13318\n",
      "Episode 722, Total Reward = 5.0, Total Steps = 13323\n",
      "Episode 723, Total Reward = 19.0, Total Steps = 13342\n",
      "Episode 724, Total Reward = 36.0, Total Steps = 13378\n",
      "Episode 725, Total Reward = 17.0, Total Steps = 13395\n",
      "Episode 726, Total Reward = 28.0, Total Steps = 13423\n",
      "Episode 727, Total Reward = 23.0, Total Steps = 13446\n",
      "Episode 728, Total Reward = 40.0, Total Steps = 13486\n",
      "Episode 729, Total Reward = 29.0, Total Steps = 13515\n",
      "Episode 730, Total Reward = 12.0, Total Steps = 13527\n",
      "Episode 731, Total Reward = 53.0, Total Steps = 13580\n",
      "Episode 732, Total Reward = 94.0, Total Steps = 13674\n",
      "Episode 733, Total Reward = 10.0, Total Steps = 13684\n",
      "Episode 734, Total Reward = 68.0, Total Steps = 13752\n",
      "Episode 735, Total Reward = 24.0, Total Steps = 13776\n",
      "Episode 736, Total Reward = 21.0, Total Steps = 13797\n",
      "Episode 737, Total Reward = 7.0, Total Steps = 13804\n",
      "Episode 738, Total Reward = 21.0, Total Steps = 13825\n",
      "Episode 739, Total Reward = 8.0, Total Steps = 13833\n",
      "Episode 740, Total Reward = 23.0, Total Steps = 13856\n",
      "Episode 741, Total Reward = 7.0, Total Steps = 13863\n",
      "Episode 742, Total Reward = 20.0, Total Steps = 13883\n",
      "Episode 743, Total Reward = 28.0, Total Steps = 13911\n",
      "Episode 744, Total Reward = 42.0, Total Steps = 13953\n",
      "Episode 745, Total Reward = 86.0, Total Steps = 14039\n",
      "Episode 746, Total Reward = 42.0, Total Steps = 14081\n",
      "Episode 747, Total Reward = 7.0, Total Steps = 14088\n",
      "Episode 748, Total Reward = 14.0, Total Steps = 14102\n",
      "Episode 749, Total Reward = 14.0, Total Steps = 14116\n",
      "Episode 750, Total Reward = 7.0, Total Steps = 14123\n",
      "Episode 751, Total Reward = 8.0, Total Steps = 14131\n",
      "Episode 752, Total Reward = 43.0, Total Steps = 14174\n",
      "Episode 753, Total Reward = 27.0, Total Steps = 14201\n",
      "Episode 754, Total Reward = 12.0, Total Steps = 14213\n",
      "Episode 755, Total Reward = 42.0, Total Steps = 14255\n",
      "Episode 756, Total Reward = 42.0, Total Steps = 14297\n",
      "Episode 757, Total Reward = 21.0, Total Steps = 14318\n",
      "Episode 758, Total Reward = 44.0, Total Steps = 14362\n",
      "Episode 759, Total Reward = 51.0, Total Steps = 14413\n",
      "Episode 760, Total Reward = 35.0, Total Steps = 14448\n",
      "Episode 761, Total Reward = 53.0, Total Steps = 14501\n",
      "Episode 762, Total Reward = 13.0, Total Steps = 14514\n",
      "Episode 763, Total Reward = 11.0, Total Steps = 14525\n",
      "Episode 764, Total Reward = 8.0, Total Steps = 14533\n",
      "Episode 765, Total Reward = 6.0, Total Steps = 14539\n",
      "Episode 766, Total Reward = 11.0, Total Steps = 14550\n",
      "Episode 767, Total Reward = 10.0, Total Steps = 14560\n",
      "Episode 768, Total Reward = 15.0, Total Steps = 14575\n",
      "Episode 769, Total Reward = 17.0, Total Steps = 14592\n",
      "Episode 770, Total Reward = 26.0, Total Steps = 14618\n",
      "Episode 771, Total Reward = 21.0, Total Steps = 14639\n",
      "Episode 772, Total Reward = 8.0, Total Steps = 14647\n",
      "Episode 773, Total Reward = 174.0, Total Steps = 14821\n",
      "Episode 774, Total Reward = 71.0, Total Steps = 14892\n",
      "Episode 775, Total Reward = 21.0, Total Steps = 14913\n",
      "Episode 776, Total Reward = 36.0, Total Steps = 14949\n",
      "Episode 777, Total Reward = 38.0, Total Steps = 14987\n",
      "Episode 778, Total Reward = 17.0, Total Steps = 15004\n",
      "Episode 779, Total Reward = 20.0, Total Steps = 15024\n",
      "Episode 780, Total Reward = 11.0, Total Steps = 15035\n",
      "Episode 781, Total Reward = 43.0, Total Steps = 15078\n",
      "Episode 782, Total Reward = 21.0, Total Steps = 15099\n",
      "Episode 783, Total Reward = 4.0, Total Steps = 15103\n",
      "Episode 784, Total Reward = 26.0, Total Steps = 15129\n",
      "Episode 785, Total Reward = 39.0, Total Steps = 15168\n",
      "Episode 786, Total Reward = 13.0, Total Steps = 15181\n",
      "Episode 787, Total Reward = 30.0, Total Steps = 15211\n",
      "Episode 788, Total Reward = 22.0, Total Steps = 15233\n",
      "Episode 789, Total Reward = 23.0, Total Steps = 15256\n",
      "Episode 790, Total Reward = 23.0, Total Steps = 15279\n",
      "Episode 791, Total Reward = 17.0, Total Steps = 15296\n",
      "Episode 792, Total Reward = 68.0, Total Steps = 15364\n",
      "Episode 793, Total Reward = 58.0, Total Steps = 15422\n",
      "Episode 794, Total Reward = 18.0, Total Steps = 15440\n",
      "Episode 795, Total Reward = 48.0, Total Steps = 15488\n",
      "Episode 796, Total Reward = 58.0, Total Steps = 15546\n",
      "Episode 797, Total Reward = 15.0, Total Steps = 15561\n",
      "Episode 798, Total Reward = 13.0, Total Steps = 15574\n",
      "Episode 799, Total Reward = 74.0, Total Steps = 15648\n",
      "Episode 800, Total Reward = 47.0, Total Steps = 15695\n",
      "Episode 801, Total Reward = 38.0, Total Steps = 15733\n",
      "Episode 802, Total Reward = 16.0, Total Steps = 15749\n",
      "Episode 803, Total Reward = 68.0, Total Steps = 15817\n",
      "Episode 804, Total Reward = 9.0, Total Steps = 15826\n",
      "Episode 805, Total Reward = 7.0, Total Steps = 15833\n",
      "Episode 806, Total Reward = 31.0, Total Steps = 15864\n",
      "Episode 807, Total Reward = 38.0, Total Steps = 15902\n",
      "Episode 808, Total Reward = 28.0, Total Steps = 15930\n",
      "Episode 809, Total Reward = 54.0, Total Steps = 15984\n",
      "Episode 810, Total Reward = 22.0, Total Steps = 16006\n",
      "Episode 811, Total Reward = 13.0, Total Steps = 16019\n",
      "Episode 812, Total Reward = 29.0, Total Steps = 16048\n",
      "Episode 813, Total Reward = 43.0, Total Steps = 16091\n",
      "Episode 814, Total Reward = 51.0, Total Steps = 16142\n",
      "Episode 815, Total Reward = 29.0, Total Steps = 16171\n",
      "Episode 816, Total Reward = 14.0, Total Steps = 16185\n",
      "Episode 817, Total Reward = 47.0, Total Steps = 16232\n",
      "Episode 818, Total Reward = 8.0, Total Steps = 16240\n",
      "Episode 819, Total Reward = 23.0, Total Steps = 16263\n",
      "Episode 820, Total Reward = 45.0, Total Steps = 16308\n",
      "Episode 821, Total Reward = 74.0, Total Steps = 16382\n",
      "Episode 822, Total Reward = 45.0, Total Steps = 16427\n",
      "Episode 823, Total Reward = 61.0, Total Steps = 16488\n",
      "Episode 824, Total Reward = 32.0, Total Steps = 16520\n",
      "Episode 825, Total Reward = 11.0, Total Steps = 16531\n",
      "Episode 826, Total Reward = 7.0, Total Steps = 16538\n",
      "Episode 827, Total Reward = 12.0, Total Steps = 16550\n",
      "Episode 828, Total Reward = 20.0, Total Steps = 16570\n",
      "Episode 829, Total Reward = 37.0, Total Steps = 16607\n",
      "Episode 830, Total Reward = 51.0, Total Steps = 16658\n",
      "Episode 831, Total Reward = 26.0, Total Steps = 16684\n",
      "Episode 832, Total Reward = 78.0, Total Steps = 16762\n",
      "Episode 833, Total Reward = 39.0, Total Steps = 16801\n",
      "Episode 834, Total Reward = 57.0, Total Steps = 16858\n",
      "Episode 835, Total Reward = 25.0, Total Steps = 16883\n",
      "Episode 836, Total Reward = 49.0, Total Steps = 16932\n",
      "Episode 837, Total Reward = 24.0, Total Steps = 16956\n",
      "Episode 838, Total Reward = 49.0, Total Steps = 17005\n",
      "Episode 839, Total Reward = 35.0, Total Steps = 17040\n",
      "Episode 840, Total Reward = 45.0, Total Steps = 17085\n",
      "Episode 841, Total Reward = 34.0, Total Steps = 17119\n",
      "Episode 842, Total Reward = 17.0, Total Steps = 17136\n",
      "Episode 843, Total Reward = 53.0, Total Steps = 17189\n",
      "Episode 844, Total Reward = 27.0, Total Steps = 17216\n",
      "Episode 845, Total Reward = 39.0, Total Steps = 17255\n",
      "Episode 846, Total Reward = 36.0, Total Steps = 17291\n",
      "Episode 847, Total Reward = 11.0, Total Steps = 17302\n",
      "Episode 848, Total Reward = 62.0, Total Steps = 17364\n",
      "Episode 849, Total Reward = 24.0, Total Steps = 17388\n",
      "Episode 850, Total Reward = 46.0, Total Steps = 17434\n",
      "Episode 851, Total Reward = 56.0, Total Steps = 17490\n",
      "Episode 852, Total Reward = 12.0, Total Steps = 17502\n",
      "Episode 853, Total Reward = 100.0, Total Steps = 17602\n",
      "Episode 854, Total Reward = 65.0, Total Steps = 17667\n",
      "Episode 855, Total Reward = 9.0, Total Steps = 17676\n",
      "Episode 856, Total Reward = 24.0, Total Steps = 17700\n",
      "Episode 857, Total Reward = 38.0, Total Steps = 17738\n",
      "Episode 858, Total Reward = 41.0, Total Steps = 17779\n",
      "Episode 859, Total Reward = 15.0, Total Steps = 17794\n",
      "Episode 860, Total Reward = 8.0, Total Steps = 17802\n",
      "Episode 861, Total Reward = 13.0, Total Steps = 17815\n",
      "Episode 862, Total Reward = 41.0, Total Steps = 17856\n",
      "Episode 863, Total Reward = 37.0, Total Steps = 17893\n",
      "Episode 864, Total Reward = 74.0, Total Steps = 17967\n",
      "Episode 865, Total Reward = 28.0, Total Steps = 17995\n",
      "Episode 866, Total Reward = 57.0, Total Steps = 18052\n",
      "Episode 867, Total Reward = 10.0, Total Steps = 18062\n",
      "Episode 868, Total Reward = 13.0, Total Steps = 18075\n",
      "Episode 869, Total Reward = 16.0, Total Steps = 18091\n",
      "Episode 870, Total Reward = 18.0, Total Steps = 18109\n",
      "Episode 871, Total Reward = 48.0, Total Steps = 18157\n",
      "Episode 872, Total Reward = 57.0, Total Steps = 18214\n",
      "Episode 873, Total Reward = 32.0, Total Steps = 18246\n",
      "Episode 874, Total Reward = 11.0, Total Steps = 18257\n",
      "Episode 875, Total Reward = 13.0, Total Steps = 18270\n",
      "Episode 876, Total Reward = 64.0, Total Steps = 18334\n",
      "Episode 877, Total Reward = 19.0, Total Steps = 18353\n",
      "Episode 878, Total Reward = 45.0, Total Steps = 18398\n",
      "Episode 879, Total Reward = 52.0, Total Steps = 18450\n",
      "Episode 880, Total Reward = 28.0, Total Steps = 18478\n",
      "Episode 881, Total Reward = 16.0, Total Steps = 18494\n",
      "Episode 882, Total Reward = 29.0, Total Steps = 18523\n",
      "Episode 883, Total Reward = 18.0, Total Steps = 18541\n",
      "Episode 884, Total Reward = 10.0, Total Steps = 18551\n",
      "Episode 885, Total Reward = 9.0, Total Steps = 18560\n",
      "Episode 886, Total Reward = 66.0, Total Steps = 18626\n",
      "Episode 887, Total Reward = 71.0, Total Steps = 18697\n",
      "Episode 888, Total Reward = 36.0, Total Steps = 18733\n",
      "Episode 889, Total Reward = 46.0, Total Steps = 18779\n",
      "Episode 890, Total Reward = 44.0, Total Steps = 18823\n",
      "Episode 891, Total Reward = 80.0, Total Steps = 18903\n",
      "Episode 892, Total Reward = 29.0, Total Steps = 18932\n",
      "Episode 893, Total Reward = 64.0, Total Steps = 18996\n",
      "Episode 894, Total Reward = 51.0, Total Steps = 19047\n",
      "Episode 895, Total Reward = 50.0, Total Steps = 19097\n",
      "Episode 896, Total Reward = 5.0, Total Steps = 19102\n",
      "Episode 897, Total Reward = 23.0, Total Steps = 19125\n",
      "Episode 898, Total Reward = 76.0, Total Steps = 19201\n",
      "Episode 899, Total Reward = 23.0, Total Steps = 19224\n",
      "Episode 900, Total Reward = 23.0, Total Steps = 19247\n",
      "Episode 901, Total Reward = 25.0, Total Steps = 19272\n",
      "Episode 902, Total Reward = 40.0, Total Steps = 19312\n",
      "Episode 903, Total Reward = 85.0, Total Steps = 19397\n",
      "Episode 904, Total Reward = 10.0, Total Steps = 19407\n",
      "Episode 905, Total Reward = 28.0, Total Steps = 19435\n",
      "Episode 906, Total Reward = 16.0, Total Steps = 19451\n",
      "Episode 907, Total Reward = 33.0, Total Steps = 19484\n",
      "Episode 908, Total Reward = 9.0, Total Steps = 19493\n",
      "Episode 909, Total Reward = 84.0, Total Steps = 19577\n",
      "Episode 910, Total Reward = 54.0, Total Steps = 19631\n",
      "Episode 911, Total Reward = 23.0, Total Steps = 19654\n",
      "Episode 912, Total Reward = 40.0, Total Steps = 19694\n",
      "Episode 913, Total Reward = 28.0, Total Steps = 19722\n",
      "Episode 914, Total Reward = 28.0, Total Steps = 19750\n",
      "Episode 915, Total Reward = 22.0, Total Steps = 19772\n",
      "Episode 916, Total Reward = 6.0, Total Steps = 19778\n",
      "Episode 917, Total Reward = 15.0, Total Steps = 19793\n",
      "Episode 918, Total Reward = 51.0, Total Steps = 19844\n",
      "Episode 919, Total Reward = 21.0, Total Steps = 19865\n",
      "Episode 920, Total Reward = 10.0, Total Steps = 19875\n",
      "Episode 921, Total Reward = 31.0, Total Steps = 19906\n",
      "Episode 922, Total Reward = 36.0, Total Steps = 19942\n",
      "Episode 923, Total Reward = 29.0, Total Steps = 19971\n",
      "Episode 924, Total Reward = 58.0, Total Steps = 20029\n",
      "Episode 925, Total Reward = 12.0, Total Steps = 20041\n",
      "Episode 926, Total Reward = 60.0, Total Steps = 20101\n",
      "Episode 927, Total Reward = 25.0, Total Steps = 20126\n",
      "Episode 928, Total Reward = 32.0, Total Steps = 20158\n",
      "Episode 929, Total Reward = 11.0, Total Steps = 20169\n",
      "Episode 930, Total Reward = 64.0, Total Steps = 20233\n",
      "Episode 931, Total Reward = 24.0, Total Steps = 20257\n",
      "Episode 932, Total Reward = 42.0, Total Steps = 20299\n",
      "Episode 933, Total Reward = 32.0, Total Steps = 20331\n",
      "Episode 934, Total Reward = 16.0, Total Steps = 20347\n",
      "Episode 935, Total Reward = 89.0, Total Steps = 20436\n",
      "Episode 936, Total Reward = 36.0, Total Steps = 20472\n",
      "Episode 937, Total Reward = 20.0, Total Steps = 20492\n",
      "Episode 938, Total Reward = 47.0, Total Steps = 20539\n",
      "Episode 939, Total Reward = 36.0, Total Steps = 20575\n",
      "Episode 940, Total Reward = 47.0, Total Steps = 20622\n",
      "Episode 941, Total Reward = 32.0, Total Steps = 20654\n",
      "Episode 942, Total Reward = 99.0, Total Steps = 20753\n",
      "Episode 943, Total Reward = 25.0, Total Steps = 20778\n",
      "Episode 944, Total Reward = 25.0, Total Steps = 20803\n",
      "Episode 945, Total Reward = 41.0, Total Steps = 20844\n",
      "Episode 946, Total Reward = 35.0, Total Steps = 20879\n",
      "Episode 947, Total Reward = 31.0, Total Steps = 20910\n",
      "Episode 948, Total Reward = 42.0, Total Steps = 20952\n",
      "Episode 949, Total Reward = 33.0, Total Steps = 20985\n",
      "Episode 950, Total Reward = 71.0, Total Steps = 21056\n",
      "Episode 951, Total Reward = 49.0, Total Steps = 21105\n",
      "Episode 952, Total Reward = 35.0, Total Steps = 21140\n",
      "Episode 953, Total Reward = 80.0, Total Steps = 21220\n",
      "Episode 954, Total Reward = 30.0, Total Steps = 21250\n",
      "Episode 955, Total Reward = 23.0, Total Steps = 21273\n",
      "Episode 956, Total Reward = 40.0, Total Steps = 21313\n",
      "Episode 957, Total Reward = 30.0, Total Steps = 21343\n",
      "Episode 958, Total Reward = 97.0, Total Steps = 21440\n",
      "Episode 959, Total Reward = 61.0, Total Steps = 21501\n",
      "Episode 960, Total Reward = 50.0, Total Steps = 21551\n",
      "Episode 961, Total Reward = 7.0, Total Steps = 21558\n",
      "Episode 962, Total Reward = 17.0, Total Steps = 21575\n",
      "Episode 963, Total Reward = 10.0, Total Steps = 21585\n",
      "Episode 964, Total Reward = 20.0, Total Steps = 21605\n",
      "Episode 965, Total Reward = 31.0, Total Steps = 21636\n",
      "Episode 966, Total Reward = 39.0, Total Steps = 21675\n",
      "Episode 967, Total Reward = 48.0, Total Steps = 21723\n",
      "Episode 968, Total Reward = 13.0, Total Steps = 21736\n",
      "Episode 969, Total Reward = 16.0, Total Steps = 21752\n",
      "Episode 970, Total Reward = 79.0, Total Steps = 21831\n",
      "Episode 971, Total Reward = 14.0, Total Steps = 21845\n",
      "Episode 972, Total Reward = 29.0, Total Steps = 21874\n",
      "Episode 973, Total Reward = 31.0, Total Steps = 21905\n",
      "Episode 974, Total Reward = 16.0, Total Steps = 21921\n",
      "Episode 975, Total Reward = 52.0, Total Steps = 21973\n",
      "Episode 976, Total Reward = 17.0, Total Steps = 21990\n",
      "Episode 977, Total Reward = 38.0, Total Steps = 22028\n",
      "Episode 978, Total Reward = 41.0, Total Steps = 22069\n",
      "Episode 979, Total Reward = 21.0, Total Steps = 22090\n",
      "Episode 980, Total Reward = 90.0, Total Steps = 22180\n",
      "Episode 981, Total Reward = 14.0, Total Steps = 22194\n",
      "Episode 982, Total Reward = 92.0, Total Steps = 22286\n",
      "Episode 983, Total Reward = 45.0, Total Steps = 22331\n",
      "Episode 984, Total Reward = 60.0, Total Steps = 22391\n",
      "Episode 985, Total Reward = 7.0, Total Steps = 22398\n",
      "Episode 986, Total Reward = 63.0, Total Steps = 22461\n",
      "Episode 987, Total Reward = 9.0, Total Steps = 22470\n",
      "Episode 988, Total Reward = 42.0, Total Steps = 22512\n",
      "Episode 989, Total Reward = 8.0, Total Steps = 22520\n",
      "Episode 990, Total Reward = 38.0, Total Steps = 22558\n",
      "Episode 991, Total Reward = 134.0, Total Steps = 22692\n",
      "Episode 992, Total Reward = 26.0, Total Steps = 22718\n",
      "Episode 993, Total Reward = 25.0, Total Steps = 22743\n",
      "Episode 994, Total Reward = 72.0, Total Steps = 22815\n",
      "Episode 995, Total Reward = 17.0, Total Steps = 22832\n",
      "Episode 996, Total Reward = 59.0, Total Steps = 22891\n",
      "Episode 997, Total Reward = 15.0, Total Steps = 22906\n",
      "Episode 998, Total Reward = 15.0, Total Steps = 22921\n",
      "Episode 999, Total Reward = 24.0, Total Steps = 22945\n",
      "Episode 1000, Total Reward = 34.0, Total Steps = 22979\n",
      "Episode 1001, Total Reward = 64.0, Total Steps = 23043\n",
      "Episode 1002, Total Reward = 61.0, Total Steps = 23104\n",
      "Episode 1003, Total Reward = 25.0, Total Steps = 23129\n",
      "Episode 1004, Total Reward = 55.0, Total Steps = 23184\n",
      "Episode 1005, Total Reward = 22.0, Total Steps = 23206\n",
      "Episode 1006, Total Reward = 23.0, Total Steps = 23229\n",
      "Episode 1007, Total Reward = 16.0, Total Steps = 23245\n",
      "Episode 1008, Total Reward = 13.0, Total Steps = 23258\n",
      "Episode 1009, Total Reward = 124.0, Total Steps = 23382\n",
      "Episode 1010, Total Reward = 39.0, Total Steps = 23421\n",
      "Episode 1011, Total Reward = 51.0, Total Steps = 23472\n",
      "Episode 1012, Total Reward = 22.0, Total Steps = 23494\n",
      "Episode 1013, Total Reward = 25.0, Total Steps = 23519\n",
      "Episode 1014, Total Reward = 33.0, Total Steps = 23552\n",
      "Episode 1015, Total Reward = 46.0, Total Steps = 23598\n",
      "Episode 1016, Total Reward = 42.0, Total Steps = 23640\n",
      "Episode 1017, Total Reward = 99.0, Total Steps = 23739\n",
      "Episode 1018, Total Reward = 91.0, Total Steps = 23830\n",
      "Episode 1019, Total Reward = 65.0, Total Steps = 23895\n",
      "Episode 1020, Total Reward = 10.0, Total Steps = 23905\n",
      "Episode 1021, Total Reward = 11.0, Total Steps = 23916\n",
      "Episode 1022, Total Reward = 24.0, Total Steps = 23940\n",
      "Episode 1023, Total Reward = 32.0, Total Steps = 23972\n",
      "Episode 1024, Total Reward = 37.0, Total Steps = 24009\n",
      "Episode 1025, Total Reward = 19.0, Total Steps = 24028\n",
      "Episode 1026, Total Reward = 63.0, Total Steps = 24091\n",
      "Episode 1027, Total Reward = 17.0, Total Steps = 24108\n",
      "Episode 1028, Total Reward = 83.0, Total Steps = 24191\n",
      "Episode 1029, Total Reward = 63.0, Total Steps = 24254\n",
      "Episode 1030, Total Reward = 85.0, Total Steps = 24339\n",
      "Episode 1031, Total Reward = 131.0, Total Steps = 24470\n",
      "Episode 1032, Total Reward = 74.0, Total Steps = 24544\n",
      "Episode 1033, Total Reward = 27.0, Total Steps = 24571\n",
      "Episode 1034, Total Reward = 7.0, Total Steps = 24578\n",
      "Episode 1035, Total Reward = 16.0, Total Steps = 24594\n",
      "Episode 1036, Total Reward = 21.0, Total Steps = 24615\n",
      "Episode 1037, Total Reward = 14.0, Total Steps = 24629\n",
      "Episode 1038, Total Reward = 64.0, Total Steps = 24693\n",
      "Episode 1039, Total Reward = 44.0, Total Steps = 24737\n",
      "Episode 1040, Total Reward = 59.0, Total Steps = 24796\n",
      "Episode 1041, Total Reward = 39.0, Total Steps = 24835\n",
      "Episode 1042, Total Reward = 52.0, Total Steps = 24887\n",
      "Episode 1043, Total Reward = 29.0, Total Steps = 24916\n",
      "Episode 1044, Total Reward = 34.0, Total Steps = 24950\n",
      "Episode 1045, Total Reward = 38.0, Total Steps = 24988\n",
      "Episode 1046, Total Reward = 12.0, Total Steps = 25000\n",
      "Episode 1047, Total Reward = 18.0, Total Steps = 25018\n",
      "Episode 1048, Total Reward = 51.0, Total Steps = 25069\n",
      "Episode 1049, Total Reward = 34.0, Total Steps = 25103\n",
      "Episode 1050, Total Reward = 52.0, Total Steps = 25155\n",
      "Episode 1051, Total Reward = 65.0, Total Steps = 25220\n",
      "Episode 1052, Total Reward = 21.0, Total Steps = 25241\n",
      "Episode 1053, Total Reward = 47.0, Total Steps = 25288\n",
      "Episode 1054, Total Reward = 65.0, Total Steps = 25353\n",
      "Episode 1055, Total Reward = 21.0, Total Steps = 25374\n",
      "Episode 1056, Total Reward = 31.0, Total Steps = 25405\n",
      "Episode 1057, Total Reward = 41.0, Total Steps = 25446\n",
      "Episode 1058, Total Reward = 15.0, Total Steps = 25461\n",
      "Episode 1059, Total Reward = 94.0, Total Steps = 25555\n",
      "Episode 1060, Total Reward = 22.0, Total Steps = 25577\n",
      "Episode 1061, Total Reward = 92.0, Total Steps = 25669\n",
      "Episode 1062, Total Reward = 50.0, Total Steps = 25719\n",
      "Episode 1063, Total Reward = 43.0, Total Steps = 25762\n",
      "Episode 1064, Total Reward = 81.0, Total Steps = 25843\n",
      "Episode 1065, Total Reward = 62.0, Total Steps = 25905\n",
      "Episode 1066, Total Reward = 69.0, Total Steps = 25974\n",
      "Episode 1067, Total Reward = 54.0, Total Steps = 26028\n",
      "Episode 1068, Total Reward = 22.0, Total Steps = 26050\n",
      "Episode 1069, Total Reward = 23.0, Total Steps = 26073\n",
      "Episode 1070, Total Reward = 32.0, Total Steps = 26105\n",
      "Episode 1071, Total Reward = 19.0, Total Steps = 26124\n",
      "Episode 1072, Total Reward = 55.0, Total Steps = 26179\n",
      "Episode 1073, Total Reward = 42.0, Total Steps = 26221\n",
      "Episode 1074, Total Reward = 19.0, Total Steps = 26240\n",
      "Episode 1075, Total Reward = 38.0, Total Steps = 26278\n",
      "Episode 1076, Total Reward = 67.0, Total Steps = 26345\n",
      "Episode 1077, Total Reward = 38.0, Total Steps = 26383\n",
      "Episode 1078, Total Reward = 66.0, Total Steps = 26449\n",
      "Episode 1079, Total Reward = 10.0, Total Steps = 26459\n",
      "Episode 1080, Total Reward = 36.0, Total Steps = 26495\n",
      "Episode 1081, Total Reward = 18.0, Total Steps = 26513\n",
      "Episode 1082, Total Reward = 73.0, Total Steps = 26586\n",
      "Episode 1083, Total Reward = 45.0, Total Steps = 26631\n",
      "Episode 1084, Total Reward = 94.0, Total Steps = 26725\n",
      "Episode 1085, Total Reward = 39.0, Total Steps = 26764\n",
      "Episode 1086, Total Reward = 18.0, Total Steps = 26782\n",
      "Episode 1087, Total Reward = 57.0, Total Steps = 26839\n",
      "Episode 1088, Total Reward = 191.0, Total Steps = 27030\n",
      "Episode 1089, Total Reward = 38.0, Total Steps = 27068\n",
      "Episode 1090, Total Reward = 29.0, Total Steps = 27097\n",
      "Episode 1091, Total Reward = 58.0, Total Steps = 27155\n",
      "Episode 1092, Total Reward = 18.0, Total Steps = 27173\n",
      "Episode 1093, Total Reward = 19.0, Total Steps = 27192\n",
      "Episode 1094, Total Reward = 35.0, Total Steps = 27227\n",
      "Episode 1095, Total Reward = 26.0, Total Steps = 27253\n",
      "Episode 1096, Total Reward = 49.0, Total Steps = 27302\n",
      "Episode 1097, Total Reward = 157.0, Total Steps = 27459\n",
      "Episode 1098, Total Reward = 80.0, Total Steps = 27539\n",
      "Episode 1099, Total Reward = 14.0, Total Steps = 27553\n",
      "Episode 1100, Total Reward = 7.0, Total Steps = 27560\n",
      "Episode 1101, Total Reward = 9.0, Total Steps = 27569\n",
      "Episode 1102, Total Reward = 9.0, Total Steps = 27578\n",
      "Episode 1103, Total Reward = 19.0, Total Steps = 27597\n",
      "Episode 1104, Total Reward = 9.0, Total Steps = 27606\n",
      "Episode 1105, Total Reward = 114.0, Total Steps = 27720\n",
      "Episode 1106, Total Reward = 35.0, Total Steps = 27755\n",
      "Episode 1107, Total Reward = 45.0, Total Steps = 27800\n",
      "Episode 1108, Total Reward = 113.0, Total Steps = 27913\n",
      "Episode 1109, Total Reward = 61.0, Total Steps = 27974\n",
      "Episode 1110, Total Reward = 81.0, Total Steps = 28055\n",
      "Episode 1111, Total Reward = 23.0, Total Steps = 28078\n",
      "Episode 1112, Total Reward = 99.0, Total Steps = 28177\n",
      "Episode 1113, Total Reward = 11.0, Total Steps = 28188\n",
      "Episode 1114, Total Reward = 149.0, Total Steps = 28337\n",
      "Episode 1115, Total Reward = 36.0, Total Steps = 28373\n",
      "Episode 1116, Total Reward = 11.0, Total Steps = 28384\n",
      "Episode 1117, Total Reward = 23.0, Total Steps = 28407\n",
      "Episode 1118, Total Reward = 17.0, Total Steps = 28424\n",
      "Episode 1119, Total Reward = 36.0, Total Steps = 28460\n",
      "Episode 1120, Total Reward = 10.0, Total Steps = 28470\n",
      "Episode 1121, Total Reward = 31.0, Total Steps = 28501\n",
      "Episode 1122, Total Reward = 54.0, Total Steps = 28555\n",
      "Episode 1123, Total Reward = 79.0, Total Steps = 28634\n",
      "Episode 1124, Total Reward = 44.0, Total Steps = 28678\n",
      "Episode 1125, Total Reward = 56.0, Total Steps = 28734\n",
      "Episode 1126, Total Reward = 62.0, Total Steps = 28796\n",
      "Episode 1127, Total Reward = 41.0, Total Steps = 28837\n",
      "Episode 1128, Total Reward = 37.0, Total Steps = 28874\n",
      "Episode 1129, Total Reward = 46.0, Total Steps = 28920\n",
      "Episode 1130, Total Reward = 13.0, Total Steps = 28933\n",
      "Episode 1131, Total Reward = 41.0, Total Steps = 28974\n",
      "Episode 1132, Total Reward = 79.0, Total Steps = 29053\n",
      "Episode 1133, Total Reward = 106.0, Total Steps = 29159\n",
      "Episode 1134, Total Reward = 32.0, Total Steps = 29191\n",
      "Episode 1135, Total Reward = 23.0, Total Steps = 29214\n",
      "Episode 1136, Total Reward = 31.0, Total Steps = 29245\n",
      "Episode 1137, Total Reward = 52.0, Total Steps = 29297\n",
      "Episode 1138, Total Reward = 74.0, Total Steps = 29371\n",
      "Episode 1139, Total Reward = 40.0, Total Steps = 29411\n",
      "Episode 1140, Total Reward = 15.0, Total Steps = 29426\n",
      "Episode 1141, Total Reward = 76.0, Total Steps = 29502\n",
      "Episode 1142, Total Reward = 77.0, Total Steps = 29579\n",
      "Episode 1143, Total Reward = 107.0, Total Steps = 29686\n",
      "Episode 1144, Total Reward = 34.0, Total Steps = 29720\n",
      "Episode 1145, Total Reward = 34.0, Total Steps = 29754\n",
      "Episode 1146, Total Reward = 16.0, Total Steps = 29770\n",
      "Episode 1147, Total Reward = 85.0, Total Steps = 29855\n",
      "Episode 1148, Total Reward = 12.0, Total Steps = 29867\n",
      "Episode 1149, Total Reward = 82.0, Total Steps = 29949\n",
      "Episode 1150, Total Reward = 64.0, Total Steps = 30013\n",
      "Episode 1151, Total Reward = 65.0, Total Steps = 30078\n",
      "Episode 1152, Total Reward = 28.0, Total Steps = 30106\n",
      "Episode 1153, Total Reward = 42.0, Total Steps = 30148\n",
      "Episode 1154, Total Reward = 43.0, Total Steps = 30191\n",
      "Episode 1155, Total Reward = 53.0, Total Steps = 30244\n",
      "Episode 1156, Total Reward = 39.0, Total Steps = 30283\n",
      "Episode 1157, Total Reward = 71.0, Total Steps = 30354\n",
      "Episode 1158, Total Reward = 88.0, Total Steps = 30442\n",
      "Episode 1159, Total Reward = 73.0, Total Steps = 30515\n",
      "Episode 1160, Total Reward = 83.0, Total Steps = 30598\n",
      "Episode 1161, Total Reward = 14.0, Total Steps = 30612\n",
      "Episode 1162, Total Reward = 17.0, Total Steps = 30629\n",
      "Episode 1163, Total Reward = 85.0, Total Steps = 30714\n",
      "Episode 1164, Total Reward = 33.0, Total Steps = 30747\n",
      "Episode 1165, Total Reward = 133.0, Total Steps = 30880\n",
      "Episode 1166, Total Reward = 41.0, Total Steps = 30921\n",
      "Episode 1167, Total Reward = 7.0, Total Steps = 30928\n",
      "Episode 1168, Total Reward = 48.0, Total Steps = 30976\n",
      "Episode 1169, Total Reward = 43.0, Total Steps = 31019\n",
      "Episode 1170, Total Reward = 10.0, Total Steps = 31029\n",
      "Episode 1171, Total Reward = 22.0, Total Steps = 31051\n",
      "Episode 1172, Total Reward = 17.0, Total Steps = 31068\n",
      "Episode 1173, Total Reward = 12.0, Total Steps = 31080\n",
      "Episode 1174, Total Reward = 13.0, Total Steps = 31093\n",
      "Episode 1175, Total Reward = 10.0, Total Steps = 31103\n",
      "Episode 1176, Total Reward = 92.0, Total Steps = 31195\n",
      "Episode 1177, Total Reward = 136.0, Total Steps = 31331\n",
      "Episode 1178, Total Reward = 61.0, Total Steps = 31392\n",
      "Episode 1179, Total Reward = 34.0, Total Steps = 31426\n",
      "Episode 1180, Total Reward = 43.0, Total Steps = 31469\n",
      "Episode 1181, Total Reward = 17.0, Total Steps = 31486\n",
      "Episode 1182, Total Reward = 74.0, Total Steps = 31560\n",
      "Episode 1183, Total Reward = 52.0, Total Steps = 31612\n",
      "Episode 1184, Total Reward = 10.0, Total Steps = 31622\n",
      "Episode 1185, Total Reward = 114.0, Total Steps = 31736\n",
      "Episode 1186, Total Reward = 33.0, Total Steps = 31769\n",
      "Episode 1187, Total Reward = 130.0, Total Steps = 31899\n",
      "Episode 1188, Total Reward = 59.0, Total Steps = 31958\n",
      "Episode 1189, Total Reward = 98.0, Total Steps = 32056\n",
      "Episode 1190, Total Reward = 30.0, Total Steps = 32086\n",
      "Episode 1191, Total Reward = 13.0, Total Steps = 32099\n",
      "Episode 1192, Total Reward = 108.0, Total Steps = 32207\n",
      "Episode 1193, Total Reward = 42.0, Total Steps = 32249\n",
      "Episode 1194, Total Reward = 43.0, Total Steps = 32292\n",
      "Episode 1195, Total Reward = 127.0, Total Steps = 32419\n",
      "Episode 1196, Total Reward = 13.0, Total Steps = 32432\n",
      "Episode 1197, Total Reward = 60.0, Total Steps = 32492\n",
      "Episode 1198, Total Reward = 28.0, Total Steps = 32520\n",
      "Episode 1199, Total Reward = 19.0, Total Steps = 32539\n",
      "Episode 1200, Total Reward = 43.0, Total Steps = 32582\n",
      "Episode 1201, Total Reward = 82.0, Total Steps = 32664\n",
      "Episode 1202, Total Reward = 72.0, Total Steps = 32736\n",
      "Episode 1203, Total Reward = 85.0, Total Steps = 32821\n",
      "Episode 1204, Total Reward = 92.0, Total Steps = 32913\n",
      "Episode 1205, Total Reward = 12.0, Total Steps = 32925\n",
      "Episode 1206, Total Reward = 47.0, Total Steps = 32972\n",
      "Episode 1207, Total Reward = 40.0, Total Steps = 33012\n",
      "Episode 1208, Total Reward = 7.0, Total Steps = 33019\n",
      "Episode 1209, Total Reward = 53.0, Total Steps = 33072\n",
      "Episode 1210, Total Reward = 37.0, Total Steps = 33109\n",
      "Episode 1211, Total Reward = 12.0, Total Steps = 33121\n",
      "Episode 1212, Total Reward = 50.0, Total Steps = 33171\n",
      "Episode 1213, Total Reward = 198.0, Total Steps = 33369\n",
      "Episode 1214, Total Reward = 42.0, Total Steps = 33411\n",
      "Episode 1215, Total Reward = 81.0, Total Steps = 33492\n",
      "Episode 1216, Total Reward = 43.0, Total Steps = 33535\n",
      "Episode 1217, Total Reward = 35.0, Total Steps = 33570\n",
      "Episode 1218, Total Reward = 70.0, Total Steps = 33640\n",
      "Episode 1219, Total Reward = 42.0, Total Steps = 33682\n",
      "Episode 1220, Total Reward = 12.0, Total Steps = 33694\n",
      "Episode 1221, Total Reward = 37.0, Total Steps = 33731\n",
      "Episode 1222, Total Reward = 186.0, Total Steps = 33917\n",
      "Episode 1223, Total Reward = 45.0, Total Steps = 33962\n",
      "Episode 1224, Total Reward = 9.0, Total Steps = 33971\n",
      "Episode 1225, Total Reward = 55.0, Total Steps = 34026\n",
      "Episode 1226, Total Reward = 16.0, Total Steps = 34042\n",
      "Episode 1227, Total Reward = 23.0, Total Steps = 34065\n",
      "Episode 1228, Total Reward = 46.0, Total Steps = 34111\n",
      "Episode 1229, Total Reward = 252.0, Total Steps = 34363\n",
      "Episode 1230, Total Reward = 16.0, Total Steps = 34379\n",
      "Episode 1231, Total Reward = 123.0, Total Steps = 34502\n",
      "Episode 1232, Total Reward = 35.0, Total Steps = 34537\n",
      "Episode 1233, Total Reward = 11.0, Total Steps = 34548\n",
      "Episode 1234, Total Reward = 38.0, Total Steps = 34586\n",
      "Episode 1235, Total Reward = 49.0, Total Steps = 34635\n",
      "Episode 1236, Total Reward = 40.0, Total Steps = 34675\n",
      "Episode 1237, Total Reward = 48.0, Total Steps = 34723\n",
      "Episode 1238, Total Reward = 10.0, Total Steps = 34733\n",
      "Episode 1239, Total Reward = 166.0, Total Steps = 34899\n",
      "Episode 1240, Total Reward = 12.0, Total Steps = 34911\n",
      "Episode 1241, Total Reward = 31.0, Total Steps = 34942\n",
      "Episode 1242, Total Reward = 75.0, Total Steps = 35017\n",
      "Episode 1243, Total Reward = 40.0, Total Steps = 35057\n",
      "Episode 1244, Total Reward = 16.0, Total Steps = 35073\n",
      "Episode 1245, Total Reward = 90.0, Total Steps = 35163\n",
      "Episode 1246, Total Reward = 63.0, Total Steps = 35226\n",
      "Episode 1247, Total Reward = 15.0, Total Steps = 35241\n",
      "Episode 1248, Total Reward = 11.0, Total Steps = 35252\n",
      "Episode 1249, Total Reward = 47.0, Total Steps = 35299\n",
      "Episode 1250, Total Reward = 17.0, Total Steps = 35316\n",
      "Episode 1251, Total Reward = 5.0, Total Steps = 35321\n",
      "Episode 1252, Total Reward = 190.0, Total Steps = 35511\n",
      "Episode 1253, Total Reward = 86.0, Total Steps = 35597\n",
      "Episode 1254, Total Reward = 64.0, Total Steps = 35661\n",
      "Episode 1255, Total Reward = 32.0, Total Steps = 35693\n",
      "Episode 1256, Total Reward = 36.0, Total Steps = 35729\n",
      "Episode 1257, Total Reward = 29.0, Total Steps = 35758\n",
      "Episode 1258, Total Reward = 51.0, Total Steps = 35809\n",
      "Episode 1259, Total Reward = 62.0, Total Steps = 35871\n",
      "Episode 1260, Total Reward = 28.0, Total Steps = 35899\n",
      "Episode 1261, Total Reward = 10.0, Total Steps = 35909\n",
      "Episode 1262, Total Reward = 40.0, Total Steps = 35949\n",
      "Episode 1263, Total Reward = 51.0, Total Steps = 36000\n",
      "Episode 1264, Total Reward = 233.0, Total Steps = 36233\n",
      "Episode 1265, Total Reward = 31.0, Total Steps = 36264\n",
      "Episode 1266, Total Reward = 125.0, Total Steps = 36389\n",
      "Episode 1267, Total Reward = 22.0, Total Steps = 36411\n",
      "Episode 1268, Total Reward = 213.0, Total Steps = 36624\n",
      "Episode 1269, Total Reward = 123.0, Total Steps = 36747\n",
      "Episode 1270, Total Reward = 45.0, Total Steps = 36792\n",
      "Episode 1271, Total Reward = 11.0, Total Steps = 36803\n",
      "Episode 1272, Total Reward = 24.0, Total Steps = 36827\n",
      "Episode 1273, Total Reward = 13.0, Total Steps = 36840\n",
      "Episode 1274, Total Reward = 16.0, Total Steps = 36856\n",
      "Episode 1275, Total Reward = 38.0, Total Steps = 36894\n",
      "Episode 1276, Total Reward = 52.0, Total Steps = 36946\n",
      "Episode 1277, Total Reward = 171.0, Total Steps = 37117\n",
      "Episode 1278, Total Reward = 103.0, Total Steps = 37220\n",
      "Episode 1279, Total Reward = 11.0, Total Steps = 37231\n",
      "Episode 1280, Total Reward = 13.0, Total Steps = 37244\n",
      "Episode 1281, Total Reward = 14.0, Total Steps = 37258\n",
      "Episode 1282, Total Reward = 74.0, Total Steps = 37332\n",
      "Episode 1283, Total Reward = 37.0, Total Steps = 37369\n",
      "Episode 1284, Total Reward = 98.0, Total Steps = 37467\n",
      "Episode 1285, Total Reward = 138.0, Total Steps = 37605\n",
      "Episode 1286, Total Reward = 152.0, Total Steps = 37757\n",
      "Episode 1287, Total Reward = 55.0, Total Steps = 37812\n",
      "Episode 1288, Total Reward = 57.0, Total Steps = 37869\n",
      "Episode 1289, Total Reward = 95.0, Total Steps = 37964\n",
      "Episode 1290, Total Reward = 86.0, Total Steps = 38050\n",
      "Episode 1291, Total Reward = 11.0, Total Steps = 38061\n",
      "Episode 1292, Total Reward = 63.0, Total Steps = 38124\n",
      "Episode 1293, Total Reward = 96.0, Total Steps = 38220\n",
      "Episode 1294, Total Reward = 59.0, Total Steps = 38279\n",
      "Episode 1295, Total Reward = 25.0, Total Steps = 38304\n",
      "Episode 1296, Total Reward = 113.0, Total Steps = 38417\n",
      "Episode 1297, Total Reward = 13.0, Total Steps = 38430\n",
      "Episode 1298, Total Reward = 54.0, Total Steps = 38484\n",
      "Episode 1299, Total Reward = 27.0, Total Steps = 38511\n",
      "Episode 1300, Total Reward = 156.0, Total Steps = 38667\n",
      "Episode 1301, Total Reward = 79.0, Total Steps = 38746\n",
      "Episode 1302, Total Reward = 36.0, Total Steps = 38782\n",
      "Episode 1303, Total Reward = 52.0, Total Steps = 38834\n",
      "Episode 1304, Total Reward = 86.0, Total Steps = 38920\n",
      "Episode 1305, Total Reward = 10.0, Total Steps = 38930\n",
      "Episode 1306, Total Reward = 11.0, Total Steps = 38941\n",
      "Episode 1307, Total Reward = 21.0, Total Steps = 38962\n",
      "Episode 1308, Total Reward = 202.0, Total Steps = 39164\n",
      "Episode 1309, Total Reward = 137.0, Total Steps = 39301\n",
      "Episode 1310, Total Reward = 51.0, Total Steps = 39352\n",
      "Episode 1311, Total Reward = 69.0, Total Steps = 39421\n",
      "Episode 1312, Total Reward = 32.0, Total Steps = 39453\n",
      "Episode 1313, Total Reward = 70.0, Total Steps = 39523\n",
      "Episode 1314, Total Reward = 46.0, Total Steps = 39569\n",
      "Episode 1315, Total Reward = 234.0, Total Steps = 39803\n",
      "Episode 1316, Total Reward = 17.0, Total Steps = 39820\n",
      "Episode 1317, Total Reward = 55.0, Total Steps = 39875\n",
      "Episode 1318, Total Reward = 27.0, Total Steps = 39902\n",
      "Episode 1319, Total Reward = 113.0, Total Steps = 40015\n",
      "Episode 1320, Total Reward = 61.0, Total Steps = 40076\n",
      "Episode 1321, Total Reward = 155.0, Total Steps = 40231\n",
      "Episode 1322, Total Reward = 41.0, Total Steps = 40272\n",
      "Episode 1323, Total Reward = 60.0, Total Steps = 40332\n",
      "Episode 1324, Total Reward = 15.0, Total Steps = 40347\n",
      "Episode 1325, Total Reward = 55.0, Total Steps = 40402\n",
      "Episode 1326, Total Reward = 28.0, Total Steps = 40430\n",
      "Episode 1327, Total Reward = 50.0, Total Steps = 40480\n",
      "Episode 1328, Total Reward = 116.0, Total Steps = 40596\n",
      "Episode 1329, Total Reward = 59.0, Total Steps = 40655\n",
      "Episode 1330, Total Reward = 146.0, Total Steps = 40801\n",
      "Episode 1331, Total Reward = 7.0, Total Steps = 40808\n",
      "Episode 1332, Total Reward = 14.0, Total Steps = 40822\n",
      "Episode 1333, Total Reward = 36.0, Total Steps = 40858\n",
      "Episode 1334, Total Reward = 78.0, Total Steps = 40936\n",
      "Episode 1335, Total Reward = 172.0, Total Steps = 41108\n",
      "Episode 1336, Total Reward = 104.0, Total Steps = 41212\n",
      "Episode 1337, Total Reward = 60.0, Total Steps = 41272\n",
      "Episode 1338, Total Reward = 319.0, Total Steps = 41591\n",
      "Episode 1339, Total Reward = 58.0, Total Steps = 41649\n",
      "Episode 1340, Total Reward = 17.0, Total Steps = 41666\n",
      "Episode 1341, Total Reward = 42.0, Total Steps = 41708\n",
      "Episode 1342, Total Reward = 24.0, Total Steps = 41732\n",
      "Episode 1343, Total Reward = 9.0, Total Steps = 41741\n",
      "Episode 1344, Total Reward = 41.0, Total Steps = 41782\n",
      "Episode 1345, Total Reward = 26.0, Total Steps = 41808\n",
      "Episode 1346, Total Reward = 11.0, Total Steps = 41819\n",
      "Episode 1347, Total Reward = 38.0, Total Steps = 41857\n",
      "Episode 1348, Total Reward = 7.0, Total Steps = 41864\n",
      "Episode 1349, Total Reward = 29.0, Total Steps = 41893\n",
      "Episode 1350, Total Reward = 165.0, Total Steps = 42058\n",
      "Episode 1351, Total Reward = 88.0, Total Steps = 42146\n",
      "Episode 1352, Total Reward = 44.0, Total Steps = 42190\n",
      "Episode 1353, Total Reward = 84.0, Total Steps = 42274\n",
      "Episode 1354, Total Reward = 43.0, Total Steps = 42317\n",
      "Episode 1355, Total Reward = 36.0, Total Steps = 42353\n",
      "Episode 1356, Total Reward = 202.0, Total Steps = 42555\n",
      "Episode 1357, Total Reward = 42.0, Total Steps = 42597\n",
      "Episode 1358, Total Reward = 92.0, Total Steps = 42689\n",
      "Episode 1359, Total Reward = 144.0, Total Steps = 42833\n",
      "Episode 1360, Total Reward = 49.0, Total Steps = 42882\n",
      "Episode 1361, Total Reward = 10.0, Total Steps = 42892\n",
      "Episode 1362, Total Reward = 39.0, Total Steps = 42931\n",
      "Episode 1363, Total Reward = 88.0, Total Steps = 43019\n",
      "Episode 1364, Total Reward = 17.0, Total Steps = 43036\n",
      "Episode 1365, Total Reward = 72.0, Total Steps = 43108\n",
      "Episode 1366, Total Reward = 112.0, Total Steps = 43220\n",
      "Episode 1367, Total Reward = 64.0, Total Steps = 43284\n",
      "Episode 1368, Total Reward = 122.0, Total Steps = 43406\n",
      "Episode 1369, Total Reward = 46.0, Total Steps = 43452\n",
      "Episode 1370, Total Reward = 115.0, Total Steps = 43567\n",
      "Episode 1371, Total Reward = 94.0, Total Steps = 43661\n",
      "Episode 1372, Total Reward = 24.0, Total Steps = 43685\n",
      "Episode 1373, Total Reward = 155.0, Total Steps = 43840\n",
      "Episode 1374, Total Reward = 44.0, Total Steps = 43884\n",
      "Episode 1375, Total Reward = 237.0, Total Steps = 44121\n",
      "Episode 1376, Total Reward = 53.0, Total Steps = 44174\n",
      "Episode 1377, Total Reward = 57.0, Total Steps = 44231\n",
      "Episode 1378, Total Reward = 32.0, Total Steps = 44263\n",
      "Episode 1379, Total Reward = 35.0, Total Steps = 44298\n",
      "Episode 1380, Total Reward = 43.0, Total Steps = 44341\n",
      "Episode 1381, Total Reward = 51.0, Total Steps = 44392\n",
      "Episode 1382, Total Reward = 32.0, Total Steps = 44424\n",
      "Episode 1383, Total Reward = 65.0, Total Steps = 44489\n",
      "Episode 1384, Total Reward = 80.0, Total Steps = 44569\n",
      "Episode 1385, Total Reward = 21.0, Total Steps = 44590\n",
      "Episode 1386, Total Reward = 137.0, Total Steps = 44727\n",
      "Episode 1387, Total Reward = 200.0, Total Steps = 44927\n",
      "Episode 1388, Total Reward = 48.0, Total Steps = 44975\n",
      "Episode 1389, Total Reward = 96.0, Total Steps = 45071\n",
      "Episode 1390, Total Reward = 79.0, Total Steps = 45150\n",
      "Episode 1391, Total Reward = 181.0, Total Steps = 45331\n",
      "Episode 1392, Total Reward = 41.0, Total Steps = 45372\n",
      "Episode 1393, Total Reward = 42.0, Total Steps = 45414\n",
      "Episode 1394, Total Reward = 9.0, Total Steps = 45423\n",
      "Episode 1395, Total Reward = 90.0, Total Steps = 45513\n",
      "Episode 1396, Total Reward = 79.0, Total Steps = 45592\n",
      "Episode 1397, Total Reward = 10.0, Total Steps = 45602\n",
      "Episode 1398, Total Reward = 204.0, Total Steps = 45806\n",
      "Episode 1399, Total Reward = 69.0, Total Steps = 45875\n",
      "Episode 1400, Total Reward = 93.0, Total Steps = 45968\n",
      "Episode 1401, Total Reward = 38.0, Total Steps = 46006\n",
      "Episode 1402, Total Reward = 10.0, Total Steps = 46016\n",
      "Episode 1403, Total Reward = 109.0, Total Steps = 46125\n",
      "Episode 1404, Total Reward = 8.0, Total Steps = 46133\n",
      "Episode 1405, Total Reward = 46.0, Total Steps = 46179\n",
      "Episode 1406, Total Reward = 98.0, Total Steps = 46277\n",
      "Episode 1407, Total Reward = 223.0, Total Steps = 46500\n",
      "Episode 1408, Total Reward = 153.0, Total Steps = 46653\n",
      "Episode 1409, Total Reward = 19.0, Total Steps = 46672\n",
      "Episode 1410, Total Reward = 55.0, Total Steps = 46727\n",
      "Episode 1411, Total Reward = 144.0, Total Steps = 46871\n",
      "Episode 1412, Total Reward = 32.0, Total Steps = 46903\n",
      "Episode 1413, Total Reward = 69.0, Total Steps = 46972\n",
      "Episode 1414, Total Reward = 93.0, Total Steps = 47065\n",
      "Episode 1415, Total Reward = 250.0, Total Steps = 47315\n",
      "Episode 1416, Total Reward = 18.0, Total Steps = 47333\n",
      "Episode 1417, Total Reward = 121.0, Total Steps = 47454\n",
      "Episode 1418, Total Reward = 38.0, Total Steps = 47492\n",
      "Episode 1419, Total Reward = 40.0, Total Steps = 47532\n",
      "Episode 1420, Total Reward = 126.0, Total Steps = 47658\n",
      "Episode 1421, Total Reward = 73.0, Total Steps = 47731\n",
      "Episode 1422, Total Reward = 47.0, Total Steps = 47778\n",
      "Episode 1423, Total Reward = 126.0, Total Steps = 47904\n",
      "Episode 1424, Total Reward = 94.0, Total Steps = 47998\n",
      "Episode 1425, Total Reward = 363.0, Total Steps = 48361\n",
      "Episode 1426, Total Reward = 137.0, Total Steps = 48498\n",
      "Episode 1427, Total Reward = 46.0, Total Steps = 48544\n",
      "Episode 1428, Total Reward = 30.0, Total Steps = 48574\n",
      "Episode 1429, Total Reward = 89.0, Total Steps = 48663\n",
      "Episode 1430, Total Reward = 95.0, Total Steps = 48758\n",
      "Episode 1431, Total Reward = 83.0, Total Steps = 48841\n",
      "Episode 1432, Total Reward = 130.0, Total Steps = 48971\n",
      "Episode 1433, Total Reward = 86.0, Total Steps = 49057\n",
      "Episode 1434, Total Reward = 9.0, Total Steps = 49066\n",
      "Episode 1435, Total Reward = 122.0, Total Steps = 49188\n",
      "Episode 1436, Total Reward = 61.0, Total Steps = 49249\n",
      "Episode 1437, Total Reward = 57.0, Total Steps = 49306\n",
      "Episode 1438, Total Reward = 50.0, Total Steps = 49356\n",
      "Episode 1439, Total Reward = 45.0, Total Steps = 49401\n",
      "Episode 1440, Total Reward = 53.0, Total Steps = 49454\n",
      "Episode 1441, Total Reward = 118.0, Total Steps = 49572\n",
      "Episode 1442, Total Reward = 41.0, Total Steps = 49613\n",
      "Episode 1443, Total Reward = 64.0, Total Steps = 49677\n",
      "Episode 1444, Total Reward = 74.0, Total Steps = 49751\n",
      "Episode 1445, Total Reward = 376.0, Total Steps = 50127\n",
      "Episode 1446, Total Reward = 64.0, Total Steps = 50191\n",
      "Episode 1447, Total Reward = 43.0, Total Steps = 50234\n",
      "Episode 1448, Total Reward = 134.0, Total Steps = 50368\n",
      "Episode 1449, Total Reward = 50.0, Total Steps = 50418\n",
      "Episode 1450, Total Reward = 36.0, Total Steps = 50454\n",
      "Episode 1451, Total Reward = 55.0, Total Steps = 50509\n",
      "Episode 1452, Total Reward = 52.0, Total Steps = 50561\n",
      "Episode 1453, Total Reward = 37.0, Total Steps = 50598\n",
      "Episode 1454, Total Reward = 221.0, Total Steps = 50819\n",
      "Episode 1455, Total Reward = 71.0, Total Steps = 50890\n",
      "Episode 1456, Total Reward = 65.0, Total Steps = 50955\n",
      "Episode 1457, Total Reward = 155.0, Total Steps = 51110\n",
      "Episode 1458, Total Reward = 94.0, Total Steps = 51204\n",
      "Episode 1459, Total Reward = 61.0, Total Steps = 51265\n",
      "Episode 1460, Total Reward = 51.0, Total Steps = 51316\n",
      "Episode 1461, Total Reward = 59.0, Total Steps = 51375\n",
      "Episode 1462, Total Reward = 87.0, Total Steps = 51462\n",
      "Episode 1463, Total Reward = 151.0, Total Steps = 51613\n",
      "Episode 1464, Total Reward = 89.0, Total Steps = 51702\n",
      "Episode 1465, Total Reward = 55.0, Total Steps = 51757\n",
      "Episode 1466, Total Reward = 42.0, Total Steps = 51799\n",
      "Episode 1467, Total Reward = 313.0, Total Steps = 52112\n",
      "Episode 1468, Total Reward = 48.0, Total Steps = 52160\n",
      "Episode 1469, Total Reward = 40.0, Total Steps = 52200\n",
      "Episode 1470, Total Reward = 75.0, Total Steps = 52275\n",
      "Episode 1471, Total Reward = 155.0, Total Steps = 52430\n",
      "Episode 1472, Total Reward = 151.0, Total Steps = 52581\n",
      "Episode 1473, Total Reward = 69.0, Total Steps = 52650\n",
      "Episode 1474, Total Reward = 30.0, Total Steps = 52680\n",
      "Episode 1475, Total Reward = 40.0, Total Steps = 52720\n",
      "Episode 1476, Total Reward = 67.0, Total Steps = 52787\n",
      "Episode 1477, Total Reward = 63.0, Total Steps = 52850\n",
      "Episode 1478, Total Reward = 52.0, Total Steps = 52902\n",
      "Episode 1479, Total Reward = 208.0, Total Steps = 53110\n",
      "Episode 1480, Total Reward = 61.0, Total Steps = 53171\n",
      "Episode 1481, Total Reward = 89.0, Total Steps = 53260\n",
      "Episode 1482, Total Reward = 76.0, Total Steps = 53336\n",
      "Episode 1483, Total Reward = 50.0, Total Steps = 53386\n",
      "Episode 1484, Total Reward = 26.0, Total Steps = 53412\n",
      "Episode 1485, Total Reward = 449.0, Total Steps = 53861\n",
      "Episode 1486, Total Reward = 57.0, Total Steps = 53918\n",
      "Episode 1487, Total Reward = 171.0, Total Steps = 54089\n",
      "Episode 1488, Total Reward = 49.0, Total Steps = 54138\n",
      "Episode 1489, Total Reward = 45.0, Total Steps = 54183\n",
      "Episode 1490, Total Reward = 121.0, Total Steps = 54304\n",
      "Episode 1491, Total Reward = 131.0, Total Steps = 54435\n",
      "Episode 1492, Total Reward = 97.0, Total Steps = 54532\n",
      "Episode 1493, Total Reward = 201.0, Total Steps = 54733\n",
      "Episode 1494, Total Reward = 11.0, Total Steps = 54744\n",
      "Episode 1495, Total Reward = 10.0, Total Steps = 54754\n",
      "Episode 1496, Total Reward = 76.0, Total Steps = 54830\n",
      "Episode 1497, Total Reward = 8.0, Total Steps = 54838\n",
      "Episode 1498, Total Reward = 8.0, Total Steps = 54846\n",
      "Episode 1499, Total Reward = 224.0, Total Steps = 55070\n",
      "Episode 1500, Total Reward = 231.0, Total Steps = 55301\n",
      "Episode 1501, Total Reward = 81.0, Total Steps = 55382\n",
      "Episode 1502, Total Reward = 82.0, Total Steps = 55464\n",
      "Episode 1503, Total Reward = 84.0, Total Steps = 55548\n",
      "Episode 1504, Total Reward = 228.0, Total Steps = 55776\n",
      "Episode 1505, Total Reward = 52.0, Total Steps = 55828\n",
      "Episode 1506, Total Reward = 155.0, Total Steps = 55983\n",
      "Episode 1507, Total Reward = 11.0, Total Steps = 55994\n",
      "Episode 1508, Total Reward = 129.0, Total Steps = 56123\n",
      "Episode 1509, Total Reward = 27.0, Total Steps = 56150\n",
      "Episode 1510, Total Reward = 196.0, Total Steps = 56346\n",
      "Episode 1511, Total Reward = 13.0, Total Steps = 56359\n",
      "Episode 1512, Total Reward = 83.0, Total Steps = 56442\n",
      "Episode 1513, Total Reward = 129.0, Total Steps = 56571\n",
      "Episode 1514, Total Reward = 46.0, Total Steps = 56617\n",
      "Episode 1515, Total Reward = 57.0, Total Steps = 56674\n",
      "Episode 1516, Total Reward = 57.0, Total Steps = 56731\n",
      "Episode 1517, Total Reward = 312.0, Total Steps = 57043\n",
      "Episode 1518, Total Reward = 68.0, Total Steps = 57111\n",
      "Episode 1519, Total Reward = 58.0, Total Steps = 57169\n",
      "Episode 1520, Total Reward = 19.0, Total Steps = 57188\n",
      "Episode 1521, Total Reward = 349.0, Total Steps = 57537\n",
      "Episode 1522, Total Reward = 160.0, Total Steps = 57697\n",
      "Episode 1523, Total Reward = 74.0, Total Steps = 57771\n",
      "Episode 1524, Total Reward = 18.0, Total Steps = 57789\n",
      "Episode 1525, Total Reward = 11.0, Total Steps = 57800\n",
      "Episode 1526, Total Reward = 18.0, Total Steps = 57818\n",
      "Episode 1527, Total Reward = 456.0, Total Steps = 58274\n",
      "Episode 1528, Total Reward = 22.0, Total Steps = 58296\n",
      "Episode 1529, Total Reward = 50.0, Total Steps = 58346\n",
      "Episode 1530, Total Reward = 56.0, Total Steps = 58402\n",
      "Episode 1531, Total Reward = 23.0, Total Steps = 58425\n",
      "Episode 1532, Total Reward = 22.0, Total Steps = 58447\n",
      "Episode 1533, Total Reward = 230.0, Total Steps = 58677\n",
      "Episode 1534, Total Reward = 75.0, Total Steps = 58752\n",
      "Episode 1535, Total Reward = 226.0, Total Steps = 58978\n",
      "Episode 1536, Total Reward = 77.0, Total Steps = 59055\n",
      "Episode 1537, Total Reward = 37.0, Total Steps = 59092\n",
      "Episode 1538, Total Reward = 256.0, Total Steps = 59348\n",
      "Episode 1539, Total Reward = 62.0, Total Steps = 59410\n",
      "Episode 1540, Total Reward = 128.0, Total Steps = 59538\n",
      "Episode 1541, Total Reward = 87.0, Total Steps = 59625\n",
      "Episode 1542, Total Reward = 416.0, Total Steps = 60041\n",
      "Episode 1543, Total Reward = 89.0, Total Steps = 60130\n",
      "Episode 1544, Total Reward = 13.0, Total Steps = 60143\n",
      "Episode 1545, Total Reward = 31.0, Total Steps = 60174\n",
      "Episode 1546, Total Reward = 48.0, Total Steps = 60222\n",
      "Episode 1547, Total Reward = 67.0, Total Steps = 60289\n",
      "Episode 1548, Total Reward = 161.0, Total Steps = 60450\n",
      "Episode 1549, Total Reward = 159.0, Total Steps = 60609\n",
      "Episode 1550, Total Reward = 7.0, Total Steps = 60616\n",
      "Episode 1551, Total Reward = 62.0, Total Steps = 60678\n",
      "Episode 1552, Total Reward = 29.0, Total Steps = 60707\n",
      "Episode 1553, Total Reward = 254.0, Total Steps = 60961\n",
      "Episode 1554, Total Reward = 89.0, Total Steps = 61050\n",
      "Episode 1555, Total Reward = 105.0, Total Steps = 61155\n",
      "Episode 1556, Total Reward = 46.0, Total Steps = 61201\n",
      "Episode 1557, Total Reward = 228.0, Total Steps = 61429\n",
      "Episode 1558, Total Reward = 49.0, Total Steps = 61478\n",
      "Episode 1559, Total Reward = 78.0, Total Steps = 61556\n",
      "Episode 1560, Total Reward = 72.0, Total Steps = 61628\n",
      "Episode 1561, Total Reward = 323.0, Total Steps = 61951\n",
      "Episode 1562, Total Reward = 8.0, Total Steps = 61959\n",
      "Episode 1563, Total Reward = 358.0, Total Steps = 62317\n",
      "Episode 1564, Total Reward = 200.0, Total Steps = 62517\n",
      "Episode 1565, Total Reward = 91.0, Total Steps = 62608\n",
      "Episode 1566, Total Reward = 101.0, Total Steps = 62709\n",
      "Episode 1567, Total Reward = 55.0, Total Steps = 62764\n",
      "Episode 1568, Total Reward = 164.0, Total Steps = 62928\n",
      "Episode 1569, Total Reward = 47.0, Total Steps = 62975\n",
      "Episode 1570, Total Reward = 94.0, Total Steps = 63069\n",
      "Episode 1571, Total Reward = 221.0, Total Steps = 63290\n",
      "Episode 1572, Total Reward = 88.0, Total Steps = 63378\n",
      "Episode 1573, Total Reward = 218.0, Total Steps = 63596\n",
      "Episode 1574, Total Reward = 421.0, Total Steps = 64017\n",
      "Episode 1575, Total Reward = 132.0, Total Steps = 64149\n",
      "Episode 1576, Total Reward = 177.0, Total Steps = 64326\n",
      "Episode 1577, Total Reward = 107.0, Total Steps = 64433\n",
      "Episode 1578, Total Reward = 68.0, Total Steps = 64501\n",
      "Episode 1579, Total Reward = 207.0, Total Steps = 64708\n",
      "Episode 1580, Total Reward = 50.0, Total Steps = 64758\n",
      "Episode 1581, Total Reward = 10.0, Total Steps = 64768\n",
      "Episode 1582, Total Reward = 57.0, Total Steps = 64825\n",
      "Episode 1583, Total Reward = 143.0, Total Steps = 64968\n",
      "Episode 1584, Total Reward = 25.0, Total Steps = 64993\n",
      "Episode 1585, Total Reward = 230.0, Total Steps = 65223\n",
      "Episode 1586, Total Reward = 19.0, Total Steps = 65242\n",
      "Episode 1587, Total Reward = 248.0, Total Steps = 65490\n",
      "Episode 1588, Total Reward = 51.0, Total Steps = 65541\n",
      "Episode 1589, Total Reward = 89.0, Total Steps = 65630\n",
      "Episode 1590, Total Reward = 12.0, Total Steps = 65642\n",
      "Episode 1591, Total Reward = 37.0, Total Steps = 65679\n",
      "Episode 1592, Total Reward = 568.0, Total Steps = 66247\n",
      "Episode 1593, Total Reward = 11.0, Total Steps = 66258\n",
      "Episode 1594, Total Reward = 31.0, Total Steps = 66289\n",
      "Episode 1595, Total Reward = 49.0, Total Steps = 66338\n",
      "Episode 1596, Total Reward = 68.0, Total Steps = 66406\n",
      "Episode 1597, Total Reward = 228.0, Total Steps = 66634\n",
      "Episode 1598, Total Reward = 126.0, Total Steps = 66760\n",
      "Episode 1599, Total Reward = 59.0, Total Steps = 66819\n",
      "Episode 1600, Total Reward = 12.0, Total Steps = 66831\n",
      "Episode 1601, Total Reward = 16.0, Total Steps = 66847\n",
      "Episode 1602, Total Reward = 468.0, Total Steps = 67315\n",
      "Episode 1603, Total Reward = 56.0, Total Steps = 67371\n",
      "Episode 1604, Total Reward = 66.0, Total Steps = 67437\n",
      "Episode 1605, Total Reward = 245.0, Total Steps = 67682\n",
      "Episode 1606, Total Reward = 264.0, Total Steps = 67946\n",
      "Episode 1607, Total Reward = 95.0, Total Steps = 68041\n",
      "Episode 1608, Total Reward = 155.0, Total Steps = 68196\n",
      "Episode 1609, Total Reward = 44.0, Total Steps = 68240\n",
      "Episode 1610, Total Reward = 222.0, Total Steps = 68462\n",
      "Episode 1611, Total Reward = 17.0, Total Steps = 68479\n",
      "Episode 1612, Total Reward = 194.0, Total Steps = 68673\n",
      "Episode 1613, Total Reward = 229.0, Total Steps = 68902\n",
      "Episode 1614, Total Reward = 39.0, Total Steps = 68941\n",
      "Episode 1615, Total Reward = 19.0, Total Steps = 68960\n",
      "Episode 1616, Total Reward = 256.0, Total Steps = 69216\n",
      "Episode 1617, Total Reward = 144.0, Total Steps = 69360\n",
      "Episode 1618, Total Reward = 121.0, Total Steps = 69481\n",
      "Episode 1619, Total Reward = 87.0, Total Steps = 69568\n",
      "Episode 1620, Total Reward = 26.0, Total Steps = 69594\n",
      "Episode 1621, Total Reward = 340.0, Total Steps = 69934\n",
      "Episode 1622, Total Reward = 180.0, Total Steps = 70114\n",
      "Episode 1623, Total Reward = 91.0, Total Steps = 70205\n",
      "Episode 1624, Total Reward = 447.0, Total Steps = 70652\n",
      "Episode 1625, Total Reward = 36.0, Total Steps = 70688\n",
      "Episode 1626, Total Reward = 202.0, Total Steps = 70890\n",
      "Episode 1627, Total Reward = 237.0, Total Steps = 71127\n",
      "Episode 1628, Total Reward = 171.0, Total Steps = 71298\n",
      "Episode 1629, Total Reward = 338.0, Total Steps = 71636\n",
      "Episode 1630, Total Reward = 174.0, Total Steps = 71810\n",
      "Episode 1631, Total Reward = 60.0, Total Steps = 71870\n",
      "Episode 1632, Total Reward = 28.0, Total Steps = 71898\n",
      "Episode 1633, Total Reward = 149.0, Total Steps = 72047\n",
      "Episode 1634, Total Reward = 259.0, Total Steps = 72306\n",
      "Episode 1635, Total Reward = 78.0, Total Steps = 72384\n",
      "Episode 1636, Total Reward = 587.0, Total Steps = 72971\n",
      "Episode 1637, Total Reward = 52.0, Total Steps = 73023\n",
      "Episode 1638, Total Reward = 71.0, Total Steps = 73094\n",
      "Episode 1639, Total Reward = 10.0, Total Steps = 73104\n",
      "Episode 1640, Total Reward = 18.0, Total Steps = 73122\n",
      "Episode 1641, Total Reward = 117.0, Total Steps = 73239\n",
      "Episode 1642, Total Reward = 329.0, Total Steps = 73568\n",
      "Episode 1643, Total Reward = 152.0, Total Steps = 73720\n",
      "Episode 1644, Total Reward = 278.0, Total Steps = 73998\n",
      "Episode 1645, Total Reward = 67.0, Total Steps = 74065\n",
      "Episode 1646, Total Reward = 146.0, Total Steps = 74211\n",
      "Episode 1647, Total Reward = 45.0, Total Steps = 74256\n",
      "Episode 1648, Total Reward = 284.0, Total Steps = 74540\n",
      "Episode 1649, Total Reward = 42.0, Total Steps = 74582\n",
      "Episode 1650, Total Reward = 25.0, Total Steps = 74607\n",
      "Episode 1651, Total Reward = 35.0, Total Steps = 74642\n",
      "Episode 1652, Total Reward = 150.0, Total Steps = 74792\n",
      "Episode 1653, Total Reward = 564.0, Total Steps = 75356\n",
      "Episode 1654, Total Reward = 256.0, Total Steps = 75612\n",
      "Episode 1655, Total Reward = 28.0, Total Steps = 75640\n",
      "Episode 1656, Total Reward = 139.0, Total Steps = 75779\n",
      "Episode 1657, Total Reward = 75.0, Total Steps = 75854\n",
      "Episode 1658, Total Reward = 12.0, Total Steps = 75866\n",
      "Episode 1659, Total Reward = 55.0, Total Steps = 75921\n",
      "Episode 1660, Total Reward = 120.0, Total Steps = 76041\n",
      "Episode 1661, Total Reward = 120.0, Total Steps = 76161\n",
      "Episode 1662, Total Reward = 41.0, Total Steps = 76202\n",
      "Episode 1663, Total Reward = 201.0, Total Steps = 76403\n",
      "Episode 1664, Total Reward = 287.0, Total Steps = 76690\n",
      "Episode 1665, Total Reward = 59.0, Total Steps = 76749\n",
      "Episode 1666, Total Reward = 330.0, Total Steps = 77079\n",
      "Episode 1667, Total Reward = 180.0, Total Steps = 77259\n",
      "Episode 1668, Total Reward = 194.0, Total Steps = 77453\n",
      "Episode 1669, Total Reward = 52.0, Total Steps = 77505\n",
      "Episode 1670, Total Reward = 328.0, Total Steps = 77833\n",
      "Episode 1671, Total Reward = 38.0, Total Steps = 77871\n",
      "Episode 1672, Total Reward = 53.0, Total Steps = 77924\n",
      "Episode 1673, Total Reward = 15.0, Total Steps = 77939\n",
      "Episode 1674, Total Reward = 367.0, Total Steps = 78306\n",
      "Episode 1675, Total Reward = 402.0, Total Steps = 78708\n",
      "Episode 1676, Total Reward = 62.0, Total Steps = 78770\n",
      "Episode 1677, Total Reward = 396.0, Total Steps = 79166\n",
      "Episode 1678, Total Reward = 40.0, Total Steps = 79206\n",
      "Episode 1679, Total Reward = 305.0, Total Steps = 79511\n",
      "Episode 1680, Total Reward = 48.0, Total Steps = 79559\n",
      "Episode 1681, Total Reward = 249.0, Total Steps = 79808\n",
      "Episode 1682, Total Reward = 75.0, Total Steps = 79883\n",
      "Episode 1683, Total Reward = 316.0, Total Steps = 80199\n",
      "Episode 1684, Total Reward = 14.0, Total Steps = 80213\n",
      "Episode 1685, Total Reward = 420.0, Total Steps = 80633\n",
      "Episode 1686, Total Reward = 55.0, Total Steps = 80688\n",
      "Episode 1687, Total Reward = 450.0, Total Steps = 81138\n",
      "Episode 1688, Total Reward = 285.0, Total Steps = 81423\n",
      "Episode 1689, Total Reward = 13.0, Total Steps = 81436\n",
      "Episode 1690, Total Reward = 13.0, Total Steps = 81449\n",
      "Episode 1691, Total Reward = 73.0, Total Steps = 81522\n",
      "Episode 1692, Total Reward = 458.0, Total Steps = 81980\n",
      "Episode 1693, Total Reward = 94.0, Total Steps = 82074\n",
      "Episode 1694, Total Reward = 180.0, Total Steps = 82254\n",
      "Episode 1695, Total Reward = 49.0, Total Steps = 82303\n",
      "Episode 1696, Total Reward = 179.0, Total Steps = 82482\n",
      "Episode 1697, Total Reward = 144.0, Total Steps = 82626\n",
      "Episode 1698, Total Reward = 153.0, Total Steps = 82779\n",
      "Episode 1699, Total Reward = 318.0, Total Steps = 83097\n",
      "Episode 1700, Total Reward = 116.0, Total Steps = 83213\n",
      "Episode 1701, Total Reward = 94.0, Total Steps = 83307\n",
      "Episode 1702, Total Reward = 221.0, Total Steps = 83528\n",
      "Episode 1703, Total Reward = 58.0, Total Steps = 83586\n",
      "Episode 1704, Total Reward = 292.0, Total Steps = 83878\n",
      "Episode 1705, Total Reward = 231.0, Total Steps = 84109\n",
      "Episode 1706, Total Reward = 428.0, Total Steps = 84537\n",
      "Episode 1707, Total Reward = 189.0, Total Steps = 84726\n",
      "Episode 1708, Total Reward = 222.0, Total Steps = 84948\n",
      "Episode 1709, Total Reward = 31.0, Total Steps = 84979\n",
      "Episode 1710, Total Reward = 151.0, Total Steps = 85130\n",
      "Episode 1711, Total Reward = 142.0, Total Steps = 85272\n",
      "Episode 1712, Total Reward = 270.0, Total Steps = 85542\n",
      "Episode 1713, Total Reward = 92.0, Total Steps = 85634\n",
      "Episode 1714, Total Reward = 487.0, Total Steps = 86121\n",
      "Episode 1715, Total Reward = 290.0, Total Steps = 86411\n",
      "Episode 1716, Total Reward = 13.0, Total Steps = 86424\n",
      "Episode 1717, Total Reward = 153.0, Total Steps = 86577\n",
      "Episode 1718, Total Reward = 280.0, Total Steps = 86857\n",
      "Episode 1719, Total Reward = 165.0, Total Steps = 87022\n",
      "Episode 1720, Total Reward = 301.0, Total Steps = 87323\n",
      "Episode 1721, Total Reward = 37.0, Total Steps = 87360\n",
      "Episode 1722, Total Reward = 352.0, Total Steps = 87712\n",
      "Episode 1723, Total Reward = 37.0, Total Steps = 87749\n",
      "Episode 1724, Total Reward = 206.0, Total Steps = 87955\n",
      "Episode 1725, Total Reward = 368.0, Total Steps = 88323\n",
      "Episode 1726, Total Reward = 49.0, Total Steps = 88372\n",
      "Episode 1727, Total Reward = 508.0, Total Steps = 88880\n",
      "Episode 1728, Total Reward = 71.0, Total Steps = 88951\n",
      "Episode 1729, Total Reward = 341.0, Total Steps = 89292\n",
      "Episode 1730, Total Reward = 388.0, Total Steps = 89680\n",
      "Episode 1731, Total Reward = 53.0, Total Steps = 89733\n",
      "Episode 1732, Total Reward = 448.0, Total Steps = 90181\n",
      "Episode 1733, Total Reward = 343.0, Total Steps = 90524\n",
      "Episode 1734, Total Reward = 23.0, Total Steps = 90547\n",
      "Episode 1735, Total Reward = 118.0, Total Steps = 90665\n",
      "Episode 1736, Total Reward = 175.0, Total Steps = 90840\n",
      "Episode 1737, Total Reward = 577.0, Total Steps = 91417\n",
      "Episode 1738, Total Reward = 212.0, Total Steps = 91629\n",
      "Episode 1739, Total Reward = 120.0, Total Steps = 91749\n",
      "Episode 1740, Total Reward = 444.0, Total Steps = 92193\n",
      "Episode 1741, Total Reward = 107.0, Total Steps = 92300\n",
      "Episode 1742, Total Reward = 52.0, Total Steps = 92352\n",
      "Episode 1743, Total Reward = 85.0, Total Steps = 92437\n",
      "Episode 1744, Total Reward = 466.0, Total Steps = 92903\n",
      "Episode 1745, Total Reward = 188.0, Total Steps = 93091\n",
      "Episode 1746, Total Reward = 418.0, Total Steps = 93509\n",
      "Episode 1747, Total Reward = 191.0, Total Steps = 93700\n",
      "Episode 1748, Total Reward = 365.0, Total Steps = 94065\n",
      "Episode 1749, Total Reward = 144.0, Total Steps = 94209\n",
      "Episode 1750, Total Reward = 349.0, Total Steps = 94558\n",
      "Episode 1751, Total Reward = 150.0, Total Steps = 94708\n",
      "Episode 1752, Total Reward = 221.0, Total Steps = 94929\n",
      "Episode 1753, Total Reward = 228.0, Total Steps = 95157\n",
      "Episode 1754, Total Reward = 289.0, Total Steps = 95446\n",
      "Episode 1755, Total Reward = 196.0, Total Steps = 95642\n",
      "Episode 1756, Total Reward = 341.0, Total Steps = 95983\n",
      "Episode 1757, Total Reward = 407.0, Total Steps = 96390\n",
      "Episode 1758, Total Reward = 77.0, Total Steps = 96467\n",
      "Episode 1759, Total Reward = 90.0, Total Steps = 96557\n",
      "Episode 1760, Total Reward = 355.0, Total Steps = 96912\n",
      "Episode 1761, Total Reward = 105.0, Total Steps = 97017\n",
      "Episode 1762, Total Reward = 7.0, Total Steps = 97024\n",
      "Episode 1763, Total Reward = 434.0, Total Steps = 97458\n",
      "Episode 1764, Total Reward = 279.0, Total Steps = 97737\n",
      "Episode 1765, Total Reward = 51.0, Total Steps = 97788\n",
      "Episode 1766, Total Reward = 98.0, Total Steps = 97886\n",
      "Episode 1767, Total Reward = 393.0, Total Steps = 98279\n",
      "Episode 1768, Total Reward = 194.0, Total Steps = 98473\n",
      "Episode 1769, Total Reward = 66.0, Total Steps = 98539\n",
      "Episode 1770, Total Reward = 872.0, Total Steps = 99411\n",
      "Episode 1771, Total Reward = 88.0, Total Steps = 99499\n",
      "Episode 1772, Total Reward = 169.0, Total Steps = 99668\n",
      "Episode 1773, Total Reward = 221.0, Total Steps = 99889\n",
      "Episode 1774, Total Reward = 217.0, Total Steps = 100106\n",
      "Episode 1775, Total Reward = 358.0, Total Steps = 100464\n",
      "Episode 1776, Total Reward = 544.0, Total Steps = 101008\n",
      "Episode 1777, Total Reward = 7.0, Total Steps = 101015\n",
      "Episode 1778, Total Reward = 21.0, Total Steps = 101036\n",
      "Episode 1779, Total Reward = 279.0, Total Steps = 101315\n",
      "Episode 1780, Total Reward = 234.0, Total Steps = 101549\n",
      "Episode 1781, Total Reward = 284.0, Total Steps = 101833\n",
      "Episode 1782, Total Reward = 40.0, Total Steps = 101873\n",
      "Episode 1783, Total Reward = 73.0, Total Steps = 101946\n",
      "Episode 1784, Total Reward = 127.0, Total Steps = 102073\n",
      "Episode 1785, Total Reward = 631.0, Total Steps = 102704\n",
      "Episode 1786, Total Reward = 404.0, Total Steps = 103108\n",
      "Episode 1787, Total Reward = 397.0, Total Steps = 103505\n",
      "Episode 1788, Total Reward = 400.0, Total Steps = 103905\n",
      "Episode 1789, Total Reward = 406.0, Total Steps = 104311\n",
      "Episode 1790, Total Reward = 265.0, Total Steps = 104576\n",
      "Episode 1791, Total Reward = 400.0, Total Steps = 104976\n",
      "Episode 1792, Total Reward = 73.0, Total Steps = 105049\n",
      "Episode 1793, Total Reward = 394.0, Total Steps = 105443\n",
      "Episode 1794, Total Reward = 42.0, Total Steps = 105485\n",
      "Episode 1795, Total Reward = 774.0, Total Steps = 106259\n",
      "Episode 1796, Total Reward = 210.0, Total Steps = 106469\n",
      "Episode 1797, Total Reward = 86.0, Total Steps = 106555\n",
      "Episode 1798, Total Reward = 607.0, Total Steps = 107162\n",
      "Episode 1799, Total Reward = 413.0, Total Steps = 107575\n",
      "Episode 1800, Total Reward = 60.0, Total Steps = 107635\n",
      "Episode 1801, Total Reward = 11.0, Total Steps = 107646\n",
      "Episode 1802, Total Reward = 971.0, Total Steps = 108617\n",
      "Episode 1803, Total Reward = 371.0, Total Steps = 108988\n",
      "Episode 1804, Total Reward = 38.0, Total Steps = 109026\n",
      "Episode 1805, Total Reward = 633.0, Total Steps = 109659\n",
      "Episode 1806, Total Reward = 581.0, Total Steps = 110240\n",
      "Episode 1807, Total Reward = 133.0, Total Steps = 110373\n",
      "Episode 1808, Total Reward = 482.0, Total Steps = 110855\n",
      "Episode 1809, Total Reward = 124.0, Total Steps = 110979\n",
      "Episode 1810, Total Reward = 139.0, Total Steps = 111118\n",
      "Episode 1811, Total Reward = 135.0, Total Steps = 111253\n",
      "Episode 1812, Total Reward = 641.0, Total Steps = 111894\n",
      "Episode 1813, Total Reward = 392.0, Total Steps = 112286\n",
      "Episode 1814, Total Reward = 28.0, Total Steps = 112314\n",
      "Episode 1815, Total Reward = 492.0, Total Steps = 112806\n",
      "Episode 1816, Total Reward = 34.0, Total Steps = 112840\n",
      "Episode 1817, Total Reward = 524.0, Total Steps = 113364\n",
      "Episode 1818, Total Reward = 10.0, Total Steps = 113374\n",
      "Episode 1819, Total Reward = 366.0, Total Steps = 113740\n",
      "Episode 1820, Total Reward = 396.0, Total Steps = 114136\n",
      "Episode 1821, Total Reward = 401.0, Total Steps = 114537\n",
      "Episode 1822, Total Reward = 782.0, Total Steps = 115319\n",
      "Episode 1823, Total Reward = 209.0, Total Steps = 115528\n",
      "Episode 1824, Total Reward = 482.0, Total Steps = 116010\n",
      "Episode 1825, Total Reward = 204.0, Total Steps = 116214\n",
      "Episode 1826, Total Reward = 551.0, Total Steps = 116765\n",
      "Episode 1827, Total Reward = 354.0, Total Steps = 117119\n",
      "Episode 1828, Total Reward = 540.0, Total Steps = 117659\n",
      "Episode 1829, Total Reward = 228.0, Total Steps = 117887\n",
      "Episode 1830, Total Reward = 748.0, Total Steps = 118635\n",
      "Episode 1831, Total Reward = 134.0, Total Steps = 118769\n",
      "Episode 1832, Total Reward = 960.0, Total Steps = 119729\n",
      "Episode 1833, Total Reward = 486.0, Total Steps = 120215\n",
      "Episode 1834, Total Reward = 29.0, Total Steps = 120244\n",
      "Episode 1835, Total Reward = 77.0, Total Steps = 120321\n",
      "Episode 1836, Total Reward = 446.0, Total Steps = 120767\n",
      "Episode 1837, Total Reward = 97.0, Total Steps = 120864\n",
      "Episode 1838, Total Reward = 655.0, Total Steps = 121519\n",
      "Episode 1839, Total Reward = 10.0, Total Steps = 121529\n",
      "Episode 1840, Total Reward = 192.0, Total Steps = 121721\n",
      "Episode 1841, Total Reward = 340.0, Total Steps = 122061\n",
      "Episode 1842, Total Reward = 768.0, Total Steps = 122829\n",
      "Episode 1843, Total Reward = 64.0, Total Steps = 122893\n",
      "Episode 1844, Total Reward = 152.0, Total Steps = 123045\n",
      "Episode 1845, Total Reward = 292.0, Total Steps = 123337\n",
      "Episode 1846, Total Reward = 568.0, Total Steps = 123905\n",
      "Episode 1847, Total Reward = 385.0, Total Steps = 124290\n",
      "Episode 1848, Total Reward = 889.0, Total Steps = 125179\n",
      "Episode 1849, Total Reward = 35.0, Total Steps = 125214\n",
      "Episode 1850, Total Reward = 533.0, Total Steps = 125747\n",
      "Episode 1851, Total Reward = 667.0, Total Steps = 126414\n",
      "Episode 1852, Total Reward = 93.0, Total Steps = 126507\n",
      "Episode 1853, Total Reward = 333.0, Total Steps = 126840\n",
      "Episode 1854, Total Reward = 1000.0, Total Steps = 127840\n",
      "Episode 1855, Total Reward = 50.0, Total Steps = 127890\n",
      "Episode 1856, Total Reward = 803.0, Total Steps = 128693\n",
      "Episode 1857, Total Reward = 788.0, Total Steps = 129481\n",
      "Episode 1858, Total Reward = 323.0, Total Steps = 129804\n",
      "Episode 1859, Total Reward = 467.0, Total Steps = 130271\n",
      "Episode 1860, Total Reward = 782.0, Total Steps = 131053\n",
      "Episode 1861, Total Reward = 47.0, Total Steps = 131100\n",
      "Episode 1862, Total Reward = 1000.0, Total Steps = 132100\n",
      "Episode 1863, Total Reward = 114.0, Total Steps = 132214\n",
      "Episode 1864, Total Reward = 579.0, Total Steps = 132793\n",
      "Episode 1865, Total Reward = 515.0, Total Steps = 133308\n",
      "Episode 1866, Total Reward = 729.0, Total Steps = 134037\n",
      "Episode 1867, Total Reward = 1000.0, Total Steps = 135037\n",
      "Episode 1868, Total Reward = 46.0, Total Steps = 135083\n",
      "Episode 1869, Total Reward = 12.0, Total Steps = 135095\n",
      "Episode 1870, Total Reward = 169.0, Total Steps = 135264\n",
      "Episode 1871, Total Reward = 185.0, Total Steps = 135449\n",
      "Episode 1872, Total Reward = 141.0, Total Steps = 135590\n",
      "Episode 1873, Total Reward = 1000.0, Total Steps = 136590\n",
      "Episode 1874, Total Reward = 176.0, Total Steps = 136766\n",
      "Episode 1875, Total Reward = 243.0, Total Steps = 137009\n",
      "Episode 1876, Total Reward = 652.0, Total Steps = 137661\n",
      "Episode 1877, Total Reward = 446.0, Total Steps = 138107\n",
      "Episode 1878, Total Reward = 236.0, Total Steps = 138343\n",
      "Episode 1879, Total Reward = 1000.0, Total Steps = 139343\n",
      "Episode 1880, Total Reward = 553.0, Total Steps = 139896\n",
      "Episode 1881, Total Reward = 166.0, Total Steps = 140062\n",
      "Episode 1882, Total Reward = 694.0, Total Steps = 140756\n",
      "Episode 1883, Total Reward = 464.0, Total Steps = 141220\n",
      "Episode 1884, Total Reward = 361.0, Total Steps = 141581\n",
      "Episode 1885, Total Reward = 66.0, Total Steps = 141647\n",
      "Episode 1886, Total Reward = 56.0, Total Steps = 141703\n",
      "Episode 1887, Total Reward = 75.0, Total Steps = 141778\n",
      "Episode 1888, Total Reward = 64.0, Total Steps = 141842\n",
      "Episode 1889, Total Reward = 1000.0, Total Steps = 142842\n",
      "Episode 1890, Total Reward = 229.0, Total Steps = 143071\n",
      "Episode 1891, Total Reward = 236.0, Total Steps = 143307\n",
      "Episode 1892, Total Reward = 89.0, Total Steps = 143396\n",
      "Episode 1893, Total Reward = 161.0, Total Steps = 143557\n",
      "Episode 1894, Total Reward = 1000.0, Total Steps = 144557\n",
      "Episode 1895, Total Reward = 43.0, Total Steps = 144600\n",
      "Episode 1896, Total Reward = 117.0, Total Steps = 144717\n",
      "Episode 1897, Total Reward = 1000.0, Total Steps = 145717\n",
      "Episode 1898, Total Reward = 153.0, Total Steps = 145870\n",
      "Episode 1899, Total Reward = 855.0, Total Steps = 146725\n",
      "Episode 1900, Total Reward = 446.0, Total Steps = 147171\n",
      "Episode 1901, Total Reward = 284.0, Total Steps = 147455\n",
      "Episode 1902, Total Reward = 1000.0, Total Steps = 148455\n",
      "Episode 1903, Total Reward = 717.0, Total Steps = 149172\n",
      "Episode 1904, Total Reward = 386.0, Total Steps = 149558\n",
      "Episode 1905, Total Reward = 1000.0, Total Steps = 150558\n",
      "Episode 1906, Total Reward = 67.0, Total Steps = 150625\n",
      "Episode 1907, Total Reward = 398.0, Total Steps = 151023\n",
      "Episode 1908, Total Reward = 1000.0, Total Steps = 152023\n",
      "Episode 1909, Total Reward = 424.0, Total Steps = 152447\n",
      "Episode 1910, Total Reward = 507.0, Total Steps = 152954\n",
      "Episode 1911, Total Reward = 380.0, Total Steps = 153334\n",
      "Episode 1912, Total Reward = 754.0, Total Steps = 154088\n",
      "Episode 1913, Total Reward = 300.0, Total Steps = 154388\n",
      "Episode 1914, Total Reward = 1000.0, Total Steps = 155388\n",
      "Episode 1915, Total Reward = 782.0, Total Steps = 156170\n",
      "Episode 1916, Total Reward = 588.0, Total Steps = 156758\n",
      "Episode 1917, Total Reward = 435.0, Total Steps = 157193\n",
      "Episode 1918, Total Reward = 141.0, Total Steps = 157334\n",
      "Episode 1919, Total Reward = 1000.0, Total Steps = 158334\n",
      "Episode 1920, Total Reward = 1000.0, Total Steps = 159334\n",
      "Episode 1921, Total Reward = 39.0, Total Steps = 159373\n",
      "Episode 1922, Total Reward = 1000.0, Total Steps = 160373\n",
      "Episode 1923, Total Reward = 191.0, Total Steps = 160564\n",
      "Episode 1924, Total Reward = 9.0, Total Steps = 160573\n",
      "Episode 1925, Total Reward = 427.0, Total Steps = 161000\n",
      "Episode 1926, Total Reward = 659.0, Total Steps = 161659\n",
      "Episode 1927, Total Reward = 1000.0, Total Steps = 162659\n",
      "Episode 1928, Total Reward = 980.0, Total Steps = 163639\n",
      "Episode 1929, Total Reward = 356.0, Total Steps = 163995\n",
      "Episode 1930, Total Reward = 468.0, Total Steps = 164463\n",
      "Episode 1931, Total Reward = 1000.0, Total Steps = 165463\n",
      "Episode 1932, Total Reward = 1000.0, Total Steps = 166463\n",
      "Episode 1933, Total Reward = 41.0, Total Steps = 166504\n",
      "Episode 1934, Total Reward = 437.0, Total Steps = 166941\n",
      "Episode 1935, Total Reward = 477.0, Total Steps = 167418\n",
      "Episode 1936, Total Reward = 865.0, Total Steps = 168283\n",
      "Episode 1937, Total Reward = 615.0, Total Steps = 168898\n",
      "Episode 1938, Total Reward = 229.0, Total Steps = 169127\n",
      "Episode 1939, Total Reward = 1000.0, Total Steps = 170127\n",
      "Episode 1940, Total Reward = 1000.0, Total Steps = 171127\n",
      "Episode 1941, Total Reward = 87.0, Total Steps = 171214\n",
      "Episode 1942, Total Reward = 718.0, Total Steps = 171932\n",
      "Episode 1943, Total Reward = 1000.0, Total Steps = 172932\n",
      "Episode 1944, Total Reward = 138.0, Total Steps = 173070\n",
      "Episode 1945, Total Reward = 178.0, Total Steps = 173248\n",
      "Episode 1946, Total Reward = 221.0, Total Steps = 173469\n",
      "Episode 1947, Total Reward = 1000.0, Total Steps = 174469\n",
      "Episode 1948, Total Reward = 804.0, Total Steps = 175273\n",
      "Episode 1949, Total Reward = 1000.0, Total Steps = 176273\n",
      "Episode 1950, Total Reward = 110.0, Total Steps = 176383\n",
      "Episode 1951, Total Reward = 1000.0, Total Steps = 177383\n",
      "Episode 1952, Total Reward = 209.0, Total Steps = 177592\n",
      "Episode 1953, Total Reward = 1000.0, Total Steps = 178592\n",
      "Episode 1954, Total Reward = 880.0, Total Steps = 179472\n",
      "Episode 1955, Total Reward = 501.0, Total Steps = 179973\n",
      "Episode 1956, Total Reward = 494.0, Total Steps = 180467\n",
      "Episode 1957, Total Reward = 1000.0, Total Steps = 181467\n",
      "Episode 1958, Total Reward = 427.0, Total Steps = 181894\n",
      "Episode 1959, Total Reward = 771.0, Total Steps = 182665\n",
      "Episode 1960, Total Reward = 645.0, Total Steps = 183310\n",
      "Episode 1961, Total Reward = 981.0, Total Steps = 184291\n",
      "Episode 1962, Total Reward = 76.0, Total Steps = 184367\n",
      "Episode 1963, Total Reward = 1000.0, Total Steps = 185367\n",
      "Episode 1964, Total Reward = 585.0, Total Steps = 185952\n",
      "Episode 1965, Total Reward = 1000.0, Total Steps = 186952\n",
      "Episode 1966, Total Reward = 1000.0, Total Steps = 187952\n",
      "Episode 1967, Total Reward = 1000.0, Total Steps = 188952\n",
      "Episode 1968, Total Reward = 242.0, Total Steps = 189194\n",
      "Episode 1969, Total Reward = 1000.0, Total Steps = 190194\n",
      "Episode 1970, Total Reward = 101.0, Total Steps = 190295\n",
      "Episode 1971, Total Reward = 154.0, Total Steps = 190449\n",
      "Episode 1972, Total Reward = 1000.0, Total Steps = 191449\n",
      "Episode 1973, Total Reward = 1000.0, Total Steps = 192449\n",
      "Episode 1974, Total Reward = 417.0, Total Steps = 192866\n",
      "Episode 1975, Total Reward = 1000.0, Total Steps = 193866\n",
      "Episode 1976, Total Reward = 1000.0, Total Steps = 194866\n",
      "Episode 1977, Total Reward = 101.0, Total Steps = 194967\n",
      "Episode 1978, Total Reward = 1000.0, Total Steps = 195967\n",
      "Episode 1979, Total Reward = 413.0, Total Steps = 196380\n",
      "Episode 1980, Total Reward = 1000.0, Total Steps = 197380\n",
      "Episode 1981, Total Reward = 1000.0, Total Steps = 198380\n",
      "Episode 1982, Total Reward = 311.0, Total Steps = 198691\n",
      "Episode 1983, Total Reward = 1000.0, Total Steps = 199691\n",
      "Episode 1984, Total Reward = 926.0, Total Steps = 200617\n",
      "Episode 1985, Total Reward = 1000.0, Total Steps = 201617\n",
      "Episode 1986, Total Reward = 630.0, Total Steps = 202247\n",
      "Episode 1987, Total Reward = 1000.0, Total Steps = 203247\n",
      "Episode 1988, Total Reward = 1000.0, Total Steps = 204247\n",
      "Episode 1989, Total Reward = 114.0, Total Steps = 204361\n",
      "Episode 1990, Total Reward = 1000.0, Total Steps = 205361\n",
      "Episode 1991, Total Reward = 1000.0, Total Steps = 206361\n",
      "Episode 1992, Total Reward = 1000.0, Total Steps = 207361\n",
      "Episode 1993, Total Reward = 428.0, Total Steps = 207789\n",
      "Episode 1994, Total Reward = 572.0, Total Steps = 208361\n",
      "Episode 1995, Total Reward = 1000.0, Total Steps = 209361\n",
      "Episode 1996, Total Reward = 1000.0, Total Steps = 210361\n",
      "Episode 1997, Total Reward = 1000.0, Total Steps = 211361\n",
      "Episode 1998, Total Reward = 1000.0, Total Steps = 212361\n",
      "Episode 1999, Total Reward = 354.0, Total Steps = 212715\n",
      "Episode 2000, Total Reward = 190.0, Total Steps = 212905\n",
      "Episode 2001, Total Reward = 1000.0, Total Steps = 213905\n",
      "Episode 2002, Total Reward = 1000.0, Total Steps = 214905\n",
      "Episode 2003, Total Reward = 1000.0, Total Steps = 215905\n",
      "Episode 2004, Total Reward = 691.0, Total Steps = 216596\n",
      "Episode 2005, Total Reward = 1000.0, Total Steps = 217596\n",
      "Episode 2006, Total Reward = 81.0, Total Steps = 217677\n",
      "Episode 2007, Total Reward = 382.0, Total Steps = 218059\n",
      "Episode 2008, Total Reward = 283.0, Total Steps = 218342\n",
      "Episode 2009, Total Reward = 1000.0, Total Steps = 219342\n",
      "Episode 2010, Total Reward = 1000.0, Total Steps = 220342\n",
      "Episode 2011, Total Reward = 1000.0, Total Steps = 221342\n",
      "Episode 2012, Total Reward = 1000.0, Total Steps = 222342\n",
      "Episode 2013, Total Reward = 1000.0, Total Steps = 223342\n",
      "Episode 2014, Total Reward = 648.0, Total Steps = 223990\n",
      "Episode 2015, Total Reward = 1000.0, Total Steps = 224990\n",
      "Episode 2016, Total Reward = 1000.0, Total Steps = 225990\n",
      "Episode 2017, Total Reward = 1000.0, Total Steps = 226990\n",
      "Episode 2018, Total Reward = 1000.0, Total Steps = 227990\n",
      "Episode 2019, Total Reward = 1000.0, Total Steps = 228990\n",
      "Episode 2020, Total Reward = 1000.0, Total Steps = 229990\n",
      "Episode 2021, Total Reward = 1000.0, Total Steps = 230990\n",
      "Episode 2022, Total Reward = 44.0, Total Steps = 231034\n",
      "Episode 2023, Total Reward = 1000.0, Total Steps = 232034\n",
      "Episode 2024, Total Reward = 1000.0, Total Steps = 233034\n",
      "Episode 2025, Total Reward = 1000.0, Total Steps = 234034\n",
      "Episode 2026, Total Reward = 918.0, Total Steps = 234952\n",
      "Episode 2027, Total Reward = 1000.0, Total Steps = 235952\n",
      "Episode 2028, Total Reward = 1000.0, Total Steps = 236952\n",
      "Episode 2029, Total Reward = 1000.0, Total Steps = 237952\n",
      "Episode 2030, Total Reward = 1000.0, Total Steps = 238952\n",
      "Episode 2031, Total Reward = 1000.0, Total Steps = 239952\n",
      "Episode 2032, Total Reward = 1000.0, Total Steps = 240952\n",
      "Episode 2033, Total Reward = 1000.0, Total Steps = 241952\n",
      "Episode 2034, Total Reward = 1000.0, Total Steps = 242952\n",
      "Episode 2035, Total Reward = 1000.0, Total Steps = 243952\n",
      "Episode 2036, Total Reward = 1000.0, Total Steps = 244952\n",
      "Episode 2037, Total Reward = 1000.0, Total Steps = 245952\n",
      "Episode 2038, Total Reward = 1000.0, Total Steps = 246952\n",
      "Episode 2039, Total Reward = 1000.0, Total Steps = 247952\n",
      "Episode 2040, Total Reward = 321.0, Total Steps = 248273\n",
      "Episode 2041, Total Reward = 1000.0, Total Steps = 249273\n",
      "Episode 2042, Total Reward = 1000.0, Total Steps = 250273\n",
      "Episode 2043, Total Reward = 1000.0, Total Steps = 251273\n",
      "Episode 2044, Total Reward = 1000.0, Total Steps = 252273\n",
      "Episode 2045, Total Reward = 1000.0, Total Steps = 253273\n",
      "Episode 2046, Total Reward = 1000.0, Total Steps = 254273\n",
      "Episode 2047, Total Reward = 1000.0, Total Steps = 255273\n",
      "Episode 2048, Total Reward = 1000.0, Total Steps = 256273\n",
      "Episode 2049, Total Reward = 1000.0, Total Steps = 257273\n",
      "Episode 2050, Total Reward = 1000.0, Total Steps = 258273\n",
      "Episode 2051, Total Reward = 1000.0, Total Steps = 259273\n",
      "Episode 2052, Total Reward = 1000.0, Total Steps = 260273\n",
      "Episode 2053, Total Reward = 346.0, Total Steps = 260619\n",
      "Episode 2054, Total Reward = 1000.0, Total Steps = 261619\n",
      "Episode 2055, Total Reward = 1000.0, Total Steps = 262619\n",
      "Episode 2056, Total Reward = 1000.0, Total Steps = 263619\n",
      "Episode 2057, Total Reward = 1000.0, Total Steps = 264619\n",
      "Episode 2058, Total Reward = 1000.0, Total Steps = 265619\n",
      "Episode 2059, Total Reward = 1000.0, Total Steps = 266619\n",
      "Episode 2060, Total Reward = 1000.0, Total Steps = 267619\n",
      "Episode 2061, Total Reward = 1000.0, Total Steps = 268619\n",
      "Episode 2062, Total Reward = 1000.0, Total Steps = 269619\n",
      "Episode 2063, Total Reward = 1000.0, Total Steps = 270619\n",
      "Episode 2064, Total Reward = 1000.0, Total Steps = 271619\n",
      "Episode 2065, Total Reward = 1000.0, Total Steps = 272619\n",
      "Episode 2066, Total Reward = 1000.0, Total Steps = 273619\n",
      "Episode 2067, Total Reward = 1000.0, Total Steps = 274619\n",
      "Episode 2068, Total Reward = 1000.0, Total Steps = 275619\n",
      "Episode 2069, Total Reward = 1000.0, Total Steps = 276619\n",
      "Episode 2070, Total Reward = 1000.0, Total Steps = 277619\n",
      "Episode 2071, Total Reward = 1000.0, Total Steps = 278619\n",
      "Episode 2072, Total Reward = 1000.0, Total Steps = 279619\n",
      "Episode 2073, Total Reward = 1000.0, Total Steps = 280619\n",
      "Episode 2074, Total Reward = 1000.0, Total Steps = 281619\n",
      "Episode 2075, Total Reward = 1000.0, Total Steps = 282619\n",
      "Episode 2076, Total Reward = 1000.0, Total Steps = 283619\n",
      "Episode 2077, Total Reward = 786.0, Total Steps = 284405\n",
      "Episode 2078, Total Reward = 1000.0, Total Steps = 285405\n",
      "Episode 2079, Total Reward = 1000.0, Total Steps = 286405\n",
      "Episode 2080, Total Reward = 1000.0, Total Steps = 287405\n",
      "Episode 2081, Total Reward = 1000.0, Total Steps = 288405\n",
      "Episode 2082, Total Reward = 1000.0, Total Steps = 289405\n",
      "Episode 2083, Total Reward = 1000.0, Total Steps = 290405\n",
      "Episode 2084, Total Reward = 1000.0, Total Steps = 291405\n",
      "Episode 2085, Total Reward = 1000.0, Total Steps = 292405\n",
      "Episode 2086, Total Reward = 1000.0, Total Steps = 293405\n",
      "Episode 2087, Total Reward = 1000.0, Total Steps = 294405\n",
      "Episode 2088, Total Reward = 1000.0, Total Steps = 295405\n",
      "Episode 2089, Total Reward = 1000.0, Total Steps = 296405\n",
      "Episode 2090, Total Reward = 1000.0, Total Steps = 297405\n",
      "Episode 2091, Total Reward = 1000.0, Total Steps = 298405\n",
      "Episode 2092, Total Reward = 1000.0, Total Steps = 299405\n",
      "Episode 2093, Total Reward = 1000.0, Total Steps = 300405\n",
      "Episode 2094, Total Reward = 1000.0, Total Steps = 301405\n",
      "Episode 2095, Total Reward = 1000.0, Total Steps = 302405\n",
      "Episode 2096, Total Reward = 1000.0, Total Steps = 303405\n",
      "Episode 2097, Total Reward = 1000.0, Total Steps = 304405\n",
      "Episode 2098, Total Reward = 1000.0, Total Steps = 305405\n",
      "Episode 2099, Total Reward = 1000.0, Total Steps = 306405\n",
      "Episode 2100, Total Reward = 1000.0, Total Steps = 307405\n",
      "Episode 2101, Total Reward = 1000.0, Total Steps = 308405\n",
      "Episode 2102, Total Reward = 1000.0, Total Steps = 309405\n",
      "Episode 2103, Total Reward = 1000.0, Total Steps = 310405\n",
      "Episode 2104, Total Reward = 1000.0, Total Steps = 311405\n",
      "Episode 2105, Total Reward = 1000.0, Total Steps = 312405\n",
      "Episode 2106, Total Reward = 848.0, Total Steps = 313253\n",
      "Episode 2107, Total Reward = 1000.0, Total Steps = 314253\n",
      "Episode 2108, Total Reward = 1000.0, Total Steps = 315253\n",
      "Episode 2109, Total Reward = 1000.0, Total Steps = 316253\n",
      "Episode 2110, Total Reward = 1000.0, Total Steps = 317253\n",
      "Episode 2111, Total Reward = 1000.0, Total Steps = 318253\n",
      "Episode 2112, Total Reward = 1000.0, Total Steps = 319253\n",
      "Episode 2113, Total Reward = 1000.0, Total Steps = 320253\n",
      "Episode 2114, Total Reward = 1000.0, Total Steps = 321253\n",
      "Episode 2115, Total Reward = 1000.0, Total Steps = 322253\n",
      "Episode 2116, Total Reward = 1000.0, Total Steps = 323253\n",
      "Episode 2117, Total Reward = 1000.0, Total Steps = 324253\n",
      "Episode 2118, Total Reward = 1000.0, Total Steps = 325253\n",
      "Episode 2119, Total Reward = 1000.0, Total Steps = 326253\n",
      "Episode 2120, Total Reward = 1000.0, Total Steps = 327253\n",
      "Episode 2121, Total Reward = 1000.0, Total Steps = 328253\n",
      "Episode 2122, Total Reward = 1000.0, Total Steps = 329253\n",
      "Episode 2123, Total Reward = 1000.0, Total Steps = 330253\n",
      "Episode 2124, Total Reward = 1000.0, Total Steps = 331253\n",
      "Episode 2125, Total Reward = 1000.0, Total Steps = 332253\n",
      "Episode 2126, Total Reward = 1000.0, Total Steps = 333253\n",
      "Episode 2127, Total Reward = 1000.0, Total Steps = 334253\n",
      "Episode 2128, Total Reward = 1000.0, Total Steps = 335253\n",
      "Episode 2129, Total Reward = 1000.0, Total Steps = 336253\n",
      "Episode 2130, Total Reward = 1000.0, Total Steps = 337253\n",
      "Episode 2131, Total Reward = 1000.0, Total Steps = 338253\n",
      "Episode 2132, Total Reward = 1000.0, Total Steps = 339253\n",
      "Episode 2133, Total Reward = 1000.0, Total Steps = 340253\n",
      "Episode 2134, Total Reward = 1000.0, Total Steps = 341253\n",
      "Episode 2135, Total Reward = 1000.0, Total Steps = 342253\n",
      "Episode 2136, Total Reward = 1000.0, Total Steps = 343253\n",
      "Episode 2137, Total Reward = 1000.0, Total Steps = 344253\n",
      "Episode 2138, Total Reward = 1000.0, Total Steps = 345253\n",
      "Episode 2139, Total Reward = 1000.0, Total Steps = 346253\n",
      "Episode 2140, Total Reward = 1000.0, Total Steps = 347253\n",
      "Episode 2141, Total Reward = 1000.0, Total Steps = 348253\n",
      "Episode 2142, Total Reward = 1000.0, Total Steps = 349253\n",
      "Episode 2143, Total Reward = 1000.0, Total Steps = 350253\n",
      "Episode 2144, Total Reward = 1000.0, Total Steps = 351253\n",
      "Episode 2145, Total Reward = 1000.0, Total Steps = 352253\n",
      "Episode 2146, Total Reward = 1000.0, Total Steps = 353253\n",
      "Episode 2147, Total Reward = 1000.0, Total Steps = 354253\n",
      "Episode 2148, Total Reward = 1000.0, Total Steps = 355253\n",
      "Episode 2149, Total Reward = 1000.0, Total Steps = 356253\n",
      "Episode 2150, Total Reward = 1000.0, Total Steps = 357253\n",
      "Episode 2151, Total Reward = 1000.0, Total Steps = 358253\n",
      "Episode 2152, Total Reward = 1000.0, Total Steps = 359253\n",
      "Episode 2153, Total Reward = 1000.0, Total Steps = 360253\n",
      "Episode 2154, Total Reward = 1000.0, Total Steps = 361253\n",
      "Episode 2155, Total Reward = 1000.0, Total Steps = 362253\n",
      "Episode 2156, Total Reward = 1000.0, Total Steps = 363253\n",
      "Episode 2157, Total Reward = 1000.0, Total Steps = 364253\n",
      "Episode 2158, Total Reward = 1000.0, Total Steps = 365253\n",
      "Episode 2159, Total Reward = 1000.0, Total Steps = 366253\n",
      "Episode 2160, Total Reward = 1000.0, Total Steps = 367253\n",
      "Episode 2161, Total Reward = 1000.0, Total Steps = 368253\n",
      "Episode 2162, Total Reward = 1000.0, Total Steps = 369253\n",
      "Episode 2163, Total Reward = 1000.0, Total Steps = 370253\n",
      "Episode 2164, Total Reward = 1000.0, Total Steps = 371253\n",
      "Episode 2165, Total Reward = 1000.0, Total Steps = 372253\n",
      "Episode 2166, Total Reward = 1000.0, Total Steps = 373253\n",
      "Episode 2167, Total Reward = 1000.0, Total Steps = 374253\n",
      "Episode 2168, Total Reward = 1000.0, Total Steps = 375253\n",
      "Episode 2169, Total Reward = 1000.0, Total Steps = 376253\n",
      "Episode 2170, Total Reward = 1000.0, Total Steps = 377253\n",
      "Episode 2171, Total Reward = 1000.0, Total Steps = 378253\n",
      "Episode 2172, Total Reward = 1000.0, Total Steps = 379253\n",
      "Episode 2173, Total Reward = 1000.0, Total Steps = 380253\n",
      "Episode 2174, Total Reward = 1000.0, Total Steps = 381253\n",
      "Episode 2175, Total Reward = 1000.0, Total Steps = 382253\n",
      "Episode 2176, Total Reward = 1000.0, Total Steps = 383253\n",
      "Episode 2177, Total Reward = 1000.0, Total Steps = 384253\n",
      "Episode 2178, Total Reward = 1000.0, Total Steps = 385253\n",
      "Episode 2179, Total Reward = 1000.0, Total Steps = 386253\n",
      "Episode 2180, Total Reward = 1000.0, Total Steps = 387253\n",
      "Episode 2181, Total Reward = 1000.0, Total Steps = 388253\n",
      "Episode 2182, Total Reward = 1000.0, Total Steps = 389253\n",
      "Episode 2183, Total Reward = 1000.0, Total Steps = 390253\n",
      "Episode 2184, Total Reward = 1000.0, Total Steps = 391253\n",
      "Episode 2185, Total Reward = 1000.0, Total Steps = 392253\n",
      "Episode 2186, Total Reward = 1000.0, Total Steps = 393253\n",
      "Episode 2187, Total Reward = 1000.0, Total Steps = 394253\n",
      "Episode 2188, Total Reward = 1000.0, Total Steps = 395253\n",
      "Episode 2189, Total Reward = 1000.0, Total Steps = 396253\n",
      "Episode 2190, Total Reward = 1000.0, Total Steps = 397253\n",
      "Episode 2191, Total Reward = 1000.0, Total Steps = 398253\n",
      "Episode 2192, Total Reward = 1000.0, Total Steps = 399253\n",
      "Episode 2193, Total Reward = 1000.0, Total Steps = 400253\n",
      "Episode 2194, Total Reward = 1000.0, Total Steps = 401253\n",
      "Episode 2195, Total Reward = 1000.0, Total Steps = 402253\n",
      "Episode 2196, Total Reward = 1000.0, Total Steps = 403253\n",
      "Episode 2197, Total Reward = 1000.0, Total Steps = 404253\n",
      "Episode 2198, Total Reward = 1000.0, Total Steps = 405253\n",
      "Episode 2199, Total Reward = 1000.0, Total Steps = 406253\n",
      "Episode 2200, Total Reward = 1000.0, Total Steps = 407253\n",
      "Episode 2201, Total Reward = 1000.0, Total Steps = 408253\n",
      "Episode 2202, Total Reward = 1000.0, Total Steps = 409253\n",
      "Episode 2203, Total Reward = 1000.0, Total Steps = 410253\n",
      "Episode 2204, Total Reward = 1000.0, Total Steps = 411253\n",
      "Episode 2205, Total Reward = 1000.0, Total Steps = 412253\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 144\u001b[0m\n\u001b[0;32m    142\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPOAgent(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    143\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvertedPendulum-v4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 109\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[1;34m(self, env, episodes, batch_size)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m    108\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m--> 109\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    110\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    112\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat())\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 23\u001b[0m, in \u001b[0;36mActorCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_common(x)\n\u001b[0;32m     22\u001b[0m mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_actor(x)\n\u001b[1;32m---> 23\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mexpand_as(mu)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mu, std, value\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.fc_common = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.fc_actor = nn.Linear(64, action_dim)\n",
    "        self.fc_critic = nn.Linear(64, 1)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_common(x)\n",
    "        mu = self.fc_actor(x)\n",
    "        value = self.fc_critic(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        return mu, std, value\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, lr=3e-4, gamma=0.99, epsilon=0.2, k_epochs=10, minibatch_size=64, gae_lambda=0.95):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.actor_critic = ActorCritic(input_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        mu, std, _ = self.actor_critic(state)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.k_epochs):\n",
    "            indices = np.arange(states.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            for start in range(0, states.shape[0], self.minibatch_size):\n",
    "                end = start + self.minibatch_size\n",
    "                minibatch_indices = indices[start:end]\n",
    "                \n",
    "                minibatch_states = states[minibatch_indices]\n",
    "                minibatch_actions = actions[minibatch_indices]\n",
    "                minibatch_log_probs_old = log_probs_old[minibatch_indices]\n",
    "                minibatch_returns = returns[minibatch_indices]\n",
    "                minibatch_advantages = advantages[minibatch_indices]\n",
    "\n",
    "                mu, std, values = self.actor_critic(minibatch_states)\n",
    "                dist = torch.distributions.Normal(mu, std)\n",
    "                log_probs_new = dist.log_prob(minibatch_actions).sum(dim=-1)\n",
    "                entropy = dist.entropy().mean()\n",
    "\n",
    "                ratios = torch.exp(log_probs_new - minibatch_log_probs_old)\n",
    "                surr1 = ratios * minibatch_advantages\n",
    "                surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * minibatch_advantages\n",
    "\n",
    "                actor_loss = -torch.min(surr1, surr2).mean() - 0.01 * entropy\n",
    "                critic_loss = self.mse_loss(values, minibatch_returns)\n",
    "\n",
    "                loss = actor_loss + 0.5 * critic_loss\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "    def train(self, env, episodes=1000000, batch_size=64):\n",
    "        all_rewards = []\n",
    "        step_counter = 0\n",
    "\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()[0]\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "                step_counter += 1\n",
    "\n",
    "                if len(states) % batch_size == 0:\n",
    "                    next_value = self.actor_critic(torch.from_numpy(state).float())[-1].item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {episode + 1}, Total Reward = {episode_rewards}, Total Steps = {step_counter}\")\n",
    "\n",
    "            \n",
    "\n",
    "        return all_rewards\n",
    "\n",
    "ppo = PPOAgent(4, 1)\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "rewards = ppo.train(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "\n",
    "class AgentNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(AgentNet, self).__init__()\n",
    "        self.affine = nn.Linear(num_inputs, 128)\n",
    "        self.action_head = nn.Linear(128, num_outputs)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.affine(x))\n",
    "        action_probs = torch.softmax(self.action_head(x), dim=-1)\n",
    "        state_values = self.value_head(x)\n",
    "        return action_probs, state_values\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, config, policy_params):\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        self.model = AgentNet(env.observation_space.shape[0], env.action_space.n)\n",
    "        self.model.load_state_dict(policy_params)\n",
    "\n",
    "    def step(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs, state_value = self.model(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        next_state, reward, done, _ = self.env.step(action.item())\n",
    "        return action.item(), log_prob, state_value, next_state, reward, done\n",
    "\n",
    "    def update_policy(self, states, actions, old_log_probs, rewards, values, optimizer):\n",
    "        actions = torch.tensor(actions)\n",
    "        old_log_probs = torch.stack(old_log_probs)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        values = torch.cat(values)\n",
    "        masks = torch.tensor([1.0] * len(rewards))\n",
    "\n",
    "        # Adding last value for advantage calculation\n",
    "        _, last_value = self.model(torch.from_numpy(states[-1]).float().unsqueeze(0))\n",
    "        values = torch.cat([values, last_value.detach()])\n",
    "\n",
    "        advantages = self.compute_advantages(rewards, masks, values)\n",
    "\n",
    "        # Convert advantages to tensor and standardize\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Convert rewards to returns\n",
    "        returns = advantages + values[:-1]\n",
    "\n",
    "        # Optimization loop\n",
    "        for _ in range(self.config['epochs']):\n",
    "            idx = torch.randperm(len(states))\n",
    "            for batch_indices in idx.split(self.config['batch_size']):\n",
    "                sampled_states = torch.tensor(states)[batch_indices]\n",
    "                sampled_actions = actions[batch_indices]\n",
    "                sampled_old_log_probs = old_log_probs[batch_indices]\n",
    "                sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                new_probs, new_values = self.model(sampled_states)\n",
    "                new_dist = Categorical(new_probs)\n",
    "                new_log_probs = new_dist.log_prob(sampled_actions)\n",
    "\n",
    "                # Calculating the ratio (pi_theta / pi_theta_old):\n",
    "                ratio = torch.exp(new_log_probs - sampled_old_log_probs)\n",
    "\n",
    "                # Clipped surrogate loss\n",
    "                surr1 = ratio * sampled_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * sampled_advantages\n",
    "                loss = -torch.min(surr1, surr2).mean()  # Focus only on the clipping part\n",
    "\n",
    "                # take gradient step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(worker_id, policy_params, config, return_dict):\n",
    "    \"\"\"Worker process to collect data from the environment.\"\"\"\n",
    "    np.random.seed(worker_id)\n",
    "    torch.manual_seed(worker_id)\n",
    "    env = gym.make(config['env_name'])\n",
    "    agent = PPOAgent(env, config, policy_params)\n",
    "\n",
    "    state = env.reset()\n",
    "    rewards, log_probs, states, actions, values = [], [], [], [], []\n",
    "    for _ in range(config['horizon']):\n",
    "        action, log_prob, value, next_state, reward, done = agent.step(state)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        state = next_state if not done else env.reset()\n",
    "\n",
    "    return_dict[worker_id] = {\n",
    "        'states': states,\n",
    "        'actions': actions,\n",
    "        'log_probs': log_probs,\n",
    "        'values': values,\n",
    "        'rewards': rewards\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    config = {\n",
    "        'env_name': 'InvertedPendulum-v4',\n",
    "        'horizon': 2048,\n",
    "        'learning_rate': 3e-4,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 10,\n",
    "        'gamma': 0.99,\n",
    "        'gae_lambda': 0.95,\n",
    "        'num_workers': 4\n",
    "    }\n",
    "\n",
    "    env = gym.make(config['env_name'])\n",
    "    model = AgentNet(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "\n",
    "    for iteration in range(10):  # run for 10 iterations\n",
    "        processes = []\n",
    "        for i in range(config['num_workers']):\n",
    "            p = mp.Process(target=worker, args=(i, model.state_dict(), config, return_dict))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        \n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "        # Aggregate data from all workers\n",
    "        print(return_dict)\n",
    "        aggregated_data = {k: [] for k in return_dict[0].keys()}\n",
    "        for i in range(config['num_workers']):\n",
    "            for key in aggregated_data.keys():\n",
    "                aggregated_data[key].extend(return_dict[i][key])\n",
    "        \n",
    "        # Convert lists to tensors and perform PPO update\n",
    "        states = torch.FloatTensor(aggregated_data['states'])\n",
    "        actions = torch.LongTensor(aggregated_data['actions'])\n",
    "        old_log_probs = torch.stack(aggregated_data['log_probs'])\n",
    "        rewards = torch.FloatTensor(aggregated_data['rewards'])\n",
    "        values = torch.stack(aggregated_data['values'])\n",
    "\n",
    "        # Example PPO update, assuming `update_policy` is implemented\n",
    "        agent = PPOAgent(env, config, model.state_dict())\n",
    "        agent.update_policy(states, actions, old_log_probs, rewards, values, optimizer)\n",
    "\n",
    "        print(f'Iteration {iteration + 1} complete.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('InvertedPendulum-v4')\n",
    "env.reset()\n",
    "env.step([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.zeros(3,2,4)\n",
    "a[:,1,:]=torch.tensor([1,1,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
