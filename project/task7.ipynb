{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "env = gym.make('InvertedPendulum-v4',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Updating\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\autograd\\__init__.py:266: UserWarning: Error detected in MulBackward0. Traceback of forward call that caused the error:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\asyncio\\base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\asyncio\\base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 542, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 531, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 359, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 775, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 446, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\shani\\AppData\\Local\\Temp\\ipykernel_2452\\3634575283.py\", line 166, in <module>\n",
      "    ppo.train()\n",
      "  File \"C:\\Users\\shani\\AppData\\Local\\Temp\\ipykernel_2452\\3634575283.py\", line 55, in train\n",
      "    returns, advantages = self.compute_gae(rewards, values, dones)\n",
      "  File \"C:\\Users\\shani\\AppData\\Local\\Temp\\ipykernel_2452\\3634575283.py\", line 115, in compute_gae\n",
      "    gae = delta + self.gamma * self.lam * (1 - dones[i]) * gae\n",
      " (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:118.)\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 166\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m average_reward\n\u001b[0;32m    165\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvertedPendulum-v4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 166\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 65\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m advantages \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(advantages)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUpdating\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Print the total reward per episode\u001b[39;00m\n\u001b[0;32m     68\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(episode_rewards)\n",
      "Cell \u001b[1;32mIn[9], line 142\u001b[0m, in \u001b[0;36mPPO.update\u001b[1;34m(self, states, actions, returns, log_probs, advantages)\u001b[0m\n\u001b[0;32m    139\u001b[0m loss \u001b[38;5;241m=\u001b[39m actor_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc1 \u001b[38;5;241m*\u001b[39m critic_loss \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc2 \u001b[38;5;241m*\u001b[39m entropy\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 142\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, horizon=2048, epochs=10, lr=3e-4, gamma=0.99, c1=0.5, c2=0.01, minibatch_size=64, lam=0.95, epsilon=0.2):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.lam = lam\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            states, actions, rewards, log_probs, values, dones, episode_rewards = self.generate_trajectories()\n",
    "            returns, advantages = self.compute_gae(rewards, values, dones)\n",
    "            \n",
    "            # Flatten the trajectories\n",
    "            states = torch.cat(states)\n",
    "            actions = torch.cat(actions)\n",
    "            log_probs = torch.cat(log_probs)\n",
    "            returns = torch.cat(returns)\n",
    "            advantages = torch.cat(advantages)\n",
    "            \n",
    "            print('Updating')\n",
    "            self.update(states, actions, returns, log_probs, advantages)\n",
    "\n",
    "            # Print the total reward per episode\n",
    "            total_reward = sum(episode_rewards)\n",
    "            print(f'Total Reward per Episode: {total_reward}')\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        dones = []\n",
    "        episode_rewards = []\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "        episode_reward = 0\n",
    "        for _ in range(self.horizon):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            dist, value = self.agent(state_tensor)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "            \n",
    "            states.append(state_tensor)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(dist.log_prob(action).sum(dim=-1, keepdim=True))\n",
    "            values.append(value)\n",
    "            dones.append(done)\n",
    "            episode_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_reward = 0\n",
    "                state = self.env.reset()[0]\n",
    "\n",
    "        if episode_reward != 0:\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "        return states, actions, rewards, log_probs, values, dones, episode_rewards\n",
    "\n",
    "    def compute_gae(self, rewards, values, dones):\n",
    "        values = values + [torch.zeros(1, 1)]\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + self.gamma * values[i + 1] * (1 - dones[i]) - values[i]\n",
    "            gae = delta + self.gamma * self.lam * (1 - dones[i]) * gae\n",
    "            returns.insert(0, gae + values[i])\n",
    "            advantages.insert(0, gae)\n",
    "        return returns, advantages\n",
    "\n",
    "    def update(self, states, actions, returns, log_probs, advantages):\n",
    "        for _ in range(self.horizon // self.minibatch_size):\n",
    "            indices = torch.randint(0, self.horizon, size=(self.minibatch_size,))\n",
    "            sampled_states = states[indices]\n",
    "            sampled_actions = actions[indices]\n",
    "            sampled_returns = returns[indices]\n",
    "            sampled_log_probs = log_probs[indices]\n",
    "            sampled_advantages = advantages[indices]\n",
    "\n",
    "            dist, values = self.agent(sampled_states)\n",
    "            new_log_probs = dist.log_prob(sampled_actions).sum(dim=-1, keepdim=True)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            ratio = (new_log_probs - sampled_log_probs).exp()\n",
    "            surr1 = ratio * sampled_advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * sampled_advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (sampled_returns - values).pow(2).mean()\n",
    "\n",
    "            loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def evaluate(self, env, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, _ = self.agent(state_tensor)\n",
    "                action = dist.mean  # Using mean action for evaluation\n",
    "                next_state, reward, done, _, _ = env.step(action.detach().numpy()[0])\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "ppo = PPO(\"InvertedPendulum-v4\")\n",
    "ppo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep  1, reward 3.0, epsilon 0.9998000100000001\n",
      "timestep  2, reward 3.0, epsilon 0.9996000599960002\n",
      "timestep  3, reward 5.0, epsilon 0.9992002799440072\n",
      "timestep  4, reward 4.0, epsilon 0.9989005498350332\n",
      "timestep  5, reward 4.0, epsilon 0.9986009096361004\n",
      "timestep  6, reward 4.0, epsilon 0.9983013593202382\n",
      "timestep  7, reward 6.0, epsilon 0.9978023084607315\n",
      "timestep  8, reward 4.0, epsilon 0.9975029977012647\n",
      "timestep  9, reward 5.0, epsilon 0.9971040563483742\n",
      "timestep  10, reward 5.0, epsilon 0.9967052745480899\n",
      "timestep  11, reward 4.0, epsilon 0.9964062928658871\n",
      "timestep  12, reward 7.0, epsilon 0.995808598531185\n",
      "timestep  13, reward 5.0, epsilon 0.9954103348363054\n",
      "timestep  14, reward 4.0, epsilon 0.9951117415971691\n",
      "timestep  15, reward 6.0, epsilon 0.9946142852275941\n",
      "timestep  16, reward 3.0, epsilon 0.9944153723166914\n",
      "timestep  17, reward 4.0, epsilon 0.9941170775364633\n",
      "timestep  18, reward 3.0, epsilon 0.9939182640621268\n",
      "timestep  19, reward 4.0, epsilon 0.9936201183994622\n",
      "timestep  20, reward 4.0, epsilon 0.9933220621715524\n",
      "timestep  21, reward 4.0, epsilon 0.9930240953515695\n",
      "timestep  22, reward 5.0, epsilon 0.9926269452909028\n",
      "timestep  23, reward 4.0, epsilon 0.9923291869851313\n",
      "timestep  24, reward 3.0, epsilon 0.9921307310710261\n",
      "timestep  25, reward 5.0, epsilon 0.9917339383024733\n",
      "timestep  26, reward 5.0, epsilon 0.9913373042272218\n",
      "timestep  27, reward 4.0, epsilon 0.9910399327750814\n",
      "timestep  28, reward 4.0, epsilon 0.9907426505254558\n",
      "timestep  29, reward 3.0, epsilon 0.9905445119027773\n",
      "timestep  30, reward 4.0, epsilon 0.9902473782645512\n",
      "timestep  31, reward 6.0, epsilon 0.9897523535902548\n",
      "timestep  32, reward 7.0, epsilon 0.9891586506211602\n",
      "timestep  33, reward 4.0, epsilon 0.9888619326997442\n",
      "timestep  34, reward 5.0, epsilon 0.988466447254425\n",
      "timestep  35, reward 5.0, epsilon 0.9880711199795563\n",
      "timestep  36, reward 4.0, epsilon 0.987774728284708\n",
      "timestep  37, reward 6.0, epsilon 0.9872809396881612\n",
      "timestep  38, reward 4.0, epsilon 0.9869847850236957\n",
      "timestep  39, reward 7.0, epsilon 0.9863927421806611\n",
      "timestep  40, reward 3.0, epsilon 0.9861954734961524\n",
      "timestep  41, reward 5.0, epsilon 0.9858010544745378\n",
      "timestep  42, reward 4.0, epsilon 0.9855053437312414\n",
      "timestep  43, reward 4.0, epsilon 0.9852097216922968\n",
      "timestep  44, reward 4.0, epsilon 0.9849141883310956\n",
      "timestep  45, reward 5.0, epsilon 0.9845202817466749\n",
      "timestep  46, reward 5.0, epsilon 0.9841265327012553\n",
      "timestep  47, reward 5.0, epsilon 0.9837329411318304\n",
      "timestep  48, reward 4.0, epsilon 0.9834378507604954\n",
      "timestep  49, reward 4.0, epsilon 0.9831428489074193\n",
      "timestep  50, reward 5.0, epsilon 0.9827496507524949\n",
      "timestep  51, reward 5.0, epsilon 0.9823566098532421\n",
      "timestep  52, reward 3.0, epsilon 0.9821601483548376\n",
      "timestep  53, reward 4.0, epsilon 0.9818655297741534\n",
      "timestep  54, reward 5.0, epsilon 0.9814728424702482\n",
      "timestep  55, reward 5.0, epsilon 0.9810803122177049\n",
      "timestep  56, reward 6.0, epsilon 0.9805898701598169\n",
      "timestep  57, reward 4.0, epsilon 0.9802957226154846\n",
      "timestep  58, reward 4.0, epsilon 0.9800016633065913\n",
      "timestep  59, reward 3.0, epsilon 0.9798056727739467\n",
      "timestep  60, reward 3.0, epsilon 0.9796097214374486\n",
      "timestep  61, reward 4.0, epsilon 0.9793158679083295\n",
      "timestep  62, reward 4.0, epsilon 0.9790221025264537\n",
      "timestep  63, reward 3.0, epsilon 0.9788263078961694\n",
      "timestep  64, reward 4.0, epsilon 0.9785326893676111\n",
      "timestep  65, reward 6.0, epsilon 0.9780435208664114\n",
      "timestep  66, reward 4.0, epsilon 0.9777501371504792\n",
      "timestep  67, reward 5.0, epsilon 0.9773590957567164\n",
      "timestep  68, reward 5.0, epsilon 0.9769682107560502\n",
      "timestep  69, reward 4.0, epsilon 0.9766751496008927\n",
      "timestep  70, reward 3.0, epsilon 0.9764798243377241\n",
      "timestep  71, reward 3.0, epsilon 0.9762845381376548\n",
      "timestep  72, reward 6.0, epsilon 0.9757964934872775\n",
      "timestep  73, reward 4.0, epsilon 0.9755037838121503\n",
      "timestep  74, reward 3.0, epsilon 0.9753086928104258\n",
      "timestep  75, reward 6.0, epsilon 0.9748211359851373\n",
      "timestep  76, reward 3.0, epsilon 0.9746261815061517\n",
      "timestep  77, reward 4.0, epsilon 0.9743338228895106\n",
      "timestep  78, reward 5.0, epsilon 0.973944147816487\n",
      "timestep  79, reward 3.0, epsilon 0.9737493687263652\n",
      "timestep  80, reward 5.0, epsilon 0.9733599273999418\n",
      "timestep  81, reward 5.0, epsilon 0.9729706418266841\n",
      "timestep  82, reward 4.0, epsilon 0.9726787798222825\n",
      "timestep  83, reward 6.0, epsilon 0.9721925376905229\n",
      "timestep  84, reward 3.0, epsilon 0.9719981089049102\n",
      "timestep  85, reward 5.0, epsilon 0.9716093679773469\n",
      "timestep  86, reward 5.0, epsilon 0.9712207825228317\n",
      "timestep  87, reward 5.0, epsilon 0.9708323524791846\n",
      "timestep  88, reward 5.0, epsilon 0.970444077784251\n",
      "timestep  89, reward 5.0, epsilon 0.9700559583759003\n",
      "timestep  90, reward 4.0, epsilon 0.9697649706890963\n",
      "timestep  91, reward 3.0, epsilon 0.9695710273926083\n",
      "timestep  92, reward 7.0, epsilon 0.9689894301924369\n",
      "timestep  93, reward 4.0, epsilon 0.9686987624320932\n",
      "timestep  94, reward 5.0, epsilon 0.9683113410451715\n",
      "timestep  95, reward 6.0, epsilon 0.9678272821961004\n",
      "timestep  96, reward 4.0, epsilon 0.9675369630452922\n",
      "timestep  97, reward 5.0, epsilon 0.967150006308422\n",
      "timestep  98, reward 4.0, epsilon 0.9668598903200626\n",
      "timestep  99, reward 5.0, epsilon 0.9664732043716606\n",
      "timestep  100, reward 4.0, epsilon 0.9661832914035788\n",
      "timestep  101, reward 5.0, epsilon 0.9657968760541503\n",
      "timestep  102, reward 5.0, epsilon 0.9654106152476781\n",
      "timestep  103, reward 3.0, epsilon 0.9652175427787347\n",
      "timestep  104, reward 3.0, epsilon 0.9650245089223544\n",
      "timestep  105, reward 4.0, epsilon 0.9647350305194479\n",
      "timestep  106, reward 3.0, epsilon 0.9645420931606943\n",
      "timestep  107, reward 7.0, epsilon 0.9639635125668224\n",
      "timestep  108, reward 5.0, epsilon 0.9635779849957506\n",
      "timestep  109, reward 4.0, epsilon 0.9632889405066278\n",
      "timestep  110, reward 4.0, epsilon 0.9629999827221808\n",
      "timestep  111, reward 4.0, epsilon 0.9627111116164007\n",
      "timestep  112, reward 6.0, epsilon 0.9622298523220771\n",
      "timestep  113, reward 5.0, epsilon 0.9618450181110908\n",
      "timestep  114, reward 5.0, epsilon 0.9614603378107003\n",
      "timestep  115, reward 4.0, epsilon 0.9611719285522058\n",
      "timestep  116, reward 7.0, epsilon 0.960595369551642\n",
      "timestep  117, reward 4.0, epsilon 0.9603072197576771\n",
      "timestep  118, reward 4.0, epsilon 0.9600191564000061\n",
      "timestep  119, reward 7.0, epsilon 0.9594432888898407\n",
      "timestep  120, reward 3.0, epsilon 0.9592514098264956\n",
      "timestep  121, reward 4.0, epsilon 0.9589636631801307\n",
      "timestep  122, reward 6.0, epsilon 0.9584842772353178\n",
      "timestep  123, reward 5.0, epsilon 0.9581009410296466\n",
      "timestep  124, reward 4.0, epsilon 0.9578135394894078\n",
      "timestep  125, reward 3.0, epsilon 0.9576219863596453\n",
      "timestep  126, reward 5.0, epsilon 0.9572389950185903\n",
      "timestep  127, reward 5.0, epsilon 0.9568561568510938\n",
      "timestep  128, reward 6.0, epsilon 0.9563778244487159\n",
      "timestep  129, reward 3.0, epsilon 0.9561865584476045\n",
      "timestep  130, reward 6.0, epsilon 0.9557085607774751\n",
      "timestep  131, reward 5.0, epsilon 0.9553263346918551\n",
      "timestep  132, reward 5.0, epsilon 0.9549442614737373\n",
      "timestep  133, reward 6.0, epsilon 0.9544668848278777\n",
      "timestep  134, reward 4.0, epsilon 0.9541805733954815\n",
      "timestep  135, reward 4.0, epsilon 0.9538943478479258\n",
      "timestep  136, reward 5.0, epsilon 0.9535128473386321\n",
      "timestep  137, reward 5.0, epsilon 0.9531314994066535\n",
      "timestep  138, reward 3.0, epsilon 0.9529408826380873\n",
      "timestep  139, reward 5.0, epsilon 0.9525597634576733\n",
      "timestep  140, reward 3.0, epsilon 0.9523692610305795\n",
      "timestep  141, reward 5.0, epsilon 0.9519883704645137\n",
      "timestep  142, reward 4.0, epsilon 0.9517028025120734\n",
      "timestep  143, reward 4.0, epsilon 0.9514173202214522\n",
      "timestep  144, reward 5.0, epsilon 0.9510368103745972\n",
      "timestep  145, reward 4.0, epsilon 0.9507515278616381\n",
      "timestep  146, reward 7.0, epsilon 0.9501812195386368\n",
      "timestep  147, reward 4.0, epsilon 0.9498961936772615\n",
      "timestep  148, reward 4.0, epsilon 0.9496112533150943\n",
      "timestep  149, reward 3.0, epsilon 0.9494213405605438\n",
      "timestep  150, reward 4.0, epsilon 0.9491365426400665\n",
      "timestep  151, reward 5.0, epsilon 0.9487569449674067\n",
      "timestep  152, reward 3.0, epsilon 0.9485672030659826\n",
      "timestep  153, reward 4.0, epsilon 0.9482826613611303\n",
      "timestep  154, reward 4.0, epsilon 0.9479982050102536\n",
      "timestep  155, reward 4.0, epsilon 0.9477138339877487\n",
      "timestep  156, reward 6.0, epsilon 0.9472400718326615\n",
      "timestep  157, reward 5.0, epsilon 0.9468612326345439\n",
      "timestep  158, reward 4.0, epsilon 0.9465772026696436\n",
      "timestep  159, reward 4.0, epsilon 0.9462932579052122\n",
      "timestep  160, reward 4.0, epsilon 0.9460093983156921\n",
      "timestep  161, reward 3.0, epsilon 0.9458202058961229\n",
      "timestep  162, reward 9.0, epsilon 0.9450638145081044\n",
      "timestep  163, reward 4.0, epsilon 0.9447803237147213\n",
      "timestep  164, reward 4.0, epsilon 0.9444969179600718\n",
      "timestep  165, reward 6.0, epsilon 0.9440247639413393\n",
      "timestep  166, reward 3.0, epsilon 0.9438359684287987\n",
      "timestep  167, reward 4.0, epsilon 0.9435528459524052\n",
      "timestep  168, reward 4.0, epsilon 0.9432698084042613\n",
      "timestep  169, reward 4.0, epsilon 0.942986855758891\n",
      "timestep  170, reward 5.0, epsilon 0.9426097175920269\n",
      "timestep  171, reward 4.0, epsilon 0.9423269629540982\n",
      "timestep  172, reward 4.0, epsilon 0.9420442931340786\n",
      "timestep  173, reward 5.0, epsilon 0.9416675319357145\n",
      "timestep  174, reward 6.0, epsilon 0.9411967923270838\n",
      "timestep  175, reward 4.0, epsilon 0.9409144615243482\n",
      "timestep  176, reward 3.0, epsilon 0.940726288041188\n",
      "timestep  177, reward 3.0, epsilon 0.9405381521908426\n",
      "timestep  178, reward 5.0, epsilon 0.9401619933584934\n",
      "timestep  179, reward 3.0, epsilon 0.9399739703614417\n",
      "timestep  180, reward 5.0, epsilon 0.9395980371679756\n",
      "timestep  181, reward 5.0, epsilon 0.9392222543252324\n",
      "timestep  182, reward 4.0, epsilon 0.9389405158246634\n",
      "timestep  183, reward 3.0, epsilon 0.9387527371109037\n",
      "timestep  184, reward 4.0, epsilon 0.9384711394514138\n",
      "timestep  185, reward 7.0, epsilon 0.9379081975196459\n",
      "timestep  186, reward 7.0, epsilon 0.9373455932686071\n",
      "timestep  187, reward 4.0, epsilon 0.937064417710057\n",
      "timestep  188, reward 3.0, epsilon 0.9368770141971592\n",
      "timestep  189, reward 4.0, epsilon 0.9365959791982735\n",
      "timestep  190, reward 5.0, epsilon 0.9362213969986066\n",
      "timestep  191, reward 4.0, epsilon 0.9359405586652128\n",
      "timestep  192, reward 7.0, epsilon 0.93537913470238\n",
      "timestep  193, reward 6.0, epsilon 0.934911538663589\n",
      "timestep  194, reward 4.0, epsilon 0.9346310932484012\n",
      "timestep  195, reward 5.0, epsilon 0.9342572968852291\n",
      "timestep  196, reward 5.0, epsilon 0.933883650018176\n",
      "timestep  197, reward 6.0, epsilon 0.9334168015721935\n",
      "timestep  198, reward 5.0, epsilon 0.9330434908528393\n",
      "timestep  199, reward 3.0, epsilon 0.9328568914851036\n",
      "timestep  200, reward 4.0, epsilon 0.9325770624024321\n",
      "timestep  201, reward 4.0, epsilon 0.9322973172600907\n",
      "timestep  202, reward 4.0, epsilon 0.9320176560329\n",
      "timestep  203, reward 5.0, epsilon 0.9316449048878183\n",
      "timestep  204, reward 5.0, epsilon 0.931272302820831\n",
      "timestep  205, reward 4.0, epsilon 0.9309929490672225\n",
      "timestep  206, reward 5.0, epsilon 0.9306206077434487\n",
      "timestep  207, reward 5.0, epsilon 0.9302484153338654\n",
      "timestep  208, reward 4.0, epsilon 0.9299693687157875\n",
      "timestep  209, reward 4.0, epsilon 0.9296904058033238\n",
      "timestep  210, reward 3.0, epsilon 0.9295044770190672\n",
      "timestep  211, reward 5.0, epsilon 0.9291327309948104\n",
      "timestep  212, reward 4.0, epsilon 0.9288540190485648\n",
      "timestep  213, reward 6.0, epsilon 0.9283896849151545\n",
      "timestep  214, reward 3.0, epsilon 0.9282040162620683\n",
      "timestep  215, reward 7.0, epsilon 0.9276472330643509\n",
      "timestep  216, reward 3.0, epsilon 0.9274617128942104\n",
      "timestep  217, reward 3.0, epsilon 0.9272762298262487\n",
      "timestep  218, reward 5.0, epsilon 0.926905374967183\n",
      "timestep  219, reward 4.0, epsilon 0.9266273311609271\n",
      "timestep  220, reward 3.0, epsilon 0.9264420149609683\n",
      "timestep  221, reward 4.0, epsilon 0.9261641101488141\n",
      "timestep  222, reward 4.0, epsilon 0.9258862886997666\n",
      "timestep  223, reward 4.0, epsilon 0.9256085505888195\n",
      "timestep  224, reward 4.0, epsilon 0.9253308957909738\n",
      "timestep  225, reward 8.0, epsilon 0.924683358451025\n",
      "timestep  226, reward 4.0, epsilon 0.9244059811830658\n",
      "timestep  227, reward 3.0, epsilon 0.9242211092308891\n",
      "timestep  228, reward 3.0, epsilon 0.924036274251254\n",
      "timestep  229, reward 4.0, epsilon 0.9237590910891429\n",
      "timestep  230, reward 4.0, epsilon 0.9234819910736651\n",
      "timestep  231, reward 5.0, epsilon 0.9231126536824614\n",
      "timestep  232, reward 7.0, epsilon 0.9225589245386892\n",
      "timestep  233, reward 4.0, epsilon 0.9222821845371728\n",
      "timestep  234, reward 5.0, epsilon 0.9219133269966001\n",
      "timestep  235, reward 5.0, epsilon 0.9215446169769135\n",
      "timestep  236, reward 4.0, epsilon 0.9212681812372374\n",
      "timestep  237, reward 10.0, epsilon 0.9204393714532942\n",
      "timestep  238, reward 4.0, epsilon 0.9201632672541189\n",
      "timestep  239, reward 4.0, epsilon 0.9198872458779206\n",
      "timestep  240, reward 4.0, epsilon 0.9196113072998547\n",
      "timestep  241, reward 4.0, epsilon 0.9193354514950844\n",
      "timestep  242, reward 5.0, epsilon 0.9189677724709363\n",
      "timestep  243, reward 4.0, epsilon 0.9186921097073092\n",
      "timestep  244, reward 3.0, epsilon 0.9185083804722889\n",
      "timestep  245, reward 6.0, epsilon 0.9180492181237063\n",
      "timestep  246, reward 3.0, epsilon 0.9178656174605737\n",
      "timestep  247, reward 4.0, epsilon 0.9175902853103862\n",
      "timestep  248, reward 4.0, epsilon 0.9173150357515841\n",
      "timestep  249, reward 5.0, epsilon 0.9169481647725165\n",
      "timestep  250, reward 4.0, epsilon 0.9166731078306128\n",
      "timestep  251, reward 4.0, epsilon 0.9163981333975403\n",
      "timestep  252, reward 3.0, epsilon 0.9162148629348421\n",
      "timestep  253, reward 5.0, epsilon 0.9158484319588951\n",
      "timestep  254, reward 6.0, epsilon 0.915390599318601\n",
      "timestep  255, reward 6.0, epsilon 0.9149329955488482\n",
      "timestep  256, reward 5.0, epsilon 0.9145670772429487\n",
      "timestep  257, reward 5.0, epsilon 0.914201305282418\n",
      "timestep  258, reward 7.0, epsilon 0.9136529216111619\n",
      "timestep  259, reward 4.0, epsilon 0.9133788531433524\n",
      "timestep  260, reward 4.0, epsilon 0.9131048668878616\n",
      "timestep  261, reward 4.0, epsilon 0.9128309628200283\n",
      "timestep  262, reward 3.0, epsilon 0.9126484057557739\n",
      "timestep  263, reward 6.0, epsilon 0.9121921728086106\n",
      "timestep  264, reward 4.0, epsilon 0.9119185425216211\n",
      "timestep  265, reward 5.0, epsilon 0.9115538298160775\n",
      "timestep  266, reward 4.0, epsilon 0.9112803910128361\n",
      "timestep  267, reward 4.0, epsilon 0.9110070342330328\n",
      "timestep  268, reward 3.0, epsilon 0.9108248419362565\n",
      "timestep  269, reward 3.0, epsilon 0.9106426860761176\n",
      "timestep  270, reward 3.0, epsilon 0.9104605666453293\n",
      "timestep  271, reward 4.0, epsilon 0.9101874557882422\n",
      "timestep  272, reward 4.0, epsilon 0.9099144268562193\n",
      "timestep  273, reward 6.0, epsilon 0.9094595606251352\n",
      "timestep  274, reward 6.0, epsilon 0.9090049217816846\n",
      "timestep  275, reward 3.0, epsilon 0.9088231298873775\n",
      "timestep  276, reward 5.0, epsilon 0.9084596551611751\n",
      "timestep  277, reward 5.0, epsilon 0.9080963258030562\n",
      "timestep  278, reward 3.0, epsilon 0.9079147156188588\n",
      "timestep  279, reward 5.0, epsilon 0.9075516042038627\n",
      "timestep  280, reward 5.0, epsilon 0.9071886380116474\n",
      "timestep  281, reward 4.0, epsilon 0.906916508634996\n",
      "timestep  282, reward 5.0, epsilon 0.9065537964429049\n",
      "timestep  283, reward 3.0, epsilon 0.9063724947491544\n",
      "timestep  284, reward 4.0, epsilon 0.9061006101909981\n",
      "timestep  285, reward 4.0, epsilon 0.905828807190053\n",
      "timestep  286, reward 3.0, epsilon 0.9056476504869031\n",
      "timestep  287, reward 4.0, epsilon 0.9053759833602809\n",
      "timestep  288, reward 3.0, epsilon 0.9051949172173688\n",
      "timestep  289, reward 4.0, epsilon 0.9049233858971459\n",
      "timestep  290, reward 4.0, epsilon 0.9046519360281735\n",
      "timestep  291, reward 3.0, epsilon 0.9044710146874873\n",
      "timestep  292, reward 3.0, epsilon 0.9042901295292599\n",
      "timestep  293, reward 6.0, epsilon 0.9038380748844658\n",
      "timestep  294, reward 3.0, epsilon 0.9036573163078697\n",
      "timestep  295, reward 4.0, epsilon 0.9033862462217932\n",
      "timestep  296, reward 3.0, epsilon 0.9032055780064113\n",
      "timestep  297, reward 4.0, epsilon 0.9029346434282735\n",
      "timestep  298, reward 4.0, epsilon 0.9026637901223813\n",
      "timestep  299, reward 4.0, epsilon 0.9023930180643557\n",
      "timestep  300, reward 4.0, epsilon 0.9021223272298245\n",
      "timestep  301, reward 4.0, epsilon 0.9018517175944234\n",
      "timestep  302, reward 5.0, epsilon 0.9014910310148815\n",
      "timestep  303, reward 8.0, epsilon 0.9008601765747386\n",
      "timestep  304, reward 5.0, epsilon 0.900499886552116\n",
      "timestep  305, reward 4.0, epsilon 0.9002297636002465\n",
      "timestep  306, reward 5.0, epsilon 0.8998697257049915\n",
      "timestep  307, reward 4.0, epsilon 0.899599791782472\n",
      "timestep  308, reward 6.0, epsilon 0.8991500818375644\n",
      "timestep  309, reward 3.0, epsilon 0.8989702608126977\n",
      "timestep  310, reward 5.0, epsilon 0.8986107266429926\n",
      "timestep  311, reward 3.0, epsilon 0.8984310134837713\n",
      "timestep  312, reward 4.0, epsilon 0.8981615111317581\n",
      "timestep  313, reward 4.0, epsilon 0.8978920896223659\n",
      "timestep  314, reward 4.0, epsilon 0.897622748931344\n",
      "timestep  315, reward 3.0, epsilon 0.8974432333577852\n",
      "timestep  316, reward 8.0, epsilon 0.8968152115261065\n",
      "timestep  317, reward 5.0, epsilon 0.8964565392468217\n",
      "timestep  318, reward 3.0, epsilon 0.8962772569035378\n",
      "timestep  319, reward 4.0, epsilon 0.8960084006138882\n",
      "timestep  320, reward 5.0, epsilon 0.8956500510105627\n",
      "timestep  321, reward 4.0, epsilon 0.8953813828638655\n",
      "timestep  322, reward 4.0, epsilon 0.8951127953095525\n",
      "timestep  323, reward 4.0, epsilon 0.8948442883234484\n",
      "timestep  324, reward 6.0, epsilon 0.8943969556547676\n",
      "timestep  325, reward 3.0, epsilon 0.8942180852076062\n",
      "timestep  326, reward 4.0, epsilon 0.8939498466076922\n",
      "timestep  327, reward 3.0, epsilon 0.8937710655778691\n",
      "timestep  328, reward 3.0, epsilon 0.8935923203024643\n",
      "timestep  329, reward 5.0, epsilon 0.8932349369863083\n",
      "timestep  330, reward 5.0, epsilon 0.8928776966020373\n",
      "timestep  331, reward 3.0, epsilon 0.8926991299914938\n",
      "timestep  332, reward 4.0, epsilon 0.8924313470325775\n",
      "timestep  333, reward 4.0, epsilon 0.8921636444005158\n",
      "timestep  334, reward 4.0, epsilon 0.8918960220712128\n",
      "timestep  335, reward 4.0, epsilon 0.8916284800205801\n",
      "timestep  336, reward 4.0, epsilon 0.8913610182245367\n",
      "timestep  337, reward 6.0, epsilon 0.8909154268426132\n",
      "timestep  338, reward 6.0, epsilon 0.8904700582118259\n",
      "timestep  339, reward 3.0, epsilon 0.8902919731048842\n",
      "timestep  340, reward 4.0, epsilon 0.8900249122208217\n",
      "timestep  341, reward 4.0, epsilon 0.8897579314470128\n",
      "timestep  342, reward 4.0, epsilon 0.8894910307594268\n",
      "timestep  343, reward 4.0, epsilon 0.8892242101340405\n",
      "timestep  344, reward 5.0, epsilon 0.8888685737998827\n",
      "timestep  345, reward 6.0, epsilon 0.888424228390952\n",
      "timestep  346, reward 4.0, epsilon 0.8881577277742732\n",
      "timestep  347, reward 7.0, epsilon 0.887624966343506\n",
      "timestep  348, reward 3.0, epsilon 0.887447450226487\n",
      "timestep  349, reward 8.0, epsilon 0.8868264233442354\n",
      "timestep  350, reward 4.0, epsilon 0.8865604020211381\n",
      "timestep  351, reward 4.0, epsilon 0.8862944604964572\n",
      "timestep  352, reward 5.0, epsilon 0.8859399958863813\n",
      "timestep  353, reward 5.0, epsilon 0.8855856730408828\n",
      "timestep  354, reward 4.0, epsilon 0.8853200239056551\n",
      "timestep  355, reward 4.0, epsilon 0.8850544544571989\n",
      "timestep  356, reward 5.0, epsilon 0.8847004857751432\n",
      "timestep  357, reward 3.0, epsilon 0.884523554524993\n",
      "timestep  358, reward 5.0, epsilon 0.8841697981710582\n",
      "timestep  359, reward 5.0, epsilon 0.8838161832984411\n",
      "timestep  360, reward 9.0, epsilon 0.8831093777708461\n",
      "timestep  361, reward 4.0, epsilon 0.8828444714499131\n",
      "timestep  362, reward 5.0, epsilon 0.8824913866284702\n",
      "timestep  363, reward 3.0, epsilon 0.8823148971760584\n",
      "timestep  364, reward 4.0, epsilon 0.8820502291754702\n",
      "timestep  365, reward 3.0, epsilon 0.8818738279501374\n",
      "timestep  366, reward 5.0, epsilon 0.8815211313278596\n",
      "timestep  367, reward 6.0, epsilon 0.8810804589054941\n",
      "timestep  368, reward 5.0, epsilon 0.8807280795832353\n",
      "timestep  369, reward 4.0, epsilon 0.880463887580322\n",
      "timestep  370, reward 7.0, epsilon 0.879935741299749\n",
      "timestep  371, reward 3.0, epsilon 0.8797597629508466\n",
      "timestep  372, reward 4.0, epsilon 0.8794958614138745\n",
      "timestep  373, reward 4.0, epsilon 0.8792320390394467\n",
      "timestep  374, reward 7.0, epsilon 0.8787046316832456\n",
      "timestep  375, reward 5.0, epsilon 0.8783532025493357\n",
      "timestep  376, reward 4.0, epsilon 0.8780897229382887\n",
      "timestep  377, reward 3.0, epsilon 0.8779141137745984\n",
      "timestep  378, reward 5.0, epsilon 0.8775630008004238\n",
      "timestep  379, reward 5.0, epsilon 0.8772120282503735\n",
      "timestep  380, reward 5.0, epsilon 0.8768611960682863\n",
      "timestep  381, reward 6.0, epsilon 0.8764228531476036\n",
      "timestep  382, reward 6.0, epsilon 0.8759847293545513\n",
      "timestep  383, reward 5.0, epsilon 0.8756343880183893\n",
      "timestep  384, reward 4.0, epsilon 0.87537172397014\n",
      "timestep  385, reward 4.0, epsilon 0.8751091387132254\n",
      "timestep  386, reward 4.0, epsilon 0.8748466322240105\n",
      "timestep  387, reward 3.0, epsilon 0.874671671646032\n",
      "timestep  388, reward 3.0, epsilon 0.8744967460584195\n",
      "timestep  389, reward 3.0, epsilon 0.8743218554541753\n",
      "timestep  390, reward 5.0, epsilon 0.8739721791678078\n",
      "timestep  391, reward 5.0, epsilon 0.8736226427309757\n",
      "timestep  392, reward 6.0, epsilon 0.8731859187631387\n",
      "timestep  393, reward 7.0, epsilon 0.8726621381723062\n",
      "timestep  394, reward 3.0, epsilon 0.8724876144712931\n",
      "timestep  395, reward 3.0, epsilon 0.8723131256732749\n",
      "timestep  396, reward 4.0, epsilon 0.8720514579040944\n",
      "timestep  397, reward 3.0, epsilon 0.8718770563330283\n",
      "timestep  398, reward 4.0, epsilon 0.8716155193715681\n",
      "timestep  399, reward 3.0, epsilon 0.871441204983849\n",
      "timestep  400, reward 4.0, epsilon 0.8711797987647186\n",
      "timestep  401, reward 3.0, epsilon 0.8710055715167637\n",
      "timestep  402, reward 4.0, epsilon 0.8707442959746048\n",
      "timestep  403, reward 4.0, epsilon 0.8704830988072706\n",
      "timestep  404, reward 3.0, epsilon 0.8703090108923401\n",
      "timestep  405, reward 7.0, epsilon 0.8697869560147516\n",
      "timestep  406, reward 4.0, epsilon 0.8695260460206861\n",
      "timestep  407, reward 3.0, epsilon 0.8693521495067424\n",
      "timestep  408, reward 3.0, epsilon 0.8691782877703627\n",
      "timestep  409, reward 7.0, epsilon 0.8686569111570615\n",
      "timestep  410, reward 3.0, epsilon 0.8684831884613992\n",
      "timestep  411, reward 5.0, epsilon 0.8681358472915321\n",
      "timestep  412, reward 6.0, epsilon 0.8677018661727902\n",
      "timestep  413, reward 8.0, epsilon 0.8670946570534946\n",
      "timestep  414, reward 4.0, epsilon 0.8668345546683512\n",
      "timestep  415, reward 3.0, epsilon 0.8666611964257631\n",
      "timestep  416, reward 6.0, epsilon 0.8662279524850037\n",
      "timestep  417, reward 3.0, epsilon 0.8660547155567863\n",
      "timestep  418, reward 4.0, epsilon 0.8657949251228946\n",
      "timestep  419, reward 3.0, epsilon 0.8656217747958194\n",
      "timestep  420, reward 4.0, epsilon 0.8653621142311683\n",
      "timestep  421, reward 4.0, epsilon 0.8651025315568971\n",
      "timestep  422, reward 9.0, epsilon 0.8644106917119208\n",
      "timestep  423, reward 3.0, epsilon 0.8642378182176853\n",
      "timestep  424, reward 3.0, epsilon 0.8640649792964199\n",
      "timestep  425, reward 7.0, epsilon 0.8635466699013089\n",
      "timestep  426, reward 5.0, epsilon 0.8632013030426944\n",
      "timestep  427, reward 6.0, epsilon 0.8627697887026718\n",
      "timestep  428, reward 4.0, epsilon 0.8625109836482919\n",
      "timestep  429, reward 3.0, epsilon 0.8623384900766722\n",
      "timestep  430, reward 4.0, epsilon 0.8620798143989417\n",
      "timestep  431, reward 4.0, epsilon 0.8618212163161544\n",
      "timestep  432, reward 4.0, epsilon 0.8615626958050343\n",
      "timestep  433, reward 4.0, epsilon 0.861304252842312\n",
      "timestep  434, reward 4.0, epsilon 0.8610458874047257\n",
      "timestep  435, reward 5.0, epsilon 0.8607015207090729\n",
      "timestep  436, reward 4.0, epsilon 0.8604433360730452\n",
      "timestep  437, reward 4.0, epsilon 0.860185228884663\n",
      "timestep  438, reward 6.0, epsilon 0.8597552222801421\n",
      "timestep  439, reward 4.0, epsilon 0.859497321505255\n",
      "timestep  440, reward 4.0, epsilon 0.8592394980928637\n",
      "timestep  441, reward 4.0, epsilon 0.8589817520197615\n",
      "timestep  442, reward 5.0, epsilon 0.8586382108544228\n",
      "timestep  443, reward 4.0, epsilon 0.8583806451494542\n",
      "timestep  444, reward 5.0, epsilon 0.8580373443907998\n",
      "timestep  445, reward 4.0, epsilon 0.8577799589277448\n",
      "timestep  446, reward 6.0, epsilon 0.8573511547176996\n",
      "timestep  447, reward 4.0, epsilon 0.8570939750909616\n",
      "timestep  448, reward 3.0, epsilon 0.8569225648668832\n",
      "timestep  449, reward 5.0, epsilon 0.8565798472528627\n",
      "timestep  450, reward 4.0, epsilon 0.8563228989952257\n",
      "timestep  451, reward 4.0, epsilon 0.8560660278143578\n",
      "timestep  452, reward 6.0, epsilon 0.8556380803984931\n",
      "timestep  453, reward 3.0, epsilon 0.8554669613387943\n",
      "timestep  454, reward 4.0, epsilon 0.855210346913546\n",
      "timestep  455, reward 4.0, epsilon 0.854953809464927\n",
      "timestep  456, reward 4.0, epsilon 0.854697348969847\n",
      "timestep  457, reward 3.0, epsilon 0.8545264180470264\n",
      "timestep  458, reward 4.0, epsilon 0.8542700857565504\n",
      "timestep  459, reward 4.0, epsilon 0.8540138303580718\n",
      "timestep  460, reward 6.0, epsilon 0.8535869088357361\n",
      "timestep  461, reward 3.0, epsilon 0.8534161999898381\n",
      "timestep  462, reward 5.0, epsilon 0.8530748847114006\n",
      "timestep  463, reward 6.0, epsilon 0.8526484325680032\n",
      "timestep  464, reward 3.0, epsilon 0.8524779114079739\n",
      "timestep  465, reward 4.0, epsilon 0.8522221936080363\n",
      "timestep  466, reward 4.0, epsilon 0.8519665525157675\n",
      "timestep  467, reward 3.0, epsilon 0.8517961677249299\n",
      "timestep  468, reward 4.0, epsilon 0.8515406544276458\n",
      "timestep  469, reward 4.0, epsilon 0.8512852177766856\n",
      "timestep  470, reward 3.0, epsilon 0.8511149692459825\n",
      "timestep  471, reward 5.0, epsilon 0.8507745743217779\n",
      "timestep  472, reward 4.0, epsilon 0.850519367471868\n",
      "timestep  473, reward 6.0, epsilon 0.8500941928315641\n",
      "timestep  474, reward 3.0, epsilon 0.8499241824939398\n",
      "timestep  475, reward 4.0, epsilon 0.8496692307360671\n",
      "timestep  476, reward 7.0, epsilon 0.849159556631018\n",
      "timestep  477, reward 5.0, epsilon 0.8488199437545426\n",
      "timestep  478, reward 4.0, epsilon 0.8485653232351658\n",
      "timestep  479, reward 5.0, epsilon 0.848225948016397\n",
      "timestep  480, reward 4.0, epsilon 0.8479715056779223\n",
      "timestep  481, reward 4.0, epsilon 0.8477171396645162\n",
      "timestep  482, reward 4.0, epsilon 0.8474628499532834\n",
      "timestep  483, reward 3.0, epsilon 0.8472933658579213\n",
      "timestep  484, reward 4.0, epsilon 0.8470392032661177\n",
      "timestep  485, reward 5.0, epsilon 0.8467004384037754\n",
      "timestep  486, reward 5.0, epsilon 0.8463618090270535\n",
      "timestep  487, reward 4.0, epsilon 0.8461079258743534\n",
      "timestep  488, reward 3.0, epsilon 0.8459387127502577\n",
      "timestep  489, reward 4.0, epsilon 0.845684956513748\n",
      "timestep  490, reward 4.0, epsilon 0.8454312763964968\n",
      "timestep  491, reward 4.0, epsilon 0.8451776723756708\n",
      "timestep  492, reward 3.0, epsilon 0.8450086452929724\n",
      "timestep  493, reward 6.0, epsilon 0.8445862254627408\n",
      "timestep  494, reward 4.0, epsilon 0.8443328749318442\n",
      "timestep  495, reward 3.0, epsilon 0.8441640168001866\n",
      "timestep  496, reward 4.0, epsilon 0.8439107929192229\n",
      "timestep  497, reward 3.0, epsilon 0.843742019199747\n",
      "timestep  498, reward 5.0, epsilon 0.8434045730132134\n",
      "timestep  499, reward 4.0, epsilon 0.8431515769426032\n",
      "timestep  500, reward 4.0, epsilon 0.8428986567632247\n",
      "timestep  501, reward 5.0, epsilon 0.8425615478710674\n",
      "timestep  502, reward 4.0, epsilon 0.84230880468271\n",
      "timestep  503, reward 5.0, epsilon 0.8419719316959959\n",
      "timestep  504, reward 4.0, epsilon 0.8417193653748032\n",
      "timestep  505, reward 6.0, epsilon 0.8412985898556357\n",
      "timestep  506, reward 6.0, epsilon 0.8408780246821543\n",
      "timestep  507, reward 4.0, epsilon 0.8406257865002497\n",
      "timestep  508, reward 5.0, epsilon 0.8402895866198343\n",
      "timestep  509, reward 5.0, epsilon 0.8399535211992005\n",
      "timestep  510, reward 5.0, epsilon 0.8396175901845725\n",
      "timestep  511, reward 6.0, epsilon 0.8391978653428435\n",
      "timestep  512, reward 6.0, epsilon 0.8387783503215669\n",
      "timestep  513, reward 3.0, epsilon 0.838610603039286\n",
      "timestep  514, reward 4.0, epsilon 0.8383590450158538\n",
      "timestep  515, reward 4.0, epsilon 0.8381075624522821\n",
      "timestep  516, reward 6.0, epsilon 0.8376885924734316\n",
      "timestep  517, reward 4.0, epsilon 0.8374373110255098\n",
      "timestep  518, reward 4.0, epsilon 0.837186104954484\n",
      "timestep  519, reward 4.0, epsilon 0.8369349742377437\n",
      "timestep  520, reward 5.0, epsilon 0.8366002504607994\n",
      "timestep  521, reward 4.0, epsilon 0.8363492954828321\n",
      "timestep  522, reward 8.0, epsilon 0.8357640265800769\n",
      "timestep  523, reward 4.0, epsilon 0.8355133224441879\n",
      "timestep  524, reward 4.0, epsilon 0.8352626935120188\n",
      "timestep  525, reward 4.0, epsilon 0.8350121397610107\n",
      "timestep  526, reward 4.0, epsilon 0.8347616611686117\n",
      "timestep  527, reward 9.0, epsilon 0.8340940855262012\n",
      "timestep  528, reward 3.0, epsilon 0.8339272750500369\n",
      "timestep  529, reward 5.0, epsilon 0.8335937541723177\n",
      "timestep  530, reward 4.0, epsilon 0.8333437010530451\n",
      "timestep  531, reward 4.0, epsilon 0.8330937229422068\n",
      "timestep  532, reward 3.0, epsilon 0.8329271125285557\n",
      "timestep  533, reward 5.0, epsilon 0.8325939916558395\n",
      "timestep  534, reward 4.0, epsilon 0.8323442384353299\n",
      "timestep  535, reward 5.0, epsilon 0.8320113506772808\n",
      "timestep  536, reward 3.0, epsilon 0.8318449567272589\n",
      "timestep  537, reward 5.0, epsilon 0.8315122686519382\n",
      "timestep  538, reward 4.0, epsilon 0.8312628399158791\n",
      "timestep  539, reward 4.0, epsilon 0.8310134860009584\n",
      "timestep  540, reward 4.0, epsilon 0.8307642068847316\n",
      "timestep  541, reward 5.0, epsilon 0.8304319510445072\n",
      "timestep  542, reward 4.0, epsilon 0.8301828463713219\n",
      "timestep  543, reward 4.0, epsilon 0.8299338164220658\n",
      "timestep  544, reward 3.0, epsilon 0.8297678379581195\n",
      "timestep  545, reward 3.0, epsilon 0.8296018926882063\n",
      "timestep  546, reward 11.0, epsilon 0.828772664016835\n",
      "timestep  547, reward 4.0, epsilon 0.8285240570799811\n",
      "timestep  548, reward 6.0, epsilon 0.828109877895562\n",
      "timestep  549, reward 5.0, epsilon 0.8277786836276841\n",
      "timestep  550, reward 3.0, epsilon 0.8276131361687454\n",
      "timestep  551, reward 4.0, epsilon 0.8273648770554614\n",
      "timestep  552, reward 3.0, epsilon 0.8271994123536991\n",
      "timestep  553, reward 5.0, epsilon 0.8268685822174137\n",
      "timestep  554, reward 4.0, epsilon 0.826620546447979\n",
      "timestep  555, reward 3.0, epsilon 0.8264552306048949\n",
      "timestep  556, reward 4.0, epsilon 0.8262073188285439\n",
      "timestep  557, reward 3.0, epsilon 0.8260420856268514\n",
      "timestep  558, reward 4.0, epsilon 0.8257942977816\n",
      "timestep  559, reward 5.0, epsilon 0.8254640296068421\n",
      "timestep  560, reward 5.0, epsilon 0.8251338935195394\n",
      "timestep  561, reward 4.0, epsilon 0.8248863781046752\n",
      "timestep  562, reward 4.0, epsilon 0.8246389369370104\n",
      "timestep  563, reward 5.0, epsilon 0.8243091308372733\n",
      "timestep  564, reward 4.0, epsilon 0.8240618628264719\n",
      "timestep  565, reward 6.0, epsilon 0.8236499142930047\n",
      "timestep  566, reward 5.0, epsilon 0.8233205037429878\n",
      "timestep  567, reward 4.0, epsilon 0.8230735322906567\n",
      "timestep  568, reward 4.0, epsilon 0.8228266349223525\n",
      "timestep  569, reward 5.0, epsilon 0.8224975536346905\n",
      "timestep  570, reward 6.0, epsilon 0.822086387099404\n",
      "timestep  571, reward 4.0, epsilon 0.8218397858450438\n",
      "timestep  572, reward 5.0, epsilon 0.8215110992378057\n",
      "timestep  573, reward 4.0, epsilon 0.8212646705525458\n",
      "timestep  574, reward 5.0, epsilon 0.8209362139569201\n",
      "timestep  575, reward 4.0, epsilon 0.8206899577199985\n",
      "timestep  576, reward 6.0, epsilon 0.8202796948019279\n",
      "timestep  577, reward 4.0, epsilon 0.8200336355010579\n",
      "timestep  578, reward 4.0, epsilon 0.8197876500105966\n",
      "timestep  579, reward 5.0, epsilon 0.8194597841345723\n",
      "timestep  580, reward 5.0, epsilon 0.8191320493852278\n",
      "timestep  581, reward 4.0, epsilon 0.8188863343435545\n",
      "timestep  582, reward 5.0, epsilon 0.8185588289397216\n",
      "timestep  583, reward 4.0, epsilon 0.818313285846986\n",
      "timestep  584, reward 3.0, epsilon 0.8181496313729496\n",
      "timestep  585, reward 3.0, epsilon 0.8179860096281714\n",
      "timestep  586, reward 3.0, epsilon 0.8178224206061059\n",
      "timestep  587, reward 5.0, epsilon 0.8174953407039375\n",
      "timestep  588, reward 5.0, epsilon 0.8171683916141065\n",
      "timestep  589, reward 3.0, epsilon 0.8170049661074676\n",
      "timestep  590, reward 4.0, epsilon 0.8167598891269674\n",
      "timestep  591, reward 6.0, epsilon 0.8163515908502257\n",
      "timestep  592, reward 4.0, epsilon 0.8161067098627021\n",
      "timestep  593, reward 4.0, epsilon 0.8158619023321285\n",
      "timestep  594, reward 5.0, epsilon 0.8155356065196465\n",
      "timestep  595, reward 4.0, epsilon 0.8152909703029433\n",
      "timestep  596, reward 4.0, epsilon 0.8150464074697662\n",
      "timestep  597, reward 7.0, epsilon 0.8145575018659459\n",
      "timestep  598, reward 4.0, epsilon 0.8143131590512966\n",
      "timestep  599, reward 4.0, epsilon 0.8140688895321618\n",
      "timestep  600, reward 4.0, epsilon 0.8138246932865548\n",
      "timestep  601, reward 4.0, epsilon 0.8135805702924959\n",
      "timestep  602, reward 4.0, epsilon 0.8133365205280116\n",
      "timestep  603, reward 4.0, epsilon 0.8130925439711356\n",
      "timestep  604, reward 3.0, epsilon 0.8129299335932668\n",
      "timestep  605, reward 4.0, epsilon 0.8126860790002739\n",
      "timestep  606, reward 4.0, epsilon 0.8124422975563436\n",
      "timestep  607, reward 5.0, epsilon 0.8121173693806092\n",
      "timestep  608, reward 4.0, epsilon 0.811873758532504\n",
      "timestep  609, reward 4.0, epsilon 0.8116302207603451\n",
      "timestep  610, reward 3.0, epsilon 0.8114679028324951\n",
      "timestep  611, reward 4.0, epsilon 0.811224486804871\n",
      "timestep  612, reward 5.0, epsilon 0.8109000456803733\n",
      "timestep  613, reward 4.0, epsilon 0.8106567999928598\n",
      "timestep  614, reward 4.0, epsilon 0.8104136272717553\n",
      "timestep  615, reward 4.0, epsilon 0.8101705274951723\n",
      "timestep  616, reward 7.0, epsilon 0.8096845466880521\n",
      "timestep  617, reward 5.0, epsilon 0.8093607214472112\n",
      "timestep  618, reward 4.0, epsilon 0.8091179375107892\n",
      "timestep  619, reward 3.0, epsilon 0.8089561220144664\n",
      "timestep  620, reward 3.0, epsilon 0.8087943388796247\n",
      "timestep  621, reward 5.0, epsilon 0.8084708696684981\n",
      "timestep  622, reward 4.0, epsilon 0.8082283526609152\n",
      "timestep  623, reward 4.0, epsilon 0.8079859084011592\n",
      "timestep  624, reward 4.0, epsilon 0.8077435368674083\n",
      "timestep  625, reward 6.0, epsilon 0.8073397458652514\n",
      "timestep  626, reward 3.0, epsilon 0.8071782859894758\n",
      "timestep  627, reward 4.0, epsilon 0.8069361567182204\n",
      "timestep  628, reward 3.0, epsilon 0.8067747775562383\n",
      "timestep  629, reward 4.0, epsilon 0.806532769325408\n",
      "timestep  630, reward 7.0, epsilon 0.8060489706275987\n",
      "timestep  631, reward 4.0, epsilon 0.8058071801170735\n",
      "timestep  632, reward 7.0, epsilon 0.8053238166639655\n",
      "timestep  633, reward 4.0, epsilon 0.8050822436778755\n",
      "timestep  634, reward 5.0, epsilon 0.8047602590821188\n",
      "timestep  635, reward 4.0, epsilon 0.8045188551463972\n",
      "timestep  636, reward 3.0, epsilon 0.8043579594205564\n",
      "timestep  637, reward 5.0, epsilon 0.8040362644950484\n",
      "timestep  638, reward 3.0, epsilon 0.8038754652825121\n",
      "timestep  639, reward 7.0, epsilon 0.8033932605685862\n",
      "timestep  640, reward 7.0, epsilon 0.8029113451051674\n",
      "timestep  641, reward 3.0, epsilon 0.8027507708652598\n",
      "timestep  642, reward 6.0, epsilon 0.8023494757468772\n",
      "timestep  643, reward 4.0, epsilon 0.802108794973835\n",
      "timestep  644, reward 3.0, epsilon 0.8019483812359283\n",
      "timestep  645, reward 5.0, epsilon 0.8016276499971291\n",
      "timestep  646, reward 3.0, epsilon 0.8014673324834062\n",
      "timestep  647, reward 3.0, epsilon 0.8013070470315828\n",
      "timestep  648, reward 3.0, epsilon 0.801146793635247\n",
      "timestep  649, reward 5.0, epsilon 0.800826382983396\n",
      "timestep  650, reward 4.0, epsilon 0.8005861590924918\n",
      "timestep  651, reward 4.0, epsilon 0.8003460072615483\n",
      "timestep  652, reward 3.0, epsilon 0.8001859460635561\n",
      "timestep  653, reward 5.0, epsilon 0.7998659196930867\n",
      "timestep  654, reward 3.0, epsilon 0.7997059545078073\n",
      "timestep  655, reward 4.0, epsilon 0.799466066711834\n",
      "timestep  656, reward 4.0, epsilon 0.799226250875003\n",
      "timestep  657, reward 4.0, epsilon 0.7989865069757288\n",
      "timestep  658, reward 4.0, epsilon 0.7987468349924324\n",
      "timestep  659, reward 4.0, epsilon 0.7985072349035409\n",
      "timestep  660, reward 4.0, epsilon 0.7982677066874884\n",
      "timestep  661, reward 4.0, epsilon 0.7980282503227151\n",
      "timestep  662, reward 8.0, epsilon 0.7974697981054936\n",
      "timestep  663, reward 3.0, epsilon 0.7973103121205705\n",
      "timestep  664, reward 7.0, epsilon 0.7968320455139001\n",
      "timestep  665, reward 4.0, epsilon 0.7965930198044103\n",
      "timestep  666, reward 3.0, epsilon 0.7964337091663797\n",
      "timestep  667, reward 5.0, epsilon 0.7961151834655501\n",
      "timestep  668, reward 5.0, epsilon 0.7957967851558905\n",
      "timestep  669, reward 5.0, epsilon 0.7954785141864521\n",
      "timestep  670, reward 6.0, epsilon 0.795080854469256\n",
      "timestep  671, reward 5.0, epsilon 0.7947628698291394\n",
      "timestep  672, reward 6.0, epsilon 0.7943655678625645\n",
      "timestep  673, reward 4.0, epsilon 0.7941272820223785\n",
      "timestep  674, reward 4.0, epsilon 0.7938890676607961\n",
      "timestep  675, reward 5.0, epsilon 0.7935715596639005\n",
      "timestep  676, reward 6.0, epsilon 0.7931748532332892\n",
      "timestep  677, reward 3.0, epsilon 0.793016226194391\n",
      "timestep  678, reward 3.0, epsilon 0.7928576308793145\n",
      "timestep  679, reward 5.0, epsilon 0.7925405353952494\n",
      "timestep  680, reward 4.0, epsilon 0.7923027970100543\n",
      "timestep  681, reward 4.0, epsilon 0.7920651299392429\n",
      "timestep  682, reward 5.0, epsilon 0.7917483514080069\n",
      "timestep  683, reward 4.0, epsilon 0.7915108506542433\n",
      "timestep  684, reward 5.0, epsilon 0.7911942938014667\n",
      "timestep  685, reward 4.0, epsilon 0.7909569592483638\n",
      "timestep  686, reward 5.0, epsilon 0.7906406239189183\n",
      "timestep  687, reward 4.0, epsilon 0.7904034554501707\n",
      "timestep  688, reward 5.0, epsilon 0.7900873414890366\n",
      "timestep  689, reward 6.0, epsilon 0.7896923768191259\n",
      "timestep  690, reward 7.0, epsilon 0.7892186798310983\n",
      "timestep  691, reward 5.0, epsilon 0.7889030397091299\n",
      "timestep  692, reward 4.0, epsilon 0.7886663924635195\n",
      "timestep  693, reward 5.0, epsilon 0.7883509732233631\n",
      "timestep  694, reward 7.0, epsilon 0.7878780808763094\n",
      "timestep  695, reward 3.0, epsilon 0.7877205131389149\n",
      "timestep  696, reward 5.0, epsilon 0.7874054721937392\n",
      "timestep  697, reward 4.0, epsilon 0.787169274173458\n",
      "timestep  698, reward 9.0, epsilon 0.78653975911744\n",
      "timestep  699, reward 3.0, epsilon 0.7863824590310141\n",
      "timestep  700, reward 4.0, epsilon 0.7861465678839923\n",
      "timestep  701, reward 4.0, epsilon 0.7859107474972381\n",
      "timestep  702, reward 4.0, epsilon 0.7856749978495254\n",
      "timestep  703, reward 5.0, epsilon 0.7853607749877429\n",
      "timestep  704, reward 3.0, epsilon 0.7852037106863532\n",
      "timestep  705, reward 4.0, epsilon 0.7849681731284734\n",
      "timestep  706, reward 3.0, epsilon 0.7848111873435295\n",
      "timestep  707, reward 5.0, epsilon 0.7844973099541243\n",
      "timestep  708, reward 6.0, epsilon 0.7841051397410336\n",
      "timestep  709, reward 4.0, epsilon 0.7838699317214814\n",
      "timestep  710, reward 6.0, epsilon 0.7834780751347756\n",
      "timestep  711, reward 4.0, epsilon 0.783243055215794\n",
      "timestep  712, reward 3.0, epsilon 0.7830864144371814\n",
      "timestep  713, reward 5.0, epsilon 0.7827732268534591\n",
      "timestep  714, reward 6.0, epsilon 0.7823819185095278\n",
      "timestep  715, reward 4.0, epsilon 0.78214722740465\n",
      "timestep  716, reward 5.0, epsilon 0.7818344154393934\n",
      "timestep  717, reward 9.0, epsilon 0.781209166776901\n",
      "timestep  718, reward 4.0, epsilon 0.7809748274623617\n",
      "timestep  719, reward 3.0, epsilon 0.7808186403066175\n",
      "timestep  720, reward 4.0, epsilon 0.7805844181383039\n",
      "timestep  721, reward 4.0, epsilon 0.7803502662296143\n",
      "timestep  722, reward 4.0, epsilon 0.7801161845594731\n",
      "timestep  723, reward 4.0, epsilon 0.7798821731068107\n",
      "timestep  724, reward 4.0, epsilon 0.7796482318505641\n",
      "timestep  725, reward 4.0, epsilon 0.7794143607696763\n",
      "timestep  726, reward 6.0, epsilon 0.7790247315229338\n",
      "timestep  727, reward 4.0, epsilon 0.7787910474734399\n",
      "timestep  728, reward 4.0, epsilon 0.7785574335221505\n",
      "timestep  729, reward 4.0, epsilon 0.7783238896480383\n",
      "timestep  730, reward 3.0, epsilon 0.7781682326533477\n",
      "timestep  731, reward 4.0, epsilon 0.7779348055278205\n",
      "timestep  732, reward 3.0, epsilon 0.7777792263460629\n",
      "timestep  733, reward 4.0, epsilon 0.7775459159107581\n",
      "timestep  734, reward 3.0, epsilon 0.777390414503035\n",
      "timestep  735, reward 4.0, epsilon 0.7771572206996192\n",
      "timestep  736, reward 5.0, epsilon 0.7768464044376642\n",
      "timestep  737, reward 4.0, epsilon 0.7766133738209482\n",
      "timestep  738, reward 4.0, epsilon 0.7763804131064266\n",
      "timestep  739, reward 3.0, epsilon 0.7762251447876094\n",
      "timestep  740, reward 5.0, epsilon 0.7759147013000982\n",
      "timestep  741, reward 4.0, epsilon 0.7756819501663733\n",
      "timestep  742, reward 6.0, epsilon 0.7752941867517288\n",
      "timestep  743, reward 4.0, epsilon 0.7750616217537537\n",
      "timestep  744, reward 3.0, epsilon 0.7749066171800192\n",
      "timestep  745, reward 5.0, epsilon 0.7745967010244447\n",
      "timestep  746, reward 4.0, epsilon 0.7743643452512637\n",
      "timestep  747, reward 4.0, epsilon 0.7741320591778444\n",
      "timestep  748, reward 4.0, epsilon 0.7738998427832787\n",
      "timestep  749, reward 3.0, epsilon 0.7737450705537205\n",
      "timestep  750, reward 4.0, epsilon 0.7735129702441328\n",
      "timestep  751, reward 5.0, epsilon 0.7732036114637194\n",
      "timestep  752, reward 4.0, epsilon 0.7729716735756155\n",
      "timestep  753, reward 8.0, epsilon 0.7724307557011127\n",
      "timestep  754, reward 4.0, epsilon 0.7721990496465526\n",
      "timestep  755, reward 4.0, epsilon 0.7719674130968579\n",
      "timestep  756, reward 4.0, epsilon 0.7717358460311793\n",
      "timestep  757, reward 5.0, epsilon 0.7714271979938307\n",
      "timestep  758, reward 5.0, epsilon 0.7711186733971794\n",
      "timestep  759, reward 3.0, epsilon 0.7709644573736867\n",
      "timestep  760, reward 3.0, epsilon 0.7708102721918565\n",
      "timestep  761, reward 3.0, epsilon 0.7706561178455209\n",
      "timestep  762, reward 5.0, epsilon 0.7703479016346673\n",
      "timestep  763, reward 4.0, epsilon 0.7701168203738437\n",
      "timestep  764, reward 4.0, epsilon 0.7698858084304661\n",
      "timestep  765, reward 4.0, epsilon 0.7696548657837413\n",
      "timestep  766, reward 4.0, epsilon 0.7694239924128826\n",
      "timestep  767, reward 4.0, epsilon 0.7691931882971091\n",
      "timestep  768, reward 4.0, epsilon 0.7689624534156465\n",
      "timestep  769, reward 5.0, epsilon 0.7686549145689517\n",
      "timestep  770, reward 5.0, epsilon 0.7683474987193445\n",
      "timestep  771, reward 4.0, epsilon 0.7681170175193853\n",
      "timestep  772, reward 4.0, epsilon 0.767886605456872\n",
      "timestep  773, reward 4.0, epsilon 0.7676562625110652\n",
      "timestep  774, reward 5.0, epsilon 0.767349246062366\n",
      "timestep  775, reward 6.0, epsilon 0.7669656481665864\n",
      "timestep  776, reward 4.0, epsilon 0.7667355814803388\n",
      "timestep  777, reward 4.0, epsilon 0.7665055838071955\n",
      "timestep  778, reward 3.0, epsilon 0.7663522903554899\n",
      "timestep  779, reward 3.0, epsilon 0.7661990275609417\n",
      "timestep  780, reward 4.0, epsilon 0.7659691908378781\n",
      "timestep  781, reward 4.0, epsilon 0.7657394230589364\n",
      "timestep  782, reward 5.0, epsilon 0.7654331732310153\n",
      "timestep  783, reward 3.0, epsilon 0.7652800942507009\n",
      "timestep  784, reward 4.0, epsilon 0.7650505331800633\n",
      "timestep  785, reward 5.0, epsilon 0.7647445588667632\n",
      "timestep  786, reward 6.0, epsilon 0.7643622630541387\n",
      "timestep  787, reward 3.0, epsilon 0.7642093982451506\n",
      "timestep  788, reward 5.0, epsilon 0.7639037603353596\n",
      "timestep  789, reward 6.0, epsilon 0.7635218848379293\n",
      "timestep  790, reward 4.0, epsilon 0.763292851177371\n",
      "timestep  791, reward 5.0, epsilon 0.7629875798314182\n",
      "timestep  792, reward 4.0, epsilon 0.7627587064463331\n",
      "timestep  793, reward 4.0, epsilon 0.7625299017163978\n",
      "timestep  794, reward 4.0, epsilon 0.7623011656210174\n",
      "timestep  795, reward 5.0, epsilon 0.7619962908897898\n",
      "timestep  796, reward 5.0, epsilon 0.7616915380901634\n",
      "timestep  797, reward 3.0, epsilon 0.7615392073994608\n",
      "timestep  798, reward 4.0, epsilon 0.7613107684826557\n",
      "timestep  799, reward 4.0, epsilon 0.7610823980906727\n",
      "timestep  800, reward 3.0, epsilon 0.7609301892218785\n",
      "timestep  801, reward 3.0, epsilon 0.760778010793336\n",
      "timestep  802, reward 4.0, epsilon 0.7605498002126775\n",
      "timestep  803, reward 6.0, epsilon 0.760169601359946\n",
      "timestep  804, reward 4.0, epsilon 0.7599415732838659\n",
      "timestep  805, reward 3.0, epsilon 0.7597895925686249\n",
      "timestep  806, reward 6.0, epsilon 0.7594097737437023\n",
      "timestep  807, reward 4.0, epsilon 0.759181973593113\n",
      "timestep  808, reward 7.0, epsilon 0.7587265782710707\n",
      "timestep  809, reward 4.0, epsilon 0.7584989830586281\n",
      "timestep  810, reward 5.0, epsilon 0.7581956289723096\n",
      "timestep  811, reward 4.0, epsilon 0.7579681930287285\n",
      "timestep  812, reward 4.0, epsilon 0.7577408253091078\n",
      "timestep  813, reward 4.0, epsilon 0.7575135257929821\n",
      "timestep  814, reward 4.0, epsilon 0.7572862944598925\n",
      "timestep  815, reward 5.0, epsilon 0.7569834253762573\n",
      "timestep  816, reward 3.0, epsilon 0.7568320362610162\n",
      "timestep  817, reward 4.0, epsilon 0.7566050093543423\n",
      "timestep  818, reward 3.0, epsilon 0.7564536959185215\n",
      "timestep  819, reward 4.0, epsilon 0.7562267825026004\n",
      "timestep  820, reward 6.0, epsilon 0.7558487447264656\n",
      "timestep  821, reward 3.0, epsilon 0.7556975825360077\n",
      "timestep  822, reward 4.0, epsilon 0.7554708959314187\n",
      "timestep  823, reward 3.0, epsilon 0.7553198093069414\n",
      "timestep  824, reward 5.0, epsilon 0.7550177266993859\n",
      "timestep  825, reward 3.0, epsilon 0.7548667307042234\n",
      "timestep  826, reward 3.0, epsilon 0.7547157649067499\n",
      "timestep  827, reward 4.0, epsilon 0.7544893728179961\n",
      "timestep  828, reward 3.0, epsilon 0.7543384824883262\n",
      "timestep  829, reward 4.0, epsilon 0.7541122035729799\n",
      "timestep  830, reward 4.0, epsilon 0.75388599253452\n",
      "timestep  831, reward 4.0, epsilon 0.7536598493525856\n",
      "timestep  832, reward 3.0, epsilon 0.7535091249193135\n",
      "timestep  833, reward 4.0, epsilon 0.753283094786358\n",
      "timestep  834, reward 4.0, epsilon 0.7530571324556617\n",
      "timestep  835, reward 4.0, epsilon 0.752831237906886\n",
      "timestep  836, reward 4.0, epsilon 0.7526054111196983\n",
      "timestep  837, reward 4.0, epsilon 0.7523796520737721\n",
      "timestep  838, reward 4.0, epsilon 0.7521539607487872\n",
      "timestep  839, reward 4.0, epsilon 0.7519283371244291\n",
      "timestep  840, reward 7.0, epsilon 0.7514772928963676\n",
      "timestep  841, reward 6.0, epsilon 0.7511016293901344\n",
      "timestep  842, reward 3.0, epsilon 0.7509514165752726\n",
      "timestep  843, reward 4.0, epsilon 0.7507261536780917\n",
      "timestep  844, reward 5.0, epsilon 0.7504259082571869\n",
      "timestep  845, reward 4.0, epsilon 0.7502008029967366\n",
      "timestep  846, reward 3.0, epsilon 0.7500507703381453\n",
      "timestep  847, reward 5.0, epsilon 0.7497507950300561\n",
      "timestep  848, reward 3.0, epsilon 0.749600852368558\n",
      "timestep  849, reward 4.0, epsilon 0.7493759946001234\n",
      "timestep  850, reward 5.0, epsilon 0.7490762891618457\n",
      "timestep  851, reward 5.0, epsilon 0.7487767035877622\n",
      "timestep  852, reward 3.0, epsilon 0.7486269557348116\n",
      "timestep  853, reward 6.0, epsilon 0.748252717112154\n",
      "timestep  854, reward 3.0, epsilon 0.7481030740512588\n",
      "timestep  855, reward 5.0, epsilon 0.7478038777048304\n",
      "timestep  856, reward 3.0, epsilon 0.7476543244073283\n",
      "timestep  857, reward 4.0, epsilon 0.7474300505388882\n",
      "timestep  858, reward 4.0, epsilon 0.7472058439458807\n",
      "timestep  859, reward 3.0, epsilon 0.74705641024915\n",
      "timestep  860, reward 7.0, epsilon 0.7466082884465222\n",
      "timestep  861, reward 4.0, epsilon 0.7463843283574902\n",
      "timestep  862, reward 3.0, epsilon 0.7462350589556621\n",
      "timestep  863, reward 4.0, epsilon 0.7460112108242809\n",
      "timestep  864, reward 4.0, epsilon 0.7457874298406241\n",
      "timestep  865, reward 3.0, epsilon 0.7456382798125303\n",
      "timestep  866, reward 4.0, epsilon 0.7454146106969893\n",
      "timestep  867, reward 3.0, epsilon 0.7452655352289961\n",
      "timestep  868, reward 4.0, epsilon 0.7450419779256482\n",
      "timestep  869, reward 3.0, epsilon 0.7448929769804828\n",
      "timestep  870, reward 4.0, epsilon 0.7446695314334332\n",
      "timestep  871, reward 4.0, epsilon 0.7444461529133445\n",
      "timestep  872, reward 4.0, epsilon 0.7442228414001106\n",
      "timestep  873, reward 4.0, epsilon 0.7439995968736316\n",
      "timestep  874, reward 4.0, epsilon 0.7437764193138133\n",
      "timestep  875, reward 4.0, epsilon 0.7435533087005679\n",
      "timestep  876, reward 5.0, epsilon 0.743255931987312\n",
      "timestep  877, reward 4.0, epsilon 0.7430329775046505\n",
      "timestep  878, reward 3.0, epsilon 0.7428843783394793\n",
      "timestep  879, reward 3.0, epsilon 0.7427358088926551\n",
      "timestep  880, reward 6.0, epsilon 0.7423645152543628\n",
      "timestep  881, reward 4.0, epsilon 0.7421418281699796\n",
      "timestep  882, reward 3.0, epsilon 0.741993407225764\n",
      "timestep  883, reward 4.0, epsilon 0.7417708314626564\n",
      "timestep  884, reward 3.0, epsilon 0.7416224847140722\n",
      "timestep  885, reward 3.0, epsilon 0.7414741676333542\n",
      "timestep  886, reward 4.0, epsilon 0.7412517476265477\n",
      "timestep  887, reward 3.0, epsilon 0.7411035046895398\n",
      "timestep  888, reward 5.0, epsilon 0.74080710775091\n",
      "timestep  889, reward 4.0, epsilon 0.7405848878420571\n",
      "timestep  890, reward 4.0, epsilon 0.7403627345925106\n",
      "timestep  891, reward 4.0, epsilon 0.7401406479822745\n",
      "timestep  892, reward 6.0, epsilon 0.7397706516649472\n",
      "timestep  893, reward 4.0, epsilon 0.7395487426618274\n",
      "timestep  894, reward 4.0, epsilon 0.7393269002247516\n",
      "timestep  895, reward 4.0, epsilon 0.7391051243337519\n",
      "timestep  896, reward 4.0, epsilon 0.7388834149688664\n",
      "timestep  897, reward 3.0, epsilon 0.7387356456747068\n",
      "timestep  898, reward 4.0, epsilon 0.738514047142335\n",
      "timestep  899, reward 5.0, epsilon 0.738218685831367\n",
      "timestep  900, reward 4.0, epsilon 0.7379972423714398\n",
      "timestep  901, reward 6.0, epsilon 0.7376283175425987\n",
      "timestep  902, reward 3.0, epsilon 0.7374807992553735\n",
      "timestep  903, reward 4.0, epsilon 0.7372595771392834\n",
      "timestep  904, reward 4.0, epsilon 0.7370384213831916\n",
      "timestep  905, reward 6.0, epsilon 0.7366699758689722\n",
      "timestep  906, reward 3.0, epsilon 0.7365226492404982\n",
      "timestep  907, reward 3.0, epsilon 0.7363753520758766\n",
      "timestep  908, reward 5.0, epsilon 0.736080846114622\n",
      "timestep  909, reward 4.0, epsilon 0.735860043942477\n",
      "timestep  910, reward 5.0, epsilon 0.7355657440735593\n",
      "timestep  911, reward 5.0, epsilon 0.7352715619069324\n",
      "timestep  912, reward 4.0, epsilon 0.735051002495772\n",
      "timestep  913, reward 4.0, epsilon 0.7348305092458184\n",
      "timestep  914, reward 6.0, epsilon 0.7344631674668985\n",
      "timestep  915, reward 4.0, epsilon 0.7342428505498191\n",
      "timestep  916, reward 5.0, epsilon 0.7339491974612334\n",
      "timestep  917, reward 3.0, epsilon 0.7338024149612332\n",
      "timestep  918, reward 4.0, epsilon 0.7335822962500834\n",
      "timestep  919, reward 3.0, epsilon 0.7334355871266565\n",
      "timestep  920, reward 5.0, epsilon 0.7331422568950074\n",
      "timestep  921, reward 3.0, epsilon 0.7329956357750511\n",
      "timestep  922, reward 4.0, epsilon 0.7327757590734547\n",
      "timestep  923, reward 5.0, epsilon 0.7324826927334398\n",
      "timestep  924, reward 4.0, epsilon 0.7322629698993681\n",
      "timestep  925, reward 3.0, epsilon 0.732116524628018\n",
      "timestep  926, reward 4.0, epsilon 0.7318969116333933\n",
      "timestep  927, reward 3.0, epsilon 0.7317505395700358\n",
      "timestep  928, reward 3.0, epsilon 0.7316041967796272\n",
      "timestep  929, reward 3.0, epsilon 0.7314578832563132\n",
      "timestep  930, reward 5.0, epsilon 0.7311653439875581\n",
      "timestep  931, reward 4.0, epsilon 0.7309460163185909\n",
      "timestep  932, reward 6.0, epsilon 0.7305806163977242\n",
      "timestep  933, reward 3.0, epsilon 0.7304345075802507\n",
      "timestep  934, reward 4.0, epsilon 0.7302153991402816\n",
      "timestep  935, reward 6.0, epsilon 0.7298503644549494\n",
      "timestep  936, reward 5.0, epsilon 0.7295584680972701\n",
      "timestep  937, reward 5.0, epsilon 0.7292666884806212\n",
      "timestep  938, reward 4.0, epsilon 0.7290479303513484\n",
      "timestep  939, reward 5.0, epsilon 0.7287563549191677\n",
      "timestep  940, reward 4.0, epsilon 0.7285377498746538\n",
      "timestep  941, reward 4.0, epsilon 0.7283192104050954\n",
      "timestep  942, reward 4.0, epsilon 0.728100736490822\n",
      "timestep  943, reward 4.0, epsilon 0.7278823281121687\n",
      "timestep  944, reward 4.0, epsilon 0.7276639852494771\n",
      "timestep  945, reward 4.0, epsilon 0.7274457078830942\n",
      "timestep  946, reward 5.0, epsilon 0.7271547732437738\n",
      "timestep  947, reward 4.0, epsilon 0.7269366486257167\n",
      "timestep  948, reward 3.0, epsilon 0.7267912685653581\n",
      "timestep  949, reward 7.0, epsilon 0.7263553028083745\n",
      "timestep  950, reward 5.0, epsilon 0.7260648042656639\n",
      "timestep  951, reward 4.0, epsilon 0.7258470066056023\n",
      "timestep  952, reward 4.0, epsilon 0.725629274278305\n",
      "timestep  953, reward 4.0, epsilon 0.7254116072641742\n",
      "timestep  954, reward 4.0, epsilon 0.7251940055436178\n",
      "timestep  955, reward 4.0, epsilon 0.7249764690970498\n",
      "timestep  956, reward 3.0, epsilon 0.7248314810529952\n",
      "timestep  957, reward 4.0, epsilon 0.724614053352899\n",
      "timestep  958, reward 6.0, epsilon 0.7242518187803821\n",
      "timestep  959, reward 4.0, epsilon 0.7240345649615784\n",
      "timestep  960, reward 4.0, epsilon 0.7238173763124028\n",
      "timestep  961, reward 3.0, epsilon 0.7236726200753141\n",
      "timestep  962, reward 3.0, epsilon 0.7235278927880252\n",
      "timestep  963, reward 4.0, epsilon 0.7233108561253022\n",
      "timestep  964, reward 3.0, epsilon 0.7231662011871857\n",
      "timestep  965, reward 3.0, epsilon 0.7230215751786102\n",
      "timestep  966, reward 3.0, epsilon 0.7228769780937903\n",
      "timestep  967, reward 5.0, epsilon 0.7225878706722799\n",
      "timestep  968, reward 4.0, epsilon 0.7223711159879919\n",
      "timestep  969, reward 8.0, epsilon 0.7218656078794543\n",
      "timestep  970, reward 6.0, epsilon 0.7215047472548571\n",
      "timestep  971, reward 4.0, epsilon 0.7212883174751016\n",
      "timestep  972, reward 5.0, epsilon 0.7209998454225257\n",
      "timestep  973, reward 5.0, epsilon 0.7207114887414634\n",
      "timestep  974, reward 4.0, epsilon 0.720495296915465\n",
      "timestep  975, reward 4.0, epsilon 0.7202791699405288\n",
      "timestep  976, reward 3.0, epsilon 0.7201351213093324\n",
      "timestep  977, reward 5.0, epsilon 0.7198471104660354\n",
      "timestep  978, reward 4.0, epsilon 0.7196311779275891\n",
      "timestep  979, reward 3.0, epsilon 0.7194872588883153\n",
      "timestep  980, reward 3.0, epsilon 0.7193433686314104\n",
      "timestep  981, reward 4.0, epsilon 0.7191275872004027\n",
      "timestep  982, reward 6.0, epsilon 0.7187680953123704\n",
      "timestep  983, reward 3.0, epsilon 0.7186243488809889\n",
      "timestep  984, reward 4.0, epsilon 0.7184087831343364\n",
      "timestep  985, reward 4.0, epsilon 0.7181932820509412\n",
      "timestep  986, reward 5.0, epsilon 0.7179060478268451\n",
      "timestep  987, reward 3.0, epsilon 0.7177624737963402\n",
      "timestep  988, reward 3.0, epsilon 0.7176189284792056\n",
      "timestep  989, reward 5.0, epsilon 0.7173319239620791\n",
      "timestep  990, reward 4.0, epsilon 0.7171167459041309\n",
      "timestep  991, reward 5.0, epsilon 0.7168299422299056\n",
      "timestep  992, reward 6.0, epsilon 0.716471598934617\n",
      "timestep  993, reward 3.0, epsilon 0.716328311779546\n",
      "timestep  994, reward 5.0, epsilon 0.7160418234316677\n",
      "timestep  995, reward 5.0, epsilon 0.7157554496619403\n",
      "timestep  996, reward 6.0, epsilon 0.7153976435054971\n",
      "timestep  997, reward 4.0, epsilon 0.7151830456736593\n",
      "timestep  998, reward 4.0, epsilon 0.7149685122147336\n",
      "timestep  999, reward 4.0, epsilon 0.7147540431094096\n",
      "timestep  1000, reward 7.0, epsilon 0.7143252978823565\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import torch.autograd\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64):\n",
    "        super(PPOAgent, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(1, action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        dist = Normal(mean, std)\n",
    "        value = self.critic(x)\n",
    "        return dist, value\n",
    "    \n",
    "class PPO:\n",
    "    def __init__(self, env_name, T=2048, minibatch_size=64, timestep = 1000, epochs=10, gamma=0.99, gae_lambda=0.95, clip_param=0.2, lr=3e-4,epsilon=0.2):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.T = T\n",
    "        self.epsilon = epsilon\n",
    "        self.timestep = timestep\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_param = clip_param\n",
    "        self.agent = PPOAgent(self.env.observation_space.shape[0], self.env.action_space.shape[0])\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "    \n",
    "        initial_state = self.env.reset()\n",
    "        state = initial_state[0] if isinstance(initial_state, tuple) else initial_state\n",
    "\n",
    "        for time in range(self.timestep):\n",
    "            \n",
    "            states, actions, rewards, dones, values, log_probs = [], [], [], [], [], []\n",
    "            state=self.env.reset()\n",
    "            for t in range(self.T):\n",
    "                # print(f't {t + 1}')\n",
    "                # Handle both tuple and direct array returns for state consistency\n",
    "                state = state[0] if isinstance(state, tuple) else state\n",
    "                \n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state array to tensor\n",
    "\n",
    "                dist, value = self.agent(state_tensor)\n",
    "                \n",
    "                \n",
    "                action = dist.sample()\n",
    "                \n",
    "                log_prob = dist.log_prob(action)\n",
    "                action=action.item()\n",
    "                \n",
    "                if np.random.rand() < self.epsilon:\n",
    "                    action= np.random.choice([0, 1, 2])\n",
    "\n",
    "                # Ensure action remains a 1-dimensional array with a single element\n",
    "                # action_numpy = action.squeeze().detach().numpy()  # Squeeze to potentially reduce dimensions\n",
    "                # if action_numpy.ndim == 0:  # If the result is a scalar, convert it back to an array\n",
    "                #     action_numpy = np.array([action_numpy])  # Convert to 1-dimensional array with one element\n",
    "                action=np.array([action])\n",
    "                # print(\"action\", action)\n",
    "                next_state, reward, done, _, _ = self.env.step(action)\n",
    "                \n",
    "\n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                dones.append(done)\n",
    "                values.append(value.detach())\n",
    "                log_probs.append(log_prob)\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                state = next_state \n",
    "                \n",
    "                # update epsilon if the agent is learning\n",
    "                if self.epsilon!=0:\n",
    "                    self.epsilon = max(0.01, self.epsilon * 0.9999)\n",
    "                \n",
    "                # print(\"reward\", reward)\n",
    "                if state is None:\n",
    "                    raise ValueError(\"Environment reset returned None state.\")\n",
    "\n",
    "            data = states, actions, rewards, dones, values, log_probs\n",
    "            print(f'timestep  {time + 1}, reward {sum(rewards)}, epsilon {self.epsilon}')\n",
    "            # print(\"train ok\")\n",
    "            self.policy_update(data)\n",
    "\n",
    "\n",
    "\n",
    "    def policy_update(self, data):\n",
    "        # print(\"policy update\")\n",
    "        states, actions, rewards, dones, values, log_probs = data\n",
    "        advantages = self.calculate_advantages(rewards, values, dones)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.FloatTensor(actions)\n",
    "        old_values = torch.cat(values).detach()  # Ensure values are detached\n",
    "        old_log_probs = torch.cat(log_probs).detach() \n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # print(f'Epoch {epoch + 1}')\n",
    "            idx = torch.randperm(len(data[0]))[:self.minibatch_size]\n",
    "            sampled_states = states[idx]\n",
    "            sampled_actions = actions[idx]\n",
    "            sampled_advantages = advantages[idx]\n",
    "            sampled_old_log_probs = old_log_probs[idx]\n",
    "\n",
    "            new_dist, new_values = self.agent(sampled_states)\n",
    "            new_log_probs = new_dist.log_prob(sampled_actions)\n",
    "\n",
    "            ratio = torch.exp(new_log_probs - sampled_old_log_probs)\n",
    "            surr1 = ratio * sampled_advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_param, 1 + self.clip_param) * sampled_advantages\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = 0.5 * (sampled_advantages - new_values.squeeze()).pow(2).mean()  # Make sure shapes match\n",
    "            loss = actor_loss + critic_loss\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()  # 'retain_graph=True' is typically not needed unless explicitly required\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "    # On the last pass, you may not need to retain the graph, or this can be handled outside the loop\n",
    "\n",
    "\n",
    "    def calculate_advantages(self, rewards, values, dones):\n",
    "        \n",
    "        # Append zero tensor with the same shape as the last value tensor\n",
    "        zero_padding = torch.zeros_like(values[-1])\n",
    "        values.append(zero_padding)\n",
    "        values = torch.cat(values)\n",
    "        gae = 0\n",
    "        advantages = []\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * (1 - dones[step]) - values[step]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1 - dones[step]) * gae\n",
    "            advantages.insert(0, gae + values[step])\n",
    "\n",
    "        return torch.FloatTensor(advantages)\n",
    "\n",
    "\n",
    "    \n",
    "    def evaluate(self, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state = state[0] if isinstance(state, tuple) else state\n",
    "                \n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Convert state array to tensor\n",
    "               \n",
    "                dist, value = self.agent(state_tensor)\n",
    "                action = dist.sample()\n",
    "               \n",
    "\n",
    "                # Ensure action remains a 1-dimensional array with a single element\n",
    "                action_numpy = action.squeeze().detach().numpy()  # Squeeze to potentially reduce dimensions\n",
    "                if action_numpy.ndim == 0:  # If the result is a scalar, convert it back to an array\n",
    "                    action_numpy = np.array([action_numpy])\n",
    "                \n",
    "               \n",
    "\n",
    "                next_state, reward, done, _, _ = self.env.step(action_numpy)\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "                print(\"reward\", reward)\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "# Example usage\n",
    "ppo = PPO(\"InvertedPendulum-v4\")\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "ppo.train()\n",
    "#average_reward = ppo.evaluate(num_episodes=1000000)\n",
    "#print(f\"Average Reward from Evaluation: {average_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.36112322431538e-22"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8*0.995**10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ignore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor-Critic Network for Continuous Action Spaces\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Learnable log standard deviation\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mean)  # Standard deviation\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        return dist, self.critic(x)\n",
    "\n",
    "# PPO Agent with Clipping Method\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, clip_epsilon=0.2):\n",
    "        self.model = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = 0.99\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        dist, _ = self.model(state_tensor)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "    def train(self, states, actions, rewards, next_states, dones, old_log_probs):\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        actions_tensor = torch.FloatTensor(actions).unsqueeze(1)\n",
    "        next_states_tensor = torch.FloatTensor(next_states)\n",
    "        rewards_tensor = torch.FloatTensor(rewards)\n",
    "        dones_tensor = torch.FloatTensor(dones)\n",
    "        old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "\n",
    "        _, values = self.model(states_tensor)\n",
    "        dists, next_values = self.model(next_states_tensor)\n",
    "\n",
    "        new_log_probs = dists.log_prob(actions_tensor).sum(axis=1, keepdim=True)\n",
    "\n",
    "        advantages = rewards_tensor + self.gamma * next_values.squeeze() * (1 - dones_tensor) - values.squeeze()\n",
    "\n",
    "        ratios = (new_log_probs - old_log_probs_tensor).exp()\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "        surrogate_loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
    "\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        loss = surrogate_loss + critic_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Training Function\n",
    "def train_agent(agent, env, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        states, actions, rewards, next_states, dones, old_log_probs = [], [], [], [], [], []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, done, _,_ = env.step([action])  # Action needs to be a list\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            old_log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            \n",
    "\n",
    "        agent.train(states, actions, rewards, next_states, dones, old_log_probs)\n",
    "        print(f'Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4')\n",
    "# agent = PPOAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "# train_agent(agent, env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent, env, episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.select_action(state)  # Use the trained policy to select actions\n",
    "            state, reward, done, _ , _= env.step([action])  # Action should be in the correct format\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f'Evaluation Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "    \n",
    "    average_reward = sum(total_rewards) / len(total_rewards)\n",
    "    print(f'Average Reward over {episodes} episodes: {average_reward}')\n",
    "    return average_reward\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4', render_mode='human')\n",
    "# evaluate_policy(agent, env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('InvertedPendulum-v4', render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        return dist, self.critic(x)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, horizon=2048, lr=3e-4, clip_epsilon=0.2, gamma=0.99, gae_lambda=0.95, epochs=10, minibatch_size=64):\n",
    "        self.model = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        dist, _ = self.model(state_tensor)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "    def train(self, states, actions, rewards, next_states, dones, old_log_probs):\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        actions_tensor = torch.FloatTensor(actions).unsqueeze(1)\n",
    "        next_states_tensor = torch.FloatTensor(next_states)\n",
    "        rewards_tensor = torch.FloatTensor(rewards)\n",
    "        dones_tensor = torch.FloatTensor(dones)\n",
    "        old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "\n",
    "        _, values = self.model(states_tensor)\n",
    "        dists, next_values = self.model(next_states_tensor)\n",
    "\n",
    "        new_log_probs = dists.log_prob(actions_tensor).sum(axis=1, keepdim=True)\n",
    "\n",
    "        advantages = rewards_tensor + self.gamma * next_values.squeeze() * (1 - dones_tensor) - values.squeeze()\n",
    "\n",
    "        ratios = (new_log_probs - old_log_probs_tensor).exp()\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "        surrogate_loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
    "\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        loss = surrogate_loss + critic_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "def train_agent(agent, env, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        states, actions, rewards, next_states, dones, old_log_probs = [], [], [], [], [], []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step([action])\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            old_log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        agent.train(states, actions, rewards, next_states, dones, old_log_probs)\n",
    "        print(f'Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4')\n",
    "# agent = PPOAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "# train_agent(agent, env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent, env, episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.select_action(state)  # Use the trained policy to select actions\n",
    "            state, reward, done, _ , _= env.step([action])  # Action should be in the correct format\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f'Evaluation Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "    \n",
    "    average_reward = sum(total_rewards) / len(total_rewards)\n",
    "    print(f'Average Reward over {episodes} episodes: {average_reward}')\n",
    "    return average_reward\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4', render_mode='human')\n",
    "# evaluate_policy(agent, env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions import Normal\n",
    "# import numpy as np\n",
    "# import os\n",
    "# # from torch.utils.tensorboard import SummaryWriter\n",
    "# from functools import reduce\n",
    "# from operator import mul\n",
    "\n",
    "# glob_i = 0\n",
    "# glob_error = 0\n",
    "\n",
    "# class PPOAgent():\n",
    "#     def __init__(self, TRAIN, env=None, traj=3, net_size=64, net_std=1,\n",
    "#                 lr=1e-4, bs=100, y=0.99, ep=10, W=None,\n",
    "#                 c1=0.5, c2=0.01):\n",
    "#         self.TRAIN  = TRAIN\n",
    "#         self.LR     = lr\n",
    "#         self.GAMMA  = y\n",
    "#         self.BATCHSIZE  = bs\n",
    "#         self.EPOCHS = ep\n",
    "#         self.NTRAJ = traj\n",
    "#         self.env = env\n",
    "#         self.C1 = c1\n",
    "#         self.C2 = c2\n",
    "\n",
    "#         self.WRITER = W\n",
    "\n",
    "#         self.input_size = reduce(mul, env.observation_space.shape, 1)\n",
    "#         self.out_size   = env.action_space.shape[0]\n",
    "\n",
    "#         self.model = AgentNet(self.input_size, self.out_size, h=net_size, std=net_std)\n",
    "#         self.opt = optim.Adam(self.model.parameters(), lr=self.LR)\n",
    "\n",
    "#         self.value_criterion = nn.MSELoss()\n",
    "\n",
    "#         self.trajectories = []\n",
    "#         self.states   = []\n",
    "#         self.rewards  = []\n",
    "#         self.actions  = []\n",
    "#         self.values   = []\n",
    "#         self.logAprob = []\n",
    "\n",
    "#     def __call__(self, state):\n",
    "#         # Each time an action is required, we save the\n",
    "#         # state value for computing advantages later\n",
    "#         state = torch.FloatTensor(state).reshape(1, self.input_size)\n",
    "#         normal, _ = self.model(state) \n",
    "#         action = normal.sample().reshape(1, self.out_size)\n",
    "\n",
    "#         self.values.append(normal.mean)\n",
    "#         self.actions.append(action)\n",
    "#         self.logAprob.append(normal.log_prob(action).squeeze(0))\n",
    "\n",
    "#         return action.numpy().reshape(self.out_size)\n",
    "\n",
    "#     def observe(self, s, r, s1, done, NEPISODE):\n",
    "#         if not self.TRAIN: return\n",
    "\n",
    "#         s  = torch.FloatTensor(s).reshape(1, self.input_size)\n",
    "#         s1 = torch.FloatTensor(s1).reshape(1, self.input_size)\n",
    "\n",
    "#         self.states.append(s)\n",
    "\n",
    "#         # For rewards we only maintain the value.\n",
    "#         self.rewards.append(r)\n",
    "\n",
    "#         if done:\n",
    "#             # Compute advantages using critic network\n",
    "#             with torch.no_grad():\n",
    "#                 s1_tensor = s1\n",
    "#                 _, next_value = self.model(s1_tensor)\n",
    "#             next_value = next_value.detach().squeeze().numpy()\n",
    "#             advantages = self._compute_advantages(next_value)\n",
    "\n",
    "#             # Update trajectories with advantages\n",
    "#             for traj, adv in zip(self.trajectories, advantages):\n",
    "#                 traj['Adv'] = torch.FloatTensor(adv)\n",
    "\n",
    "#             # Update Condition\n",
    "#             if NEPISODE != 0 and (NEPISODE % self.NTRAJ) == 0: \n",
    "#                 self.update() \n",
    "\n",
    "#             self.states   = []\n",
    "#             self.actions  = []\n",
    "#             self.logAprob = []\n",
    "#             self.values   = []\n",
    "#             self.rewards  = []\n",
    "\n",
    "#     def _compute_advantages(self, next_value):\n",
    "#         advantages = []\n",
    "#         for traj in self.trajectories:\n",
    "#             rewards = traj['r']\n",
    "#             values = traj['V'].numpy()\n",
    "#             deltas = [r + self.GAMMA * next_v - v for r, next_v, v in zip(rewards, [next_value] + values[:-1], values)]\n",
    "#             advantages.extend(self._discount_cumsum(deltas, self.GAMMA * self.C2))\n",
    "#         return advantages\n",
    "\n",
    "#     def _discount_cumsum(self, x, discount):\n",
    "#         \"\"\"\n",
    "#         Compute discounted cumulative sums of vectors.\n",
    "\n",
    "#         Parameters:\n",
    "#             x (list): Input list of numbers.\n",
    "#             discount (float): Discount factor.\n",
    "\n",
    "#         Returns:\n",
    "#             list: Discounted cumulative sums.\n",
    "#         \"\"\"\n",
    "#         discounted = [x[-1]]\n",
    "#         for v in reversed(x[:-1]):\n",
    "#             discounted.append(v + discount * discounted[-1])\n",
    "#         return list(reversed(discounted))\n",
    "\n",
    "#     def update(self):\n",
    "#         EPS = 0.2\n",
    "\n",
    "#         # Compute advantages\n",
    "#         S = torch.cat([x['S'] for x in self.trajectories], 0)\n",
    "#         A = torch.cat([x['A'] for x in self.trajectories], 0)\n",
    "#         Adv = torch.cat([x['Adv'] for x in self.trajectories], 0)\n",
    "#         Log_old = torch.cat([x['LogP'] for x in self.trajectories], 0)\n",
    "#         G = torch.cat([x['G'] for x in self.trajectories], 0)\n",
    "#         V = torch.cat([x['V'] for x in self.trajectories], 0)\n",
    "\n",
    "#         bufsize = S.size(0)\n",
    "\n",
    "#         for ep in range(self.EPOCHS*(bufsize//self.BATCHSIZE+1)):\n",
    "#             ids = np.random.randint(0, bufsize,\n",
    "#                     min(self.BATCHSIZE, bufsize))\n",
    "\n",
    "#             bS, bA, bAdv = S[ids,:], A[ids,:], Adv[ids,:]\n",
    "\n",
    "#             normal, Vnew = self.model(bS)\n",
    "#             logAprob_old = Log_old[ids,:]\n",
    "#             logAprob = normal.log_prob(bA)\n",
    "\n",
    "#             # L_CLIP\n",
    "#             ratio = (logAprob - logAprob_old).exp().squeeze(0)\n",
    "#             m1 = ratio * bAdv\n",
    "#             m2 = torch.clamp(ratio, 1.0 - EPS, 1.0 + EPS) * bAdv\n",
    "#             L_CLIP = torch.min(m1, m2).mean()\n",
    "\n",
    "#             # L_VF\n",
    "#             L_VF = (G[ids,:] - Vnew).pow(2).mean()\n",
    "\n",
    "#             # Entropy\n",
    "#             E = normal.entropy().mean()\n",
    "\n",
    "#             # Total Loss\n",
    "#             L = -L_CLIP + self.C1 * L_VF - self.C2 * E\n",
    "\n",
    "#             # Apply Gradients\n",
    "#             self.opt.zero_grad()\n",
    "#             L.backward()\n",
    "#             self.opt.step()\n",
    "\n",
    "#         # Update Graphs\n",
    "#         # self.WRITER.add_scalar(\"L_VF\", L_VF, glob_i)\n",
    "#         # global glob_i\n",
    "#         # glob_i += 1\n",
    "#         self.trajectories = []\n",
    "\n",
    "#     def load(self, path):\n",
    "#         try:\n",
    "#             self.model.load_state_dict(torch.load(path))\n",
    "#             self.model.eval()\n",
    "#         except FileNotFoundError:\n",
    "#             print(f'Error: {path} not found.')\n",
    "#             exit()\n",
    "\n",
    "#     def save(self, path):\n",
    "#         torch.save(self.model.state_dict(), path)\n",
    "\n",
    "# class AgentNet(nn.Module):\n",
    "#     def __init__(self, inp, out, h=32, std=0):\n",
    "#         super(AgentNet, self).__init__()\n",
    "\n",
    "#         self.actor = nn.Sequential(\n",
    "#             nn.Linear(inp, h),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h, h//2),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h//2, out),\n",
    "#         )\n",
    "#         self.log_std = nn.Parameter(torch.ones(1, out) * std)\n",
    "\n",
    "#         self.critic = nn.Sequential(\n",
    "#             nn.Linear(inp, h),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h, h//2),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h//2, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         actor_output = self.actor(x)\n",
    "#         critic_output = self.critic(x)\n",
    "#         std = self.log_std.exp().expand_as(actor_output)\n",
    "#         dist = Normal(actor_output, std)\n",
    "#         return dist, critic_output\n",
    "\n",
    "# # main.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # import mujoco_py\n",
    "# import gym\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# # from torch.utils.tensorboard import SummaryWriter\n",
    "# # from ppo_agent import PPOAgent\n",
    "# import sys, os, json\n",
    "# from glob import glob\n",
    "# import argparse\n",
    "\n",
    "# # Environemnt Params\n",
    "# MODELS_PATH        = 'models'\n",
    "# DEFAULT_EPISODES   = 2000\n",
    "# DEFAULT_MAX_STEPS  = 2000\n",
    "# DEFAULT_CHECKPOINT = 5\n",
    "\n",
    "# ## These may be updated by arguments\n",
    "# EnvName = 'InvertedPendulum-v4'\n",
    "# TRAIN = False\n",
    "# RENDER = True\n",
    "\n",
    "# # writer = SummaryWriter(max_queue=2)\n",
    "\n",
    "# def main():\n",
    "#     global TRAIN, RENDER, EnvName\n",
    "\n",
    "#     # Training Parameters\n",
    "#     params_path = os.path.join(MODELS_PATH, f'{EnvName}.json')\n",
    "#     with open(params_path, 'r') as f:\n",
    "#         params = json.load(f)\n",
    "\n",
    "#     EPOCHS        = params[\"EPOCHS\"] if \"EPOCHS\" in params else 200\n",
    "#     LR            = params[\"LR\"] if \"LR\" in params else 1e-3\n",
    "#     C2            = params[\"C2\"] if \"C2\" in params else 0\n",
    "#     GAMMA         = params[\"GAMMA\"] if \"GAMMA\" in params else 0.99\n",
    "#     STD           = params[\"STD\"] if \"STD\" in params else 1\n",
    "#     NETSIZE       = params[\"NETSIZE\"] if \"NETSIZE\" in params else 64\n",
    "#     BATCHSIZE     = params[\"BATCHSIZE\"] if \"BATCHSIZE\" in params else 500\n",
    "#     TRAJECTORIES  = params[\"TRAJECTORIES\"] if \"TRAJECTORIES\" in params else 10\n",
    "#     MAX_STEPS     = params[\"MAX_STEPS\"] if \"MAX_STEPS\" in params else 2000\n",
    "#     CHECKPOINT    = params[\"CHECKPOINT\"] if \"CHECKPOINT\" in params else 5\n",
    "#     EPISODES      = params[\"EPISODES\"] if \"EPISODES\" in params else 2000\n",
    "\n",
    "#     print(\"Environment:  \", EnvName)\n",
    "#     print(\"Train:        \", TRAIN)\n",
    "#     print(\"EPOCHS:       \", EPOCHS)\n",
    "#     print(\"LR:           \", LR)\n",
    "#     print(\"C2:           \", C2)\n",
    "#     print(\"GAMMA:        \", GAMMA)\n",
    "#     print(\"STD:          \", STD)\n",
    "#     print(\"NETSIZE:      \", NETSIZE)\n",
    "#     print(\"BATCHSIZE:    \", BATCHSIZE)\n",
    "#     print(\"TRAJECTORIES: \", TRAJECTORIES)\n",
    "#     print(\"MAX_STEPS:    \", MAX_STEPS)\n",
    "#     print(\"CHECKPOINT:   \", CHECKPOINT)\n",
    "#     print(\"EPISODES:     \", EPISODES)\n",
    "\n",
    "#     # save model after collecting N trajectories \n",
    "#     # (which corresponds to when the update is calculated)\n",
    "#     SAVE_STEP = CHECKPOINT * TRAJECTORIES\n",
    "#     save_model_name = os.path.join(MODELS_PATH, EnvName + \".pth\")\n",
    "\n",
    "#     total = 0\n",
    "\n",
    "#     env = gym.make(EnvName, render_mode=\"human\" if RENDER else None)\n",
    "#     agent = PPOAgent(\n",
    "#             TRAIN, env=env,\n",
    "#             lr=LR, c2=C2,\n",
    "#             net_size=NETSIZE,\n",
    "#             net_std=STD,\n",
    "#             y=GAMMA,\n",
    "#             traj=TRAJECTORIES,\n",
    "#             bs=BATCHSIZE,\n",
    "#             ep=EPOCHS\n",
    "#     )\n",
    "#     if not TRAIN: agent.load(save_model_name)\n",
    "\n",
    "#     for i in range(EPISODES):\n",
    "#         state, _ = env.reset()\n",
    "\n",
    "#         for t in range(MAX_STEPS+1):\n",
    "#             # RL Step\n",
    "#             action = agent(state)\n",
    "#             new_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "#             # Impose done=True if last-step\n",
    "#             if t == MAX_STEPS: done = True\n",
    "\n",
    "#             agent.observe(state, reward, new_state, done, i)\n",
    "\n",
    "#             total += reward\n",
    "#             state  = new_state\n",
    "\n",
    "#             if done: break\n",
    "\n",
    "#         # Print Performance\n",
    "#         print(f\"[{i}] Steps: {t}\\tReward: {total}\")\n",
    "#         # writer.add_scalar('Reward', total, i)\n",
    "#         total = 0\n",
    "\n",
    "#         if TRAIN and (i % SAVE_STEP) == SAVE_STEP -1:\n",
    "#             agent.save(save_model_name)\n",
    "#             print(\"Model Checkpoint saved\")\n",
    "\n",
    "#     env.close()\n",
    "\n",
    "\n",
    "# # envs_names = glob(f'{MODELS_PATH}/*.json')\n",
    "# # envs_names = [x.split('/')[-1].split('.')[0] for x in envs_names]\n",
    "\n",
    "# # parser = argparse.ArgumentParser(description=\"Train PPO models and run Gym environments\")\n",
    "# # parser.add_argument('env', type=str, metavar=\"environment\", help=\", \".join(envs_names),\n",
    "# #                     choices=envs_names, default=\"MountainCarContinuous-v0\")\n",
    "# # parser.add_argument('--train', action='store_true')\n",
    "# # args = parser.parse_args()\n",
    "\n",
    "# # EnvName = EnvName\n",
    "# # TRAIN   = TRAIN\n",
    "# # RENDER  = not(TRAIN)\n",
    "\n",
    "# # main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, epochs=200, lr=1e-3, gamma=0.99, c1=0.5, c2=0.01, batch_size=500):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            for _ in range(self.epochs):\n",
    "                self.update(states, actions, rewards, log_probs, values)\n",
    "\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Ensure the state tensor is of type float\n",
    "            dist, value = self.agent(state_tensor)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "            \n",
    "            states.append(state_tensor)  # Append the state tensor\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "            values.append(value)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset()[0]\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            print(states.shape, self.agent)\n",
    "            dist, new_values = self.agent(states)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "            ratio = (new_log_probs - log_probs).exp()\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.c2, 1.0 + self.c2) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (values - returns).pow(2).mean()\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = self.compute_returns(rewards) - values.squeeze(1)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# ppo = PPO(\"InvertedPendulum-v4\")\n",
    "# ppo.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this sorta works: but missing a lot and do not have correct parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Updating\n",
      "Epoch 2\n",
      "Updating\n",
      "Epoch 3\n",
      "Updating\n",
      "Epoch 4\n",
      "Updating\n",
      "Epoch 5\n",
      "Updating\n",
      "Epoch 6\n",
      "Updating\n",
      "Epoch 7\n",
      "Updating\n",
      "Epoch 8\n",
      "Updating\n",
      "Epoch 9\n",
      "Updating\n",
      "Epoch 10\n",
      "Updating\n",
      "Epoch 11\n",
      "Updating\n",
      "Epoch 12\n",
      "Updating\n",
      "Epoch 13\n",
      "Updating\n",
      "Epoch 14\n",
      "Updating\n",
      "Epoch 15\n",
      "Updating\n",
      "Epoch 16\n",
      "Updating\n",
      "Epoch 17\n",
      "Updating\n",
      "Epoch 18\n",
      "Updating\n",
      "Epoch 19\n",
      "Updating\n",
      "Epoch 20\n",
      "Updating\n",
      "Epoch 21\n",
      "Updating\n",
      "Epoch 22\n",
      "Updating\n",
      "Epoch 23\n",
      "Updating\n",
      "Epoch 24\n",
      "Updating\n",
      "Epoch 25\n",
      "Updating\n",
      "Epoch 26\n",
      "Updating\n",
      "Epoch 27\n",
      "Updating\n",
      "Epoch 28\n",
      "Updating\n",
      "Epoch 29\n",
      "Updating\n",
      "Epoch 30\n",
      "Updating\n",
      "Epoch 31\n",
      "Updating\n",
      "Epoch 32\n",
      "Updating\n",
      "Epoch 33\n",
      "Updating\n",
      "Epoch 34\n",
      "Updating\n",
      "Epoch 35\n",
      "Updating\n",
      "Epoch 36\n",
      "Updating\n",
      "Epoch 37\n",
      "Updating\n",
      "Epoch 38\n",
      "Updating\n",
      "Epoch 39\n",
      "Updating\n",
      "Epoch 40\n",
      "Updating\n",
      "Epoch 41\n",
      "Updating\n",
      "Epoch 42\n",
      "Updating\n",
      "Epoch 43\n",
      "Updating\n",
      "Epoch 44\n",
      "Updating\n",
      "Epoch 45\n",
      "Updating\n",
      "Epoch 46\n",
      "Updating\n",
      "Epoch 47\n",
      "Updating\n",
      "Epoch 48\n",
      "Updating\n",
      "Epoch 49\n",
      "Updating\n",
      "Epoch 50\n",
      "Updating\n",
      "Epoch 51\n",
      "Updating\n",
      "Epoch 52\n",
      "Updating\n",
      "Epoch 53\n",
      "Updating\n",
      "Epoch 54\n",
      "Updating\n",
      "Epoch 55\n",
      "Updating\n",
      "Epoch 56\n",
      "Updating\n",
      "Epoch 57\n",
      "Updating\n",
      "Epoch 58\n",
      "Updating\n",
      "Epoch 59\n",
      "Updating\n",
      "Epoch 60\n",
      "Updating\n",
      "Epoch 61\n",
      "Updating\n",
      "Epoch 62\n",
      "Updating\n",
      "Epoch 63\n",
      "Updating\n",
      "Epoch 64\n",
      "Updating\n",
      "Epoch 65\n",
      "Updating\n",
      "Epoch 66\n",
      "Updating\n",
      "Epoch 67\n",
      "Updating\n",
      "Epoch 68\n",
      "Updating\n",
      "Epoch 69\n",
      "Updating\n",
      "Epoch 70\n",
      "Updating\n",
      "Epoch 71\n",
      "Updating\n",
      "Epoch 72\n",
      "Updating\n",
      "Epoch 73\n",
      "Updating\n",
      "Epoch 74\n",
      "Updating\n",
      "Epoch 75\n",
      "Updating\n",
      "Epoch 76\n",
      "Updating\n",
      "Epoch 77\n",
      "Updating\n",
      "Epoch 78\n",
      "Updating\n",
      "Epoch 79\n",
      "Updating\n",
      "Epoch 80\n",
      "Updating\n",
      "Epoch 81\n",
      "Updating\n",
      "Epoch 82\n",
      "Updating\n",
      "Epoch 83\n",
      "Updating\n",
      "Epoch 84\n",
      "Updating\n",
      "Epoch 85\n",
      "Updating\n",
      "Epoch 86\n",
      "Updating\n",
      "Epoch 87\n",
      "Updating\n",
      "Epoch 88\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 146\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m average_reward\n\u001b[0;32m    145\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvertedPendulum-v4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 146\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 52\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     states, actions, rewards, log_probs, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Detach tensors before passing them to the update method\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     states_detached \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39mdetach()\n",
      "Cell \u001b[1;32mIn[40], line 82\u001b[0m, in \u001b[0;36mPPO.generate_trajectories\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m     81\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m---> 82\u001b[0m log_probs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     83\u001b[0m values\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m     85\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this one sorta, but doesnt use correct batch and stuyff\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, epochs=1000, lr=3e-4, gamma=0.99, c1=0.5, c2=0.01, batch_size=64):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            # Detach tensors before passing them to the update method\n",
    "            states_detached = states.detach()\n",
    "            actions_detached = actions.detach()\n",
    "            rewards_detached = rewards.detach()\n",
    "            log_probs_detached = log_probs.detach()\n",
    "            values_detached = values.detach()\n",
    "            print('Updating')\n",
    "            self.update(states_detached, actions_detached, rewards_detached, log_probs_detached, values_detached)\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Ensure the state tensor is of type float\n",
    "            dist, value = self.agent(state_tensor)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "            \n",
    "            states.append(state_tensor)  # Append the state tensor\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "            values.append(value)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset()[0]\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        self.optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        dist, new_values = self.agent(states)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        ratio = (new_log_probs - log_probs).exp()\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.c2, 1.0 + self.c2) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (values - returns).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy\n",
    "\n",
    "        loss.backward(retain_graph=True)  # Retain computational graph\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = self.compute_returns(rewards) - values.squeeze(1)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    def evaluate(self, env, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, _ = self.agent(state_tensor)\n",
    "                action = dist.mean  # Using mean action for evaluation\n",
    "                next_state, reward, done, _, _ = env.step(action.detach().numpy()[0])\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "ppo = PPO(\"InvertedPendulum-v4\")\n",
    "ppo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Create a new environment for evaluation\n",
    "# eval_env = gym.make('InvertedPendulum-v4', render_mode='human')\n",
    "\n",
    "# average_reward = ppo.evaluate(eval_env)\n",
    "# print(f\"Average reward over 10 episodes: {average_reward}\")\n",
    "\n",
    "# # Don't forget to close the evaluation environment when done\n",
    "# eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, horizon=2048, lr=3e-4, epochs=10, minibatch_size=64, gamma=0.99, gae_lambda=0.95):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def train(self, total_timesteps=1000):\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < total_timesteps:\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            # Update timesteps_so_far\n",
    "            print(states.shape)\n",
    "            timesteps_so_far += states.shape[0]\n",
    "\n",
    "            self.update(states, actions, rewards, log_probs, values)\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < self.horizon:\n",
    "            state = self.env.reset()[0]\n",
    "\n",
    "            for _ in range(self.minibatch_size):\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, value = self.agent(state_tensor)\n",
    "                action = dist.sample()\n",
    "\n",
    "                next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "                \n",
    "                states.append(state_tensor)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "                values.append(value)\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    state = self.env.reset()[0]\n",
    "\n",
    "            timesteps_so_far += self.minibatch_size\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        dist, new_values = self.agent(states)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        ratio = (new_log_probs - log_probs).exp()\n",
    "        print(ratio.shape, advantages.shape)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.gae_lambda, 1.0 + self.gae_lambda) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (values - returns).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = actor_loss + critic_loss - 0.01 * entropy\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = self.compute_returns(rewards) - values.squeeze(1)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# ppo = PPO(\"InvertedPendulum-v4\")\n",
    "# ppo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, horizon=2048, lr=3e-4, epochs=10, minibatch_size=64, gamma=0.99, gae_lambda=0.95):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def train(self, total_timesteps=1000000):\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < total_timesteps:\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            # Update timesteps_so_far\n",
    "            timesteps_so_far += states.shape[0]\n",
    "\n",
    "            self.update(states, actions, rewards, log_probs, values)\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < self.horizon:\n",
    "            state = self.env.reset()[0]\n",
    "\n",
    "            for _ in range(self.minibatch_size):\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, value = self.agent(state_tensor)\n",
    "                action = dist.sample()\n",
    "\n",
    "                next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "                \n",
    "                states.append(state_tensor)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "                values.append(value)\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    state = self.env.reset()[0]\n",
    "\n",
    "            timesteps_so_far += self.minibatch_size\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        dist, new_values = self.agent(states)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        # Expand ratio to match the shape of advantages\n",
    "        ratio = (new_log_probs - log_probs).exp().unsqueeze(1)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.gae_lambda, 1.0 + self.gae_lambda) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (values - returns).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = actor_loss + critic_loss - 0.01 * entropy\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = []\n",
    "        G = 0\n",
    "        for r, v in zip(reversed(rewards), reversed(values)):\n",
    "            G = r + self.gamma * G\n",
    "            advantages.insert(0, G - v.item())\n",
    "        advantages = torch.tensor(advantages)\n",
    "        \n",
    "        # Split advantages into minibatches\n",
    "        minibatch_advantages = []\n",
    "        minibatch_size = len(advantages) // self.minibatch_size\n",
    "        for i in range(self.minibatch_size):\n",
    "            start_idx = i * minibatch_size\n",
    "            end_idx = (i + 1) * minibatch_size\n",
    "            minibatch_advantages.append(advantages[start_idx:end_idx])\n",
    "        \n",
    "        # Stack minibatch advantages along a new dimension\n",
    "        minibatch_advantages = torch.stack(minibatch_advantages, dim=1)\n",
    "        \n",
    "        # Calculate mean and standard deviation batch-wise\n",
    "        advantages_mean = minibatch_advantages.mean(dim=1)\n",
    "        advantages_std = minibatch_advantages.std(dim=1) + 1e-8\n",
    "        \n",
    "        # Normalize advantages using batch-wise mean and std\n",
    "        normalized_advantages = (minibatch_advantages - advantages_mean.unsqueeze(1)) / advantages_std.unsqueeze(1)\n",
    "        \n",
    "        return normalized_advantages\n",
    "\n",
    "# ppo = PPO(\"InvertedPendulum-v4\")\n",
    "# ppo.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# joakim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward = 4.0, Total Steps = 4\n",
      "Episode 2, Total Reward = 5.0, Total Steps = 9\n",
      "Episode 3, Total Reward = 4.0, Total Steps = 13\n",
      "Episode 4, Total Reward = 5.0, Total Steps = 18\n",
      "Episode 5, Total Reward = 20.0, Total Steps = 38\n",
      "Episode 6, Total Reward = 7.0, Total Steps = 45\n",
      "Episode 7, Total Reward = 10.0, Total Steps = 55\n",
      "Episode 8, Total Reward = 9.0, Total Steps = 64\n",
      "Episode 9, Total Reward = 13.0, Total Steps = 77\n",
      "Episode 10, Total Reward = 6.0, Total Steps = 83\n",
      "Episode 11, Total Reward = 10.0, Total Steps = 93\n",
      "Episode 12, Total Reward = 8.0, Total Steps = 101\n",
      "Episode 13, Total Reward = 9.0, Total Steps = 110\n",
      "Episode 14, Total Reward = 8.0, Total Steps = 118\n",
      "Episode 15, Total Reward = 5.0, Total Steps = 123\n",
      "Episode 16, Total Reward = 14.0, Total Steps = 137\n",
      "Episode 17, Total Reward = 6.0, Total Steps = 143\n",
      "Episode 18, Total Reward = 13.0, Total Steps = 156\n",
      "Episode 19, Total Reward = 17.0, Total Steps = 173\n",
      "Episode 20, Total Reward = 5.0, Total Steps = 178\n",
      "Episode 21, Total Reward = 3.0, Total Steps = 181\n",
      "Episode 22, Total Reward = 5.0, Total Steps = 186\n",
      "Episode 23, Total Reward = 7.0, Total Steps = 193\n",
      "Episode 24, Total Reward = 7.0, Total Steps = 200\n",
      "Episode 25, Total Reward = 10.0, Total Steps = 210\n",
      "Episode 26, Total Reward = 6.0, Total Steps = 216\n",
      "Episode 27, Total Reward = 3.0, Total Steps = 219\n",
      "Episode 28, Total Reward = 10.0, Total Steps = 229\n",
      "Episode 29, Total Reward = 8.0, Total Steps = 237\n",
      "Episode 30, Total Reward = 8.0, Total Steps = 245\n",
      "Episode 31, Total Reward = 11.0, Total Steps = 256\n",
      "Episode 32, Total Reward = 9.0, Total Steps = 265\n",
      "Episode 33, Total Reward = 11.0, Total Steps = 276\n",
      "Episode 34, Total Reward = 5.0, Total Steps = 281\n",
      "Episode 35, Total Reward = 6.0, Total Steps = 287\n",
      "Episode 36, Total Reward = 9.0, Total Steps = 296\n",
      "Episode 37, Total Reward = 6.0, Total Steps = 302\n",
      "Episode 38, Total Reward = 12.0, Total Steps = 314\n",
      "Episode 39, Total Reward = 8.0, Total Steps = 322\n",
      "Episode 40, Total Reward = 7.0, Total Steps = 329\n",
      "Episode 41, Total Reward = 6.0, Total Steps = 335\n",
      "Episode 42, Total Reward = 6.0, Total Steps = 341\n",
      "Episode 43, Total Reward = 14.0, Total Steps = 355\n",
      "Episode 44, Total Reward = 5.0, Total Steps = 360\n",
      "Episode 45, Total Reward = 5.0, Total Steps = 365\n",
      "Episode 46, Total Reward = 5.0, Total Steps = 370\n",
      "Episode 47, Total Reward = 10.0, Total Steps = 380\n",
      "Episode 48, Total Reward = 11.0, Total Steps = 391\n",
      "Episode 49, Total Reward = 5.0, Total Steps = 396\n",
      "Episode 50, Total Reward = 6.0, Total Steps = 402\n",
      "Episode 51, Total Reward = 8.0, Total Steps = 410\n",
      "Episode 52, Total Reward = 28.0, Total Steps = 438\n",
      "Episode 53, Total Reward = 13.0, Total Steps = 451\n",
      "Episode 54, Total Reward = 6.0, Total Steps = 457\n",
      "Episode 55, Total Reward = 28.0, Total Steps = 485\n",
      "Episode 56, Total Reward = 6.0, Total Steps = 491\n",
      "Episode 57, Total Reward = 4.0, Total Steps = 495\n",
      "Episode 58, Total Reward = 7.0, Total Steps = 502\n",
      "Episode 59, Total Reward = 5.0, Total Steps = 507\n",
      "Episode 60, Total Reward = 4.0, Total Steps = 511\n",
      "Episode 61, Total Reward = 6.0, Total Steps = 517\n",
      "Episode 62, Total Reward = 7.0, Total Steps = 524\n",
      "Episode 63, Total Reward = 8.0, Total Steps = 532\n",
      "Episode 64, Total Reward = 11.0, Total Steps = 543\n",
      "Episode 65, Total Reward = 6.0, Total Steps = 549\n",
      "Episode 66, Total Reward = 6.0, Total Steps = 555\n",
      "Episode 67, Total Reward = 10.0, Total Steps = 565\n",
      "Episode 68, Total Reward = 5.0, Total Steps = 570\n",
      "Episode 69, Total Reward = 4.0, Total Steps = 574\n",
      "Episode 70, Total Reward = 4.0, Total Steps = 578\n",
      "Episode 71, Total Reward = 13.0, Total Steps = 591\n",
      "Episode 72, Total Reward = 8.0, Total Steps = 599\n",
      "Episode 73, Total Reward = 6.0, Total Steps = 605\n",
      "Episode 74, Total Reward = 5.0, Total Steps = 610\n",
      "Episode 75, Total Reward = 4.0, Total Steps = 614\n",
      "Episode 76, Total Reward = 9.0, Total Steps = 623\n",
      "Episode 77, Total Reward = 7.0, Total Steps = 630\n",
      "Episode 78, Total Reward = 27.0, Total Steps = 657\n",
      "Episode 79, Total Reward = 3.0, Total Steps = 660\n",
      "Episode 80, Total Reward = 10.0, Total Steps = 670\n",
      "Episode 81, Total Reward = 9.0, Total Steps = 679\n",
      "Episode 82, Total Reward = 4.0, Total Steps = 683\n",
      "Episode 83, Total Reward = 3.0, Total Steps = 686\n",
      "Episode 84, Total Reward = 7.0, Total Steps = 693\n",
      "Episode 85, Total Reward = 7.0, Total Steps = 700\n",
      "Episode 86, Total Reward = 9.0, Total Steps = 709\n",
      "Episode 87, Total Reward = 6.0, Total Steps = 715\n",
      "Episode 88, Total Reward = 14.0, Total Steps = 729\n",
      "Episode 89, Total Reward = 3.0, Total Steps = 732\n",
      "Episode 90, Total Reward = 11.0, Total Steps = 743\n",
      "Episode 91, Total Reward = 11.0, Total Steps = 754\n",
      "Episode 92, Total Reward = 5.0, Total Steps = 759\n",
      "Episode 93, Total Reward = 6.0, Total Steps = 765\n",
      "Episode 94, Total Reward = 8.0, Total Steps = 773\n",
      "Episode 95, Total Reward = 8.0, Total Steps = 781\n",
      "Episode 96, Total Reward = 15.0, Total Steps = 796\n",
      "Episode 97, Total Reward = 4.0, Total Steps = 800\n",
      "Episode 98, Total Reward = 9.0, Total Steps = 809\n",
      "Episode 99, Total Reward = 8.0, Total Steps = 817\n",
      "Episode 100, Total Reward = 4.0, Total Steps = 821\n",
      "Episode 101, Total Reward = 8.0, Total Steps = 829\n",
      "Episode 102, Total Reward = 6.0, Total Steps = 835\n",
      "Episode 103, Total Reward = 9.0, Total Steps = 844\n",
      "Episode 104, Total Reward = 12.0, Total Steps = 856\n",
      "Episode 105, Total Reward = 18.0, Total Steps = 874\n",
      "Episode 106, Total Reward = 6.0, Total Steps = 880\n",
      "Episode 107, Total Reward = 10.0, Total Steps = 890\n",
      "Episode 108, Total Reward = 8.0, Total Steps = 898\n",
      "Episode 109, Total Reward = 6.0, Total Steps = 904\n",
      "Episode 110, Total Reward = 10.0, Total Steps = 914\n",
      "Episode 111, Total Reward = 10.0, Total Steps = 924\n",
      "Episode 112, Total Reward = 11.0, Total Steps = 935\n",
      "Episode 113, Total Reward = 8.0, Total Steps = 943\n",
      "Episode 114, Total Reward = 4.0, Total Steps = 947\n",
      "Episode 115, Total Reward = 7.0, Total Steps = 954\n",
      "Episode 116, Total Reward = 8.0, Total Steps = 962\n",
      "Episode 117, Total Reward = 8.0, Total Steps = 970\n",
      "Episode 118, Total Reward = 16.0, Total Steps = 986\n",
      "Episode 119, Total Reward = 8.0, Total Steps = 994\n",
      "Episode 120, Total Reward = 5.0, Total Steps = 999\n",
      "Episode 121, Total Reward = 11.0, Total Steps = 1010\n",
      "Episode 122, Total Reward = 6.0, Total Steps = 1016\n",
      "Episode 123, Total Reward = 5.0, Total Steps = 1021\n",
      "Episode 124, Total Reward = 4.0, Total Steps = 1025\n",
      "Episode 125, Total Reward = 5.0, Total Steps = 1030\n",
      "Episode 126, Total Reward = 5.0, Total Steps = 1035\n",
      "Episode 127, Total Reward = 5.0, Total Steps = 1040\n",
      "Episode 128, Total Reward = 16.0, Total Steps = 1056\n",
      "Episode 129, Total Reward = 5.0, Total Steps = 1061\n",
      "Episode 130, Total Reward = 5.0, Total Steps = 1066\n",
      "Episode 131, Total Reward = 7.0, Total Steps = 1073\n",
      "Episode 132, Total Reward = 7.0, Total Steps = 1080\n",
      "Episode 133, Total Reward = 8.0, Total Steps = 1088\n",
      "Episode 134, Total Reward = 7.0, Total Steps = 1095\n",
      "Episode 135, Total Reward = 11.0, Total Steps = 1106\n",
      "Episode 136, Total Reward = 5.0, Total Steps = 1111\n",
      "Episode 137, Total Reward = 4.0, Total Steps = 1115\n",
      "Episode 138, Total Reward = 16.0, Total Steps = 1131\n",
      "Episode 139, Total Reward = 13.0, Total Steps = 1144\n",
      "Episode 140, Total Reward = 5.0, Total Steps = 1149\n",
      "Episode 141, Total Reward = 14.0, Total Steps = 1163\n",
      "Episode 142, Total Reward = 8.0, Total Steps = 1171\n",
      "Episode 143, Total Reward = 8.0, Total Steps = 1179\n",
      "Episode 144, Total Reward = 5.0, Total Steps = 1184\n",
      "Episode 145, Total Reward = 10.0, Total Steps = 1194\n",
      "Episode 146, Total Reward = 6.0, Total Steps = 1200\n",
      "Episode 147, Total Reward = 5.0, Total Steps = 1205\n",
      "Episode 148, Total Reward = 6.0, Total Steps = 1211\n",
      "Episode 149, Total Reward = 6.0, Total Steps = 1217\n",
      "Episode 150, Total Reward = 5.0, Total Steps = 1222\n",
      "Episode 151, Total Reward = 11.0, Total Steps = 1233\n",
      "Episode 152, Total Reward = 8.0, Total Steps = 1241\n",
      "Episode 153, Total Reward = 6.0, Total Steps = 1247\n",
      "Episode 154, Total Reward = 8.0, Total Steps = 1255\n",
      "Episode 155, Total Reward = 4.0, Total Steps = 1259\n",
      "Episode 156, Total Reward = 4.0, Total Steps = 1263\n",
      "Episode 157, Total Reward = 5.0, Total Steps = 1268\n",
      "Episode 158, Total Reward = 5.0, Total Steps = 1273\n",
      "Episode 159, Total Reward = 7.0, Total Steps = 1280\n",
      "Episode 160, Total Reward = 6.0, Total Steps = 1286\n",
      "Episode 161, Total Reward = 7.0, Total Steps = 1293\n",
      "Episode 162, Total Reward = 4.0, Total Steps = 1297\n",
      "Episode 163, Total Reward = 9.0, Total Steps = 1306\n",
      "Episode 164, Total Reward = 9.0, Total Steps = 1315\n",
      "Episode 165, Total Reward = 21.0, Total Steps = 1336\n",
      "Episode 166, Total Reward = 9.0, Total Steps = 1345\n",
      "Episode 167, Total Reward = 18.0, Total Steps = 1363\n",
      "Episode 168, Total Reward = 12.0, Total Steps = 1375\n",
      "Episode 169, Total Reward = 16.0, Total Steps = 1391\n",
      "Episode 170, Total Reward = 4.0, Total Steps = 1395\n",
      "Episode 171, Total Reward = 4.0, Total Steps = 1399\n",
      "Episode 172, Total Reward = 11.0, Total Steps = 1410\n",
      "Episode 173, Total Reward = 8.0, Total Steps = 1418\n",
      "Episode 174, Total Reward = 7.0, Total Steps = 1425\n",
      "Episode 175, Total Reward = 15.0, Total Steps = 1440\n",
      "Episode 176, Total Reward = 8.0, Total Steps = 1448\n",
      "Episode 177, Total Reward = 11.0, Total Steps = 1459\n",
      "Episode 178, Total Reward = 5.0, Total Steps = 1464\n",
      "Episode 179, Total Reward = 5.0, Total Steps = 1469\n",
      "Episode 180, Total Reward = 13.0, Total Steps = 1482\n",
      "Episode 181, Total Reward = 5.0, Total Steps = 1487\n",
      "Episode 182, Total Reward = 6.0, Total Steps = 1493\n",
      "Episode 183, Total Reward = 5.0, Total Steps = 1498\n",
      "Episode 184, Total Reward = 3.0, Total Steps = 1501\n",
      "Episode 185, Total Reward = 8.0, Total Steps = 1509\n",
      "Episode 186, Total Reward = 7.0, Total Steps = 1516\n",
      "Episode 187, Total Reward = 5.0, Total Steps = 1521\n",
      "Episode 188, Total Reward = 8.0, Total Steps = 1529\n",
      "Episode 189, Total Reward = 3.0, Total Steps = 1532\n",
      "Episode 190, Total Reward = 6.0, Total Steps = 1538\n",
      "Episode 191, Total Reward = 8.0, Total Steps = 1546\n",
      "Episode 192, Total Reward = 4.0, Total Steps = 1550\n",
      "Episode 193, Total Reward = 15.0, Total Steps = 1565\n",
      "Episode 194, Total Reward = 5.0, Total Steps = 1570\n",
      "Episode 195, Total Reward = 6.0, Total Steps = 1576\n",
      "Episode 196, Total Reward = 5.0, Total Steps = 1581\n",
      "Episode 197, Total Reward = 20.0, Total Steps = 1601\n",
      "Episode 198, Total Reward = 8.0, Total Steps = 1609\n",
      "Episode 199, Total Reward = 20.0, Total Steps = 1629\n",
      "Episode 200, Total Reward = 4.0, Total Steps = 1633\n",
      "Episode 201, Total Reward = 9.0, Total Steps = 1642\n",
      "Episode 202, Total Reward = 16.0, Total Steps = 1658\n",
      "Episode 203, Total Reward = 8.0, Total Steps = 1666\n",
      "Episode 204, Total Reward = 4.0, Total Steps = 1670\n",
      "Episode 205, Total Reward = 9.0, Total Steps = 1679\n",
      "Episode 206, Total Reward = 5.0, Total Steps = 1684\n",
      "Episode 207, Total Reward = 4.0, Total Steps = 1688\n",
      "Episode 208, Total Reward = 9.0, Total Steps = 1697\n",
      "Episode 209, Total Reward = 19.0, Total Steps = 1716\n",
      "Episode 210, Total Reward = 4.0, Total Steps = 1720\n",
      "Episode 211, Total Reward = 9.0, Total Steps = 1729\n",
      "Episode 212, Total Reward = 7.0, Total Steps = 1736\n",
      "Episode 213, Total Reward = 5.0, Total Steps = 1741\n",
      "Episode 214, Total Reward = 16.0, Total Steps = 1757\n",
      "Episode 215, Total Reward = 15.0, Total Steps = 1772\n",
      "Episode 216, Total Reward = 3.0, Total Steps = 1775\n",
      "Episode 217, Total Reward = 12.0, Total Steps = 1787\n",
      "Episode 218, Total Reward = 6.0, Total Steps = 1793\n",
      "Episode 219, Total Reward = 8.0, Total Steps = 1801\n",
      "Episode 220, Total Reward = 17.0, Total Steps = 1818\n",
      "Episode 221, Total Reward = 7.0, Total Steps = 1825\n",
      "Episode 222, Total Reward = 8.0, Total Steps = 1833\n",
      "Episode 223, Total Reward = 6.0, Total Steps = 1839\n",
      "Episode 224, Total Reward = 5.0, Total Steps = 1844\n",
      "Episode 225, Total Reward = 23.0, Total Steps = 1867\n",
      "Episode 226, Total Reward = 4.0, Total Steps = 1871\n",
      "Episode 227, Total Reward = 4.0, Total Steps = 1875\n",
      "Episode 228, Total Reward = 6.0, Total Steps = 1881\n",
      "Episode 229, Total Reward = 11.0, Total Steps = 1892\n",
      "Episode 230, Total Reward = 5.0, Total Steps = 1897\n",
      "Episode 231, Total Reward = 9.0, Total Steps = 1906\n",
      "Episode 232, Total Reward = 3.0, Total Steps = 1909\n",
      "Episode 233, Total Reward = 9.0, Total Steps = 1918\n",
      "Episode 234, Total Reward = 6.0, Total Steps = 1924\n",
      "Episode 235, Total Reward = 17.0, Total Steps = 1941\n",
      "Episode 236, Total Reward = 5.0, Total Steps = 1946\n",
      "Episode 237, Total Reward = 6.0, Total Steps = 1952\n",
      "Episode 238, Total Reward = 3.0, Total Steps = 1955\n",
      "Episode 239, Total Reward = 6.0, Total Steps = 1961\n",
      "Episode 240, Total Reward = 7.0, Total Steps = 1968\n",
      "Episode 241, Total Reward = 4.0, Total Steps = 1972\n",
      "Episode 242, Total Reward = 17.0, Total Steps = 1989\n",
      "Episode 243, Total Reward = 15.0, Total Steps = 2004\n",
      "Episode 244, Total Reward = 12.0, Total Steps = 2016\n",
      "Episode 245, Total Reward = 5.0, Total Steps = 2021\n",
      "Episode 246, Total Reward = 16.0, Total Steps = 2037\n",
      "Episode 247, Total Reward = 4.0, Total Steps = 2041\n",
      "Episode 248, Total Reward = 5.0, Total Steps = 2046\n",
      "Episode 249, Total Reward = 11.0, Total Steps = 2057\n",
      "Episode 250, Total Reward = 8.0, Total Steps = 2065\n",
      "Episode 251, Total Reward = 9.0, Total Steps = 2074\n",
      "Episode 252, Total Reward = 10.0, Total Steps = 2084\n",
      "Episode 253, Total Reward = 7.0, Total Steps = 2091\n",
      "Episode 254, Total Reward = 10.0, Total Steps = 2101\n",
      "Episode 255, Total Reward = 5.0, Total Steps = 2106\n",
      "Episode 256, Total Reward = 5.0, Total Steps = 2111\n",
      "Episode 257, Total Reward = 22.0, Total Steps = 2133\n",
      "Episode 258, Total Reward = 12.0, Total Steps = 2145\n",
      "Episode 259, Total Reward = 5.0, Total Steps = 2150\n",
      "Episode 260, Total Reward = 5.0, Total Steps = 2155\n",
      "Episode 261, Total Reward = 4.0, Total Steps = 2159\n",
      "Episode 262, Total Reward = 5.0, Total Steps = 2164\n",
      "Episode 263, Total Reward = 10.0, Total Steps = 2174\n",
      "Episode 264, Total Reward = 15.0, Total Steps = 2189\n",
      "Episode 265, Total Reward = 5.0, Total Steps = 2194\n",
      "Episode 266, Total Reward = 12.0, Total Steps = 2206\n",
      "Episode 267, Total Reward = 8.0, Total Steps = 2214\n",
      "Episode 268, Total Reward = 7.0, Total Steps = 2221\n",
      "Episode 269, Total Reward = 5.0, Total Steps = 2226\n",
      "Episode 270, Total Reward = 5.0, Total Steps = 2231\n",
      "Episode 271, Total Reward = 5.0, Total Steps = 2236\n",
      "Episode 272, Total Reward = 5.0, Total Steps = 2241\n",
      "Episode 273, Total Reward = 7.0, Total Steps = 2248\n",
      "Episode 274, Total Reward = 7.0, Total Steps = 2255\n",
      "Episode 275, Total Reward = 6.0, Total Steps = 2261\n",
      "Episode 276, Total Reward = 12.0, Total Steps = 2273\n",
      "Episode 277, Total Reward = 8.0, Total Steps = 2281\n",
      "Episode 278, Total Reward = 5.0, Total Steps = 2286\n",
      "Episode 279, Total Reward = 6.0, Total Steps = 2292\n",
      "Episode 280, Total Reward = 4.0, Total Steps = 2296\n",
      "Episode 281, Total Reward = 6.0, Total Steps = 2302\n",
      "Episode 282, Total Reward = 9.0, Total Steps = 2311\n",
      "Episode 283, Total Reward = 4.0, Total Steps = 2315\n",
      "Episode 284, Total Reward = 7.0, Total Steps = 2322\n",
      "Episode 285, Total Reward = 6.0, Total Steps = 2328\n",
      "Episode 286, Total Reward = 14.0, Total Steps = 2342\n",
      "Episode 287, Total Reward = 6.0, Total Steps = 2348\n",
      "Episode 288, Total Reward = 4.0, Total Steps = 2352\n",
      "Episode 289, Total Reward = 19.0, Total Steps = 2371\n",
      "Episode 290, Total Reward = 6.0, Total Steps = 2377\n",
      "Episode 291, Total Reward = 8.0, Total Steps = 2385\n",
      "Episode 292, Total Reward = 9.0, Total Steps = 2394\n",
      "Episode 293, Total Reward = 5.0, Total Steps = 2399\n",
      "Episode 294, Total Reward = 7.0, Total Steps = 2406\n",
      "Episode 295, Total Reward = 18.0, Total Steps = 2424\n",
      "Episode 296, Total Reward = 13.0, Total Steps = 2437\n",
      "Episode 297, Total Reward = 7.0, Total Steps = 2444\n",
      "Episode 298, Total Reward = 7.0, Total Steps = 2451\n",
      "Episode 299, Total Reward = 7.0, Total Steps = 2458\n",
      "Episode 300, Total Reward = 11.0, Total Steps = 2469\n",
      "Episode 301, Total Reward = 10.0, Total Steps = 2479\n",
      "Episode 302, Total Reward = 4.0, Total Steps = 2483\n",
      "Episode 303, Total Reward = 12.0, Total Steps = 2495\n",
      "Episode 304, Total Reward = 5.0, Total Steps = 2500\n",
      "Episode 305, Total Reward = 4.0, Total Steps = 2504\n",
      "Episode 306, Total Reward = 6.0, Total Steps = 2510\n",
      "Episode 307, Total Reward = 5.0, Total Steps = 2515\n",
      "Episode 308, Total Reward = 5.0, Total Steps = 2520\n",
      "Episode 309, Total Reward = 4.0, Total Steps = 2524\n",
      "Episode 310, Total Reward = 8.0, Total Steps = 2532\n",
      "Episode 311, Total Reward = 5.0, Total Steps = 2537\n",
      "Episode 312, Total Reward = 8.0, Total Steps = 2545\n",
      "Episode 313, Total Reward = 6.0, Total Steps = 2551\n",
      "Episode 314, Total Reward = 8.0, Total Steps = 2559\n",
      "Episode 315, Total Reward = 10.0, Total Steps = 2569\n",
      "Episode 316, Total Reward = 11.0, Total Steps = 2580\n",
      "Episode 317, Total Reward = 18.0, Total Steps = 2598\n",
      "Episode 318, Total Reward = 7.0, Total Steps = 2605\n",
      "Episode 319, Total Reward = 4.0, Total Steps = 2609\n",
      "Episode 320, Total Reward = 7.0, Total Steps = 2616\n",
      "Episode 321, Total Reward = 25.0, Total Steps = 2641\n",
      "Episode 322, Total Reward = 23.0, Total Steps = 2664\n",
      "Episode 323, Total Reward = 7.0, Total Steps = 2671\n",
      "Episode 324, Total Reward = 11.0, Total Steps = 2682\n",
      "Episode 325, Total Reward = 4.0, Total Steps = 2686\n",
      "Episode 326, Total Reward = 9.0, Total Steps = 2695\n",
      "Episode 327, Total Reward = 5.0, Total Steps = 2700\n",
      "Episode 328, Total Reward = 17.0, Total Steps = 2717\n",
      "Episode 329, Total Reward = 7.0, Total Steps = 2724\n",
      "Episode 330, Total Reward = 7.0, Total Steps = 2731\n",
      "Episode 331, Total Reward = 5.0, Total Steps = 2736\n",
      "Episode 332, Total Reward = 8.0, Total Steps = 2744\n",
      "Episode 333, Total Reward = 7.0, Total Steps = 2751\n",
      "Episode 334, Total Reward = 10.0, Total Steps = 2761\n",
      "Episode 335, Total Reward = 6.0, Total Steps = 2767\n",
      "Episode 336, Total Reward = 6.0, Total Steps = 2773\n",
      "Episode 337, Total Reward = 11.0, Total Steps = 2784\n",
      "Episode 338, Total Reward = 4.0, Total Steps = 2788\n",
      "Episode 339, Total Reward = 6.0, Total Steps = 2794\n",
      "Episode 340, Total Reward = 9.0, Total Steps = 2803\n",
      "Episode 341, Total Reward = 21.0, Total Steps = 2824\n",
      "Episode 342, Total Reward = 17.0, Total Steps = 2841\n",
      "Episode 343, Total Reward = 8.0, Total Steps = 2849\n",
      "Episode 344, Total Reward = 26.0, Total Steps = 2875\n",
      "Episode 345, Total Reward = 6.0, Total Steps = 2881\n",
      "Episode 346, Total Reward = 9.0, Total Steps = 2890\n",
      "Episode 347, Total Reward = 7.0, Total Steps = 2897\n",
      "Episode 348, Total Reward = 18.0, Total Steps = 2915\n",
      "Episode 349, Total Reward = 16.0, Total Steps = 2931\n",
      "Episode 350, Total Reward = 6.0, Total Steps = 2937\n",
      "Episode 351, Total Reward = 5.0, Total Steps = 2942\n",
      "Episode 352, Total Reward = 5.0, Total Steps = 2947\n",
      "Episode 353, Total Reward = 5.0, Total Steps = 2952\n",
      "Episode 354, Total Reward = 6.0, Total Steps = 2958\n",
      "Episode 355, Total Reward = 6.0, Total Steps = 2964\n",
      "Episode 356, Total Reward = 7.0, Total Steps = 2971\n",
      "Episode 357, Total Reward = 5.0, Total Steps = 2976\n",
      "Episode 358, Total Reward = 8.0, Total Steps = 2984\n",
      "Episode 359, Total Reward = 6.0, Total Steps = 2990\n",
      "Episode 360, Total Reward = 4.0, Total Steps = 2994\n",
      "Episode 361, Total Reward = 4.0, Total Steps = 2998\n",
      "Episode 362, Total Reward = 4.0, Total Steps = 3002\n",
      "Episode 363, Total Reward = 14.0, Total Steps = 3016\n",
      "Episode 364, Total Reward = 4.0, Total Steps = 3020\n",
      "Episode 365, Total Reward = 4.0, Total Steps = 3024\n",
      "Episode 366, Total Reward = 10.0, Total Steps = 3034\n",
      "Episode 367, Total Reward = 21.0, Total Steps = 3055\n",
      "Episode 368, Total Reward = 8.0, Total Steps = 3063\n",
      "Episode 369, Total Reward = 5.0, Total Steps = 3068\n",
      "Episode 370, Total Reward = 4.0, Total Steps = 3072\n",
      "Episode 371, Total Reward = 5.0, Total Steps = 3077\n",
      "Episode 372, Total Reward = 7.0, Total Steps = 3084\n",
      "Episode 373, Total Reward = 5.0, Total Steps = 3089\n",
      "Episode 374, Total Reward = 6.0, Total Steps = 3095\n",
      "Episode 375, Total Reward = 11.0, Total Steps = 3106\n",
      "Episode 376, Total Reward = 22.0, Total Steps = 3128\n",
      "Episode 377, Total Reward = 14.0, Total Steps = 3142\n",
      "Episode 378, Total Reward = 10.0, Total Steps = 3152\n",
      "Episode 379, Total Reward = 4.0, Total Steps = 3156\n",
      "Episode 380, Total Reward = 4.0, Total Steps = 3160\n",
      "Episode 381, Total Reward = 13.0, Total Steps = 3173\n",
      "Episode 382, Total Reward = 5.0, Total Steps = 3178\n",
      "Episode 383, Total Reward = 6.0, Total Steps = 3184\n",
      "Episode 384, Total Reward = 9.0, Total Steps = 3193\n",
      "Episode 385, Total Reward = 6.0, Total Steps = 3199\n",
      "Episode 386, Total Reward = 10.0, Total Steps = 3209\n",
      "Episode 387, Total Reward = 13.0, Total Steps = 3222\n",
      "Episode 388, Total Reward = 6.0, Total Steps = 3228\n",
      "Episode 389, Total Reward = 10.0, Total Steps = 3238\n",
      "Episode 390, Total Reward = 21.0, Total Steps = 3259\n",
      "Episode 391, Total Reward = 7.0, Total Steps = 3266\n",
      "Episode 392, Total Reward = 5.0, Total Steps = 3271\n",
      "Episode 393, Total Reward = 15.0, Total Steps = 3286\n",
      "Episode 394, Total Reward = 20.0, Total Steps = 3306\n",
      "Episode 395, Total Reward = 5.0, Total Steps = 3311\n",
      "Episode 396, Total Reward = 12.0, Total Steps = 3323\n",
      "Episode 397, Total Reward = 5.0, Total Steps = 3328\n",
      "Episode 398, Total Reward = 10.0, Total Steps = 3338\n",
      "Episode 399, Total Reward = 5.0, Total Steps = 3343\n",
      "Episode 400, Total Reward = 15.0, Total Steps = 3358\n",
      "Episode 401, Total Reward = 6.0, Total Steps = 3364\n",
      "Episode 402, Total Reward = 11.0, Total Steps = 3375\n",
      "Episode 403, Total Reward = 8.0, Total Steps = 3383\n",
      "Episode 404, Total Reward = 12.0, Total Steps = 3395\n",
      "Episode 405, Total Reward = 6.0, Total Steps = 3401\n",
      "Episode 406, Total Reward = 9.0, Total Steps = 3410\n",
      "Episode 407, Total Reward = 14.0, Total Steps = 3424\n",
      "Episode 408, Total Reward = 8.0, Total Steps = 3432\n",
      "Episode 409, Total Reward = 8.0, Total Steps = 3440\n",
      "Episode 410, Total Reward = 7.0, Total Steps = 3447\n",
      "Episode 411, Total Reward = 4.0, Total Steps = 3451\n",
      "Episode 412, Total Reward = 3.0, Total Steps = 3454\n",
      "Episode 413, Total Reward = 8.0, Total Steps = 3462\n",
      "Episode 414, Total Reward = 26.0, Total Steps = 3488\n",
      "Episode 415, Total Reward = 6.0, Total Steps = 3494\n",
      "Episode 416, Total Reward = 3.0, Total Steps = 3497\n",
      "Episode 417, Total Reward = 5.0, Total Steps = 3502\n",
      "Episode 418, Total Reward = 4.0, Total Steps = 3506\n",
      "Episode 419, Total Reward = 4.0, Total Steps = 3510\n",
      "Episode 420, Total Reward = 10.0, Total Steps = 3520\n",
      "Episode 421, Total Reward = 16.0, Total Steps = 3536\n",
      "Episode 422, Total Reward = 4.0, Total Steps = 3540\n",
      "Episode 423, Total Reward = 8.0, Total Steps = 3548\n",
      "Episode 424, Total Reward = 6.0, Total Steps = 3554\n",
      "Episode 425, Total Reward = 4.0, Total Steps = 3558\n",
      "Episode 426, Total Reward = 14.0, Total Steps = 3572\n",
      "Episode 427, Total Reward = 6.0, Total Steps = 3578\n",
      "Episode 428, Total Reward = 10.0, Total Steps = 3588\n",
      "Episode 429, Total Reward = 16.0, Total Steps = 3604\n",
      "Episode 430, Total Reward = 12.0, Total Steps = 3616\n",
      "Episode 431, Total Reward = 10.0, Total Steps = 3626\n",
      "Episode 432, Total Reward = 27.0, Total Steps = 3653\n",
      "Episode 433, Total Reward = 7.0, Total Steps = 3660\n",
      "Episode 434, Total Reward = 6.0, Total Steps = 3666\n",
      "Episode 435, Total Reward = 5.0, Total Steps = 3671\n",
      "Episode 436, Total Reward = 9.0, Total Steps = 3680\n",
      "Episode 437, Total Reward = 10.0, Total Steps = 3690\n",
      "Episode 438, Total Reward = 5.0, Total Steps = 3695\n",
      "Episode 439, Total Reward = 8.0, Total Steps = 3703\n",
      "Episode 440, Total Reward = 5.0, Total Steps = 3708\n",
      "Episode 441, Total Reward = 7.0, Total Steps = 3715\n",
      "Episode 442, Total Reward = 4.0, Total Steps = 3719\n",
      "Episode 443, Total Reward = 4.0, Total Steps = 3723\n",
      "Episode 444, Total Reward = 9.0, Total Steps = 3732\n",
      "Episode 445, Total Reward = 12.0, Total Steps = 3744\n",
      "Episode 446, Total Reward = 4.0, Total Steps = 3748\n",
      "Episode 447, Total Reward = 12.0, Total Steps = 3760\n",
      "Episode 448, Total Reward = 7.0, Total Steps = 3767\n",
      "Episode 449, Total Reward = 6.0, Total Steps = 3773\n",
      "Episode 450, Total Reward = 9.0, Total Steps = 3782\n",
      "Episode 451, Total Reward = 7.0, Total Steps = 3789\n",
      "Episode 452, Total Reward = 4.0, Total Steps = 3793\n",
      "Episode 453, Total Reward = 16.0, Total Steps = 3809\n",
      "Episode 454, Total Reward = 4.0, Total Steps = 3813\n",
      "Episode 455, Total Reward = 5.0, Total Steps = 3818\n",
      "Episode 456, Total Reward = 6.0, Total Steps = 3824\n",
      "Episode 457, Total Reward = 6.0, Total Steps = 3830\n",
      "Episode 458, Total Reward = 4.0, Total Steps = 3834\n",
      "Episode 459, Total Reward = 17.0, Total Steps = 3851\n",
      "Episode 460, Total Reward = 13.0, Total Steps = 3864\n",
      "Episode 461, Total Reward = 9.0, Total Steps = 3873\n",
      "Episode 462, Total Reward = 5.0, Total Steps = 3878\n",
      "Episode 463, Total Reward = 18.0, Total Steps = 3896\n",
      "Episode 464, Total Reward = 17.0, Total Steps = 3913\n",
      "Episode 465, Total Reward = 8.0, Total Steps = 3921\n",
      "Episode 466, Total Reward = 21.0, Total Steps = 3942\n",
      "Episode 467, Total Reward = 22.0, Total Steps = 3964\n",
      "Episode 468, Total Reward = 9.0, Total Steps = 3973\n",
      "Episode 469, Total Reward = 9.0, Total Steps = 3982\n",
      "Episode 470, Total Reward = 8.0, Total Steps = 3990\n",
      "Episode 471, Total Reward = 6.0, Total Steps = 3996\n",
      "Episode 472, Total Reward = 7.0, Total Steps = 4003\n",
      "Episode 473, Total Reward = 8.0, Total Steps = 4011\n",
      "Episode 474, Total Reward = 5.0, Total Steps = 4016\n",
      "Episode 475, Total Reward = 3.0, Total Steps = 4019\n",
      "Episode 476, Total Reward = 6.0, Total Steps = 4025\n",
      "Episode 477, Total Reward = 7.0, Total Steps = 4032\n",
      "Episode 478, Total Reward = 7.0, Total Steps = 4039\n",
      "Episode 479, Total Reward = 10.0, Total Steps = 4049\n",
      "Episode 480, Total Reward = 6.0, Total Steps = 4055\n",
      "Episode 481, Total Reward = 10.0, Total Steps = 4065\n",
      "Episode 482, Total Reward = 9.0, Total Steps = 4074\n",
      "Episode 483, Total Reward = 29.0, Total Steps = 4103\n",
      "Episode 484, Total Reward = 6.0, Total Steps = 4109\n",
      "Episode 485, Total Reward = 8.0, Total Steps = 4117\n",
      "Episode 486, Total Reward = 6.0, Total Steps = 4123\n",
      "Episode 487, Total Reward = 7.0, Total Steps = 4130\n",
      "Episode 488, Total Reward = 15.0, Total Steps = 4145\n",
      "Episode 489, Total Reward = 9.0, Total Steps = 4154\n",
      "Episode 490, Total Reward = 24.0, Total Steps = 4178\n",
      "Episode 491, Total Reward = 18.0, Total Steps = 4196\n",
      "Episode 492, Total Reward = 8.0, Total Steps = 4204\n",
      "Episode 493, Total Reward = 19.0, Total Steps = 4223\n",
      "Episode 494, Total Reward = 9.0, Total Steps = 4232\n",
      "Episode 495, Total Reward = 7.0, Total Steps = 4239\n",
      "Episode 496, Total Reward = 5.0, Total Steps = 4244\n",
      "Episode 497, Total Reward = 8.0, Total Steps = 4252\n",
      "Episode 498, Total Reward = 10.0, Total Steps = 4262\n",
      "Episode 499, Total Reward = 9.0, Total Steps = 4271\n",
      "Episode 500, Total Reward = 4.0, Total Steps = 4275\n",
      "Episode 501, Total Reward = 6.0, Total Steps = 4281\n",
      "Episode 502, Total Reward = 5.0, Total Steps = 4286\n",
      "Episode 503, Total Reward = 6.0, Total Steps = 4292\n",
      "Episode 504, Total Reward = 25.0, Total Steps = 4317\n",
      "Episode 505, Total Reward = 9.0, Total Steps = 4326\n",
      "Episode 506, Total Reward = 8.0, Total Steps = 4334\n",
      "Episode 507, Total Reward = 5.0, Total Steps = 4339\n",
      "Episode 508, Total Reward = 6.0, Total Steps = 4345\n",
      "Episode 509, Total Reward = 14.0, Total Steps = 4359\n",
      "Episode 510, Total Reward = 10.0, Total Steps = 4369\n",
      "Episode 511, Total Reward = 21.0, Total Steps = 4390\n",
      "Episode 512, Total Reward = 8.0, Total Steps = 4398\n",
      "Episode 513, Total Reward = 18.0, Total Steps = 4416\n",
      "Episode 514, Total Reward = 4.0, Total Steps = 4420\n",
      "Episode 515, Total Reward = 8.0, Total Steps = 4428\n",
      "Episode 516, Total Reward = 8.0, Total Steps = 4436\n",
      "Episode 517, Total Reward = 9.0, Total Steps = 4445\n",
      "Episode 518, Total Reward = 4.0, Total Steps = 4449\n",
      "Episode 519, Total Reward = 5.0, Total Steps = 4454\n",
      "Episode 520, Total Reward = 4.0, Total Steps = 4458\n",
      "Episode 521, Total Reward = 6.0, Total Steps = 4464\n",
      "Episode 522, Total Reward = 7.0, Total Steps = 4471\n",
      "Episode 523, Total Reward = 9.0, Total Steps = 4480\n",
      "Episode 524, Total Reward = 5.0, Total Steps = 4485\n",
      "Episode 525, Total Reward = 14.0, Total Steps = 4499\n",
      "Episode 526, Total Reward = 16.0, Total Steps = 4515\n",
      "Episode 527, Total Reward = 11.0, Total Steps = 4526\n",
      "Episode 528, Total Reward = 8.0, Total Steps = 4534\n",
      "Episode 529, Total Reward = 4.0, Total Steps = 4538\n",
      "Episode 530, Total Reward = 8.0, Total Steps = 4546\n",
      "Episode 531, Total Reward = 7.0, Total Steps = 4553\n",
      "Episode 532, Total Reward = 4.0, Total Steps = 4557\n",
      "Episode 533, Total Reward = 4.0, Total Steps = 4561\n",
      "Episode 534, Total Reward = 7.0, Total Steps = 4568\n",
      "Episode 535, Total Reward = 13.0, Total Steps = 4581\n",
      "Episode 536, Total Reward = 6.0, Total Steps = 4587\n",
      "Episode 537, Total Reward = 4.0, Total Steps = 4591\n",
      "Episode 538, Total Reward = 4.0, Total Steps = 4595\n",
      "Episode 539, Total Reward = 7.0, Total Steps = 4602\n",
      "Episode 540, Total Reward = 40.0, Total Steps = 4642\n",
      "Episode 541, Total Reward = 12.0, Total Steps = 4654\n",
      "Episode 542, Total Reward = 7.0, Total Steps = 4661\n",
      "Episode 543, Total Reward = 19.0, Total Steps = 4680\n",
      "Episode 544, Total Reward = 10.0, Total Steps = 4690\n",
      "Episode 545, Total Reward = 8.0, Total Steps = 4698\n",
      "Episode 546, Total Reward = 3.0, Total Steps = 4701\n",
      "Episode 547, Total Reward = 8.0, Total Steps = 4709\n",
      "Episode 548, Total Reward = 6.0, Total Steps = 4715\n",
      "Episode 549, Total Reward = 7.0, Total Steps = 4722\n",
      "Episode 550, Total Reward = 21.0, Total Steps = 4743\n",
      "Episode 551, Total Reward = 20.0, Total Steps = 4763\n",
      "Episode 552, Total Reward = 7.0, Total Steps = 4770\n",
      "Episode 553, Total Reward = 5.0, Total Steps = 4775\n",
      "Episode 554, Total Reward = 17.0, Total Steps = 4792\n",
      "Episode 555, Total Reward = 22.0, Total Steps = 4814\n",
      "Episode 556, Total Reward = 7.0, Total Steps = 4821\n",
      "Episode 557, Total Reward = 13.0, Total Steps = 4834\n",
      "Episode 558, Total Reward = 10.0, Total Steps = 4844\n",
      "Episode 559, Total Reward = 5.0, Total Steps = 4849\n",
      "Episode 560, Total Reward = 5.0, Total Steps = 4854\n",
      "Episode 561, Total Reward = 5.0, Total Steps = 4859\n",
      "Episode 562, Total Reward = 18.0, Total Steps = 4877\n",
      "Episode 563, Total Reward = 9.0, Total Steps = 4886\n",
      "Episode 564, Total Reward = 10.0, Total Steps = 4896\n",
      "Episode 565, Total Reward = 5.0, Total Steps = 4901\n",
      "Episode 566, Total Reward = 8.0, Total Steps = 4909\n",
      "Episode 567, Total Reward = 9.0, Total Steps = 4918\n",
      "Episode 568, Total Reward = 5.0, Total Steps = 4923\n",
      "Episode 569, Total Reward = 6.0, Total Steps = 4929\n",
      "Episode 570, Total Reward = 4.0, Total Steps = 4933\n",
      "Episode 571, Total Reward = 13.0, Total Steps = 4946\n",
      "Episode 572, Total Reward = 5.0, Total Steps = 4951\n",
      "Episode 573, Total Reward = 5.0, Total Steps = 4956\n",
      "Episode 574, Total Reward = 12.0, Total Steps = 4968\n",
      "Episode 575, Total Reward = 7.0, Total Steps = 4975\n",
      "Episode 576, Total Reward = 6.0, Total Steps = 4981\n",
      "Episode 577, Total Reward = 6.0, Total Steps = 4987\n",
      "Episode 578, Total Reward = 5.0, Total Steps = 4992\n",
      "Episode 579, Total Reward = 4.0, Total Steps = 4996\n",
      "Episode 580, Total Reward = 11.0, Total Steps = 5007\n",
      "Episode 581, Total Reward = 4.0, Total Steps = 5011\n",
      "Episode 582, Total Reward = 5.0, Total Steps = 5016\n",
      "Episode 583, Total Reward = 8.0, Total Steps = 5024\n",
      "Episode 584, Total Reward = 10.0, Total Steps = 5034\n",
      "Episode 585, Total Reward = 7.0, Total Steps = 5041\n",
      "Episode 586, Total Reward = 12.0, Total Steps = 5053\n",
      "Episode 587, Total Reward = 6.0, Total Steps = 5059\n",
      "Episode 588, Total Reward = 6.0, Total Steps = 5065\n",
      "Episode 589, Total Reward = 6.0, Total Steps = 5071\n",
      "Episode 590, Total Reward = 4.0, Total Steps = 5075\n",
      "Episode 591, Total Reward = 13.0, Total Steps = 5088\n",
      "Episode 592, Total Reward = 7.0, Total Steps = 5095\n",
      "Episode 593, Total Reward = 14.0, Total Steps = 5109\n",
      "Episode 594, Total Reward = 7.0, Total Steps = 5116\n",
      "Episode 595, Total Reward = 7.0, Total Steps = 5123\n",
      "Episode 596, Total Reward = 4.0, Total Steps = 5127\n",
      "Episode 597, Total Reward = 12.0, Total Steps = 5139\n",
      "Episode 598, Total Reward = 8.0, Total Steps = 5147\n",
      "Episode 599, Total Reward = 11.0, Total Steps = 5158\n",
      "Episode 600, Total Reward = 12.0, Total Steps = 5170\n",
      "Episode 601, Total Reward = 24.0, Total Steps = 5194\n",
      "Episode 602, Total Reward = 4.0, Total Steps = 5198\n",
      "Episode 603, Total Reward = 10.0, Total Steps = 5208\n",
      "Episode 604, Total Reward = 19.0, Total Steps = 5227\n",
      "Episode 605, Total Reward = 6.0, Total Steps = 5233\n",
      "Episode 606, Total Reward = 26.0, Total Steps = 5259\n",
      "Episode 607, Total Reward = 4.0, Total Steps = 5263\n",
      "Episode 608, Total Reward = 5.0, Total Steps = 5268\n",
      "Episode 609, Total Reward = 10.0, Total Steps = 5278\n",
      "Episode 610, Total Reward = 7.0, Total Steps = 5285\n",
      "Episode 611, Total Reward = 5.0, Total Steps = 5290\n",
      "Episode 612, Total Reward = 13.0, Total Steps = 5303\n",
      "Episode 613, Total Reward = 9.0, Total Steps = 5312\n",
      "Episode 614, Total Reward = 8.0, Total Steps = 5320\n",
      "Episode 615, Total Reward = 16.0, Total Steps = 5336\n",
      "Episode 616, Total Reward = 10.0, Total Steps = 5346\n",
      "Episode 617, Total Reward = 14.0, Total Steps = 5360\n",
      "Episode 618, Total Reward = 5.0, Total Steps = 5365\n",
      "Episode 619, Total Reward = 19.0, Total Steps = 5384\n",
      "Episode 620, Total Reward = 7.0, Total Steps = 5391\n",
      "Episode 621, Total Reward = 22.0, Total Steps = 5413\n",
      "Episode 622, Total Reward = 16.0, Total Steps = 5429\n",
      "Episode 623, Total Reward = 3.0, Total Steps = 5432\n",
      "Episode 624, Total Reward = 4.0, Total Steps = 5436\n",
      "Episode 625, Total Reward = 5.0, Total Steps = 5441\n",
      "Episode 626, Total Reward = 6.0, Total Steps = 5447\n",
      "Episode 627, Total Reward = 8.0, Total Steps = 5455\n",
      "Episode 628, Total Reward = 8.0, Total Steps = 5463\n",
      "Episode 629, Total Reward = 14.0, Total Steps = 5477\n",
      "Episode 630, Total Reward = 4.0, Total Steps = 5481\n",
      "Episode 631, Total Reward = 4.0, Total Steps = 5485\n",
      "Episode 632, Total Reward = 13.0, Total Steps = 5498\n",
      "Episode 633, Total Reward = 5.0, Total Steps = 5503\n",
      "Episode 634, Total Reward = 8.0, Total Steps = 5511\n",
      "Episode 635, Total Reward = 5.0, Total Steps = 5516\n",
      "Episode 636, Total Reward = 16.0, Total Steps = 5532\n",
      "Episode 637, Total Reward = 12.0, Total Steps = 5544\n",
      "Episode 638, Total Reward = 12.0, Total Steps = 5556\n",
      "Episode 639, Total Reward = 4.0, Total Steps = 5560\n",
      "Episode 640, Total Reward = 8.0, Total Steps = 5568\n",
      "Episode 641, Total Reward = 10.0, Total Steps = 5578\n",
      "Episode 642, Total Reward = 16.0, Total Steps = 5594\n",
      "Episode 643, Total Reward = 13.0, Total Steps = 5607\n",
      "Episode 644, Total Reward = 17.0, Total Steps = 5624\n",
      "Episode 645, Total Reward = 5.0, Total Steps = 5629\n",
      "Episode 646, Total Reward = 6.0, Total Steps = 5635\n",
      "Episode 647, Total Reward = 7.0, Total Steps = 5642\n",
      "Episode 648, Total Reward = 4.0, Total Steps = 5646\n",
      "Episode 649, Total Reward = 9.0, Total Steps = 5655\n",
      "Episode 650, Total Reward = 6.0, Total Steps = 5661\n",
      "Episode 651, Total Reward = 4.0, Total Steps = 5665\n",
      "Episode 652, Total Reward = 7.0, Total Steps = 5672\n",
      "Episode 653, Total Reward = 11.0, Total Steps = 5683\n",
      "Episode 654, Total Reward = 9.0, Total Steps = 5692\n",
      "Episode 655, Total Reward = 9.0, Total Steps = 5701\n",
      "Episode 656, Total Reward = 9.0, Total Steps = 5710\n",
      "Episode 657, Total Reward = 5.0, Total Steps = 5715\n",
      "Episode 658, Total Reward = 6.0, Total Steps = 5721\n",
      "Episode 659, Total Reward = 4.0, Total Steps = 5725\n",
      "Episode 660, Total Reward = 7.0, Total Steps = 5732\n",
      "Episode 661, Total Reward = 17.0, Total Steps = 5749\n",
      "Episode 662, Total Reward = 11.0, Total Steps = 5760\n",
      "Episode 663, Total Reward = 9.0, Total Steps = 5769\n",
      "Episode 664, Total Reward = 4.0, Total Steps = 5773\n",
      "Episode 665, Total Reward = 18.0, Total Steps = 5791\n",
      "Episode 666, Total Reward = 7.0, Total Steps = 5798\n",
      "Episode 667, Total Reward = 7.0, Total Steps = 5805\n",
      "Episode 668, Total Reward = 6.0, Total Steps = 5811\n",
      "Episode 669, Total Reward = 11.0, Total Steps = 5822\n",
      "Episode 670, Total Reward = 10.0, Total Steps = 5832\n",
      "Episode 671, Total Reward = 23.0, Total Steps = 5855\n",
      "Episode 672, Total Reward = 5.0, Total Steps = 5860\n",
      "Episode 673, Total Reward = 7.0, Total Steps = 5867\n",
      "Episode 674, Total Reward = 18.0, Total Steps = 5885\n",
      "Episode 675, Total Reward = 4.0, Total Steps = 5889\n",
      "Episode 676, Total Reward = 4.0, Total Steps = 5893\n",
      "Episode 677, Total Reward = 13.0, Total Steps = 5906\n",
      "Episode 678, Total Reward = 6.0, Total Steps = 5912\n",
      "Episode 679, Total Reward = 11.0, Total Steps = 5923\n",
      "Episode 680, Total Reward = 9.0, Total Steps = 5932\n",
      "Episode 681, Total Reward = 8.0, Total Steps = 5940\n",
      "Episode 682, Total Reward = 5.0, Total Steps = 5945\n",
      "Episode 683, Total Reward = 7.0, Total Steps = 5952\n",
      "Episode 684, Total Reward = 7.0, Total Steps = 5959\n",
      "Episode 685, Total Reward = 4.0, Total Steps = 5963\n",
      "Episode 686, Total Reward = 6.0, Total Steps = 5969\n",
      "Episode 687, Total Reward = 5.0, Total Steps = 5974\n",
      "Episode 688, Total Reward = 6.0, Total Steps = 5980\n",
      "Episode 689, Total Reward = 8.0, Total Steps = 5988\n",
      "Episode 690, Total Reward = 10.0, Total Steps = 5998\n",
      "Episode 691, Total Reward = 10.0, Total Steps = 6008\n",
      "Episode 692, Total Reward = 15.0, Total Steps = 6023\n",
      "Episode 693, Total Reward = 10.0, Total Steps = 6033\n",
      "Episode 694, Total Reward = 14.0, Total Steps = 6047\n",
      "Episode 695, Total Reward = 12.0, Total Steps = 6059\n",
      "Episode 696, Total Reward = 14.0, Total Steps = 6073\n",
      "Episode 697, Total Reward = 6.0, Total Steps = 6079\n",
      "Episode 698, Total Reward = 7.0, Total Steps = 6086\n",
      "Episode 699, Total Reward = 9.0, Total Steps = 6095\n",
      "Episode 700, Total Reward = 6.0, Total Steps = 6101\n",
      "Episode 701, Total Reward = 7.0, Total Steps = 6108\n",
      "Episode 702, Total Reward = 4.0, Total Steps = 6112\n",
      "Episode 703, Total Reward = 5.0, Total Steps = 6117\n",
      "Episode 704, Total Reward = 7.0, Total Steps = 6124\n",
      "Episode 705, Total Reward = 5.0, Total Steps = 6129\n",
      "Episode 706, Total Reward = 7.0, Total Steps = 6136\n",
      "Episode 707, Total Reward = 4.0, Total Steps = 6140\n",
      "Episode 708, Total Reward = 7.0, Total Steps = 6147\n",
      "Episode 709, Total Reward = 7.0, Total Steps = 6154\n",
      "Episode 710, Total Reward = 18.0, Total Steps = 6172\n",
      "Episode 711, Total Reward = 27.0, Total Steps = 6199\n",
      "Episode 712, Total Reward = 6.0, Total Steps = 6205\n",
      "Episode 713, Total Reward = 7.0, Total Steps = 6212\n",
      "Episode 714, Total Reward = 8.0, Total Steps = 6220\n",
      "Episode 715, Total Reward = 7.0, Total Steps = 6227\n",
      "Episode 716, Total Reward = 7.0, Total Steps = 6234\n",
      "Episode 717, Total Reward = 12.0, Total Steps = 6246\n",
      "Episode 718, Total Reward = 32.0, Total Steps = 6278\n",
      "Episode 719, Total Reward = 7.0, Total Steps = 6285\n",
      "Episode 720, Total Reward = 8.0, Total Steps = 6293\n",
      "Episode 721, Total Reward = 5.0, Total Steps = 6298\n",
      "Episode 722, Total Reward = 7.0, Total Steps = 6305\n",
      "Episode 723, Total Reward = 32.0, Total Steps = 6337\n",
      "Episode 724, Total Reward = 3.0, Total Steps = 6340\n",
      "Episode 725, Total Reward = 15.0, Total Steps = 6355\n",
      "Episode 726, Total Reward = 22.0, Total Steps = 6377\n",
      "Episode 727, Total Reward = 9.0, Total Steps = 6386\n",
      "Episode 728, Total Reward = 13.0, Total Steps = 6399\n",
      "Episode 729, Total Reward = 3.0, Total Steps = 6402\n",
      "Episode 730, Total Reward = 5.0, Total Steps = 6407\n",
      "Episode 731, Total Reward = 5.0, Total Steps = 6412\n",
      "Episode 732, Total Reward = 12.0, Total Steps = 6424\n",
      "Episode 733, Total Reward = 10.0, Total Steps = 6434\n",
      "Episode 734, Total Reward = 5.0, Total Steps = 6439\n",
      "Episode 735, Total Reward = 20.0, Total Steps = 6459\n",
      "Episode 736, Total Reward = 19.0, Total Steps = 6478\n",
      "Episode 737, Total Reward = 13.0, Total Steps = 6491\n",
      "Episode 738, Total Reward = 34.0, Total Steps = 6525\n",
      "Episode 739, Total Reward = 10.0, Total Steps = 6535\n",
      "Episode 740, Total Reward = 12.0, Total Steps = 6547\n",
      "Episode 741, Total Reward = 4.0, Total Steps = 6551\n",
      "Episode 742, Total Reward = 21.0, Total Steps = 6572\n",
      "Episode 743, Total Reward = 13.0, Total Steps = 6585\n",
      "Episode 744, Total Reward = 4.0, Total Steps = 6589\n",
      "Episode 745, Total Reward = 4.0, Total Steps = 6593\n",
      "Episode 746, Total Reward = 4.0, Total Steps = 6597\n",
      "Episode 747, Total Reward = 19.0, Total Steps = 6616\n",
      "Episode 748, Total Reward = 6.0, Total Steps = 6622\n",
      "Episode 749, Total Reward = 6.0, Total Steps = 6628\n",
      "Episode 750, Total Reward = 4.0, Total Steps = 6632\n",
      "Episode 751, Total Reward = 28.0, Total Steps = 6660\n",
      "Episode 752, Total Reward = 5.0, Total Steps = 6665\n",
      "Episode 753, Total Reward = 8.0, Total Steps = 6673\n",
      "Episode 754, Total Reward = 10.0, Total Steps = 6683\n",
      "Episode 755, Total Reward = 9.0, Total Steps = 6692\n",
      "Episode 756, Total Reward = 43.0, Total Steps = 6735\n",
      "Episode 757, Total Reward = 7.0, Total Steps = 6742\n",
      "Episode 758, Total Reward = 39.0, Total Steps = 6781\n",
      "Episode 759, Total Reward = 13.0, Total Steps = 6794\n",
      "Episode 760, Total Reward = 10.0, Total Steps = 6804\n",
      "Episode 761, Total Reward = 9.0, Total Steps = 6813\n",
      "Episode 762, Total Reward = 9.0, Total Steps = 6822\n",
      "Episode 763, Total Reward = 7.0, Total Steps = 6829\n",
      "Episode 764, Total Reward = 6.0, Total Steps = 6835\n",
      "Episode 765, Total Reward = 16.0, Total Steps = 6851\n",
      "Episode 766, Total Reward = 13.0, Total Steps = 6864\n",
      "Episode 767, Total Reward = 13.0, Total Steps = 6877\n",
      "Episode 768, Total Reward = 12.0, Total Steps = 6889\n",
      "Episode 769, Total Reward = 11.0, Total Steps = 6900\n",
      "Episode 770, Total Reward = 10.0, Total Steps = 6910\n",
      "Episode 771, Total Reward = 13.0, Total Steps = 6923\n",
      "Episode 772, Total Reward = 4.0, Total Steps = 6927\n",
      "Episode 773, Total Reward = 5.0, Total Steps = 6932\n",
      "Episode 774, Total Reward = 11.0, Total Steps = 6943\n",
      "Episode 775, Total Reward = 7.0, Total Steps = 6950\n",
      "Episode 776, Total Reward = 4.0, Total Steps = 6954\n",
      "Episode 777, Total Reward = 8.0, Total Steps = 6962\n",
      "Episode 778, Total Reward = 9.0, Total Steps = 6971\n",
      "Episode 779, Total Reward = 4.0, Total Steps = 6975\n",
      "Episode 780, Total Reward = 8.0, Total Steps = 6983\n",
      "Episode 781, Total Reward = 6.0, Total Steps = 6989\n",
      "Episode 782, Total Reward = 6.0, Total Steps = 6995\n",
      "Episode 783, Total Reward = 5.0, Total Steps = 7000\n",
      "Episode 784, Total Reward = 6.0, Total Steps = 7006\n",
      "Episode 785, Total Reward = 5.0, Total Steps = 7011\n",
      "Episode 786, Total Reward = 5.0, Total Steps = 7016\n",
      "Episode 787, Total Reward = 15.0, Total Steps = 7031\n",
      "Episode 788, Total Reward = 13.0, Total Steps = 7044\n",
      "Episode 789, Total Reward = 31.0, Total Steps = 7075\n",
      "Episode 790, Total Reward = 7.0, Total Steps = 7082\n",
      "Episode 791, Total Reward = 7.0, Total Steps = 7089\n",
      "Episode 792, Total Reward = 4.0, Total Steps = 7093\n",
      "Episode 793, Total Reward = 17.0, Total Steps = 7110\n",
      "Episode 794, Total Reward = 28.0, Total Steps = 7138\n",
      "Episode 795, Total Reward = 6.0, Total Steps = 7144\n",
      "Episode 796, Total Reward = 7.0, Total Steps = 7151\n",
      "Episode 797, Total Reward = 7.0, Total Steps = 7158\n",
      "Episode 798, Total Reward = 6.0, Total Steps = 7164\n",
      "Episode 799, Total Reward = 6.0, Total Steps = 7170\n",
      "Episode 800, Total Reward = 5.0, Total Steps = 7175\n",
      "Episode 801, Total Reward = 9.0, Total Steps = 7184\n",
      "Episode 802, Total Reward = 5.0, Total Steps = 7189\n",
      "Episode 803, Total Reward = 40.0, Total Steps = 7229\n",
      "Episode 804, Total Reward = 9.0, Total Steps = 7238\n",
      "Episode 805, Total Reward = 9.0, Total Steps = 7247\n",
      "Episode 806, Total Reward = 6.0, Total Steps = 7253\n",
      "Episode 807, Total Reward = 7.0, Total Steps = 7260\n",
      "Episode 808, Total Reward = 10.0, Total Steps = 7270\n",
      "Episode 809, Total Reward = 13.0, Total Steps = 7283\n",
      "Episode 810, Total Reward = 5.0, Total Steps = 7288\n",
      "Episode 811, Total Reward = 7.0, Total Steps = 7295\n",
      "Episode 812, Total Reward = 9.0, Total Steps = 7304\n",
      "Episode 813, Total Reward = 11.0, Total Steps = 7315\n",
      "Episode 814, Total Reward = 6.0, Total Steps = 7321\n",
      "Episode 815, Total Reward = 7.0, Total Steps = 7328\n",
      "Episode 816, Total Reward = 25.0, Total Steps = 7353\n",
      "Episode 817, Total Reward = 12.0, Total Steps = 7365\n",
      "Episode 818, Total Reward = 15.0, Total Steps = 7380\n",
      "Episode 819, Total Reward = 11.0, Total Steps = 7391\n",
      "Episode 820, Total Reward = 7.0, Total Steps = 7398\n",
      "Episode 821, Total Reward = 15.0, Total Steps = 7413\n",
      "Episode 822, Total Reward = 11.0, Total Steps = 7424\n",
      "Episode 823, Total Reward = 6.0, Total Steps = 7430\n",
      "Episode 824, Total Reward = 8.0, Total Steps = 7438\n",
      "Episode 825, Total Reward = 15.0, Total Steps = 7453\n",
      "Episode 826, Total Reward = 8.0, Total Steps = 7461\n",
      "Episode 827, Total Reward = 5.0, Total Steps = 7466\n",
      "Episode 828, Total Reward = 6.0, Total Steps = 7472\n",
      "Episode 829, Total Reward = 13.0, Total Steps = 7485\n",
      "Episode 830, Total Reward = 24.0, Total Steps = 7509\n",
      "Episode 831, Total Reward = 20.0, Total Steps = 7529\n",
      "Episode 832, Total Reward = 11.0, Total Steps = 7540\n",
      "Episode 833, Total Reward = 9.0, Total Steps = 7549\n",
      "Episode 834, Total Reward = 4.0, Total Steps = 7553\n",
      "Episode 835, Total Reward = 8.0, Total Steps = 7561\n",
      "Episode 836, Total Reward = 19.0, Total Steps = 7580\n",
      "Episode 837, Total Reward = 6.0, Total Steps = 7586\n",
      "Episode 838, Total Reward = 10.0, Total Steps = 7596\n",
      "Episode 839, Total Reward = 9.0, Total Steps = 7605\n",
      "Episode 840, Total Reward = 3.0, Total Steps = 7608\n",
      "Episode 841, Total Reward = 15.0, Total Steps = 7623\n",
      "Episode 842, Total Reward = 7.0, Total Steps = 7630\n",
      "Episode 843, Total Reward = 15.0, Total Steps = 7645\n",
      "Episode 844, Total Reward = 6.0, Total Steps = 7651\n",
      "Episode 845, Total Reward = 13.0, Total Steps = 7664\n",
      "Episode 846, Total Reward = 12.0, Total Steps = 7676\n",
      "Episode 847, Total Reward = 8.0, Total Steps = 7684\n",
      "Episode 848, Total Reward = 15.0, Total Steps = 7699\n",
      "Episode 849, Total Reward = 13.0, Total Steps = 7712\n",
      "Episode 850, Total Reward = 7.0, Total Steps = 7719\n",
      "Episode 851, Total Reward = 3.0, Total Steps = 7722\n",
      "Episode 852, Total Reward = 5.0, Total Steps = 7727\n",
      "Episode 853, Total Reward = 13.0, Total Steps = 7740\n",
      "Episode 854, Total Reward = 4.0, Total Steps = 7744\n",
      "Episode 855, Total Reward = 26.0, Total Steps = 7770\n",
      "Episode 856, Total Reward = 6.0, Total Steps = 7776\n",
      "Episode 857, Total Reward = 13.0, Total Steps = 7789\n",
      "Episode 858, Total Reward = 5.0, Total Steps = 7794\n",
      "Episode 859, Total Reward = 15.0, Total Steps = 7809\n",
      "Episode 860, Total Reward = 6.0, Total Steps = 7815\n",
      "Episode 861, Total Reward = 12.0, Total Steps = 7827\n",
      "Episode 862, Total Reward = 4.0, Total Steps = 7831\n",
      "Episode 863, Total Reward = 16.0, Total Steps = 7847\n",
      "Episode 864, Total Reward = 12.0, Total Steps = 7859\n",
      "Episode 865, Total Reward = 5.0, Total Steps = 7864\n",
      "Episode 866, Total Reward = 12.0, Total Steps = 7876\n",
      "Episode 867, Total Reward = 9.0, Total Steps = 7885\n",
      "Episode 868, Total Reward = 4.0, Total Steps = 7889\n",
      "Episode 869, Total Reward = 8.0, Total Steps = 7897\n",
      "Episode 870, Total Reward = 18.0, Total Steps = 7915\n",
      "Episode 871, Total Reward = 13.0, Total Steps = 7928\n",
      "Episode 872, Total Reward = 12.0, Total Steps = 7940\n",
      "Episode 873, Total Reward = 6.0, Total Steps = 7946\n",
      "Episode 874, Total Reward = 8.0, Total Steps = 7954\n",
      "Episode 875, Total Reward = 11.0, Total Steps = 7965\n",
      "Episode 876, Total Reward = 37.0, Total Steps = 8002\n",
      "Episode 877, Total Reward = 14.0, Total Steps = 8016\n",
      "Episode 878, Total Reward = 4.0, Total Steps = 8020\n",
      "Episode 879, Total Reward = 7.0, Total Steps = 8027\n",
      "Episode 880, Total Reward = 13.0, Total Steps = 8040\n",
      "Episode 881, Total Reward = 6.0, Total Steps = 8046\n",
      "Episode 882, Total Reward = 10.0, Total Steps = 8056\n",
      "Episode 883, Total Reward = 4.0, Total Steps = 8060\n",
      "Episode 884, Total Reward = 14.0, Total Steps = 8074\n",
      "Episode 885, Total Reward = 4.0, Total Steps = 8078\n",
      "Episode 886, Total Reward = 8.0, Total Steps = 8086\n",
      "Episode 887, Total Reward = 10.0, Total Steps = 8096\n",
      "Episode 888, Total Reward = 6.0, Total Steps = 8102\n",
      "Episode 889, Total Reward = 8.0, Total Steps = 8110\n",
      "Episode 890, Total Reward = 13.0, Total Steps = 8123\n",
      "Episode 891, Total Reward = 13.0, Total Steps = 8136\n",
      "Episode 892, Total Reward = 16.0, Total Steps = 8152\n",
      "Episode 893, Total Reward = 18.0, Total Steps = 8170\n",
      "Episode 894, Total Reward = 4.0, Total Steps = 8174\n",
      "Episode 895, Total Reward = 8.0, Total Steps = 8182\n",
      "Episode 896, Total Reward = 7.0, Total Steps = 8189\n",
      "Episode 897, Total Reward = 9.0, Total Steps = 8198\n",
      "Episode 898, Total Reward = 8.0, Total Steps = 8206\n",
      "Episode 899, Total Reward = 12.0, Total Steps = 8218\n",
      "Episode 900, Total Reward = 7.0, Total Steps = 8225\n",
      "Episode 901, Total Reward = 5.0, Total Steps = 8230\n",
      "Episode 902, Total Reward = 8.0, Total Steps = 8238\n",
      "Episode 903, Total Reward = 7.0, Total Steps = 8245\n",
      "Episode 904, Total Reward = 8.0, Total Steps = 8253\n",
      "Episode 905, Total Reward = 15.0, Total Steps = 8268\n",
      "Episode 906, Total Reward = 16.0, Total Steps = 8284\n",
      "Episode 907, Total Reward = 9.0, Total Steps = 8293\n",
      "Episode 908, Total Reward = 8.0, Total Steps = 8301\n",
      "Episode 909, Total Reward = 9.0, Total Steps = 8310\n",
      "Episode 910, Total Reward = 6.0, Total Steps = 8316\n",
      "Episode 911, Total Reward = 5.0, Total Steps = 8321\n",
      "Episode 912, Total Reward = 9.0, Total Steps = 8330\n",
      "Episode 913, Total Reward = 4.0, Total Steps = 8334\n",
      "Episode 914, Total Reward = 21.0, Total Steps = 8355\n",
      "Episode 915, Total Reward = 16.0, Total Steps = 8371\n",
      "Episode 916, Total Reward = 12.0, Total Steps = 8383\n",
      "Episode 917, Total Reward = 22.0, Total Steps = 8405\n",
      "Episode 918, Total Reward = 10.0, Total Steps = 8415\n",
      "Episode 919, Total Reward = 8.0, Total Steps = 8423\n",
      "Episode 920, Total Reward = 12.0, Total Steps = 8435\n",
      "Episode 921, Total Reward = 4.0, Total Steps = 8439\n",
      "Episode 922, Total Reward = 8.0, Total Steps = 8447\n",
      "Episode 923, Total Reward = 23.0, Total Steps = 8470\n",
      "Episode 924, Total Reward = 24.0, Total Steps = 8494\n",
      "Episode 925, Total Reward = 23.0, Total Steps = 8517\n",
      "Episode 926, Total Reward = 7.0, Total Steps = 8524\n",
      "Episode 927, Total Reward = 5.0, Total Steps = 8529\n",
      "Episode 928, Total Reward = 5.0, Total Steps = 8534\n",
      "Episode 929, Total Reward = 14.0, Total Steps = 8548\n",
      "Episode 930, Total Reward = 36.0, Total Steps = 8584\n",
      "Episode 931, Total Reward = 4.0, Total Steps = 8588\n",
      "Episode 932, Total Reward = 4.0, Total Steps = 8592\n",
      "Episode 933, Total Reward = 10.0, Total Steps = 8602\n",
      "Episode 934, Total Reward = 7.0, Total Steps = 8609\n",
      "Episode 935, Total Reward = 6.0, Total Steps = 8615\n",
      "Episode 936, Total Reward = 13.0, Total Steps = 8628\n",
      "Episode 937, Total Reward = 26.0, Total Steps = 8654\n",
      "Episode 938, Total Reward = 5.0, Total Steps = 8659\n",
      "Episode 939, Total Reward = 10.0, Total Steps = 8669\n",
      "Episode 940, Total Reward = 7.0, Total Steps = 8676\n",
      "Episode 941, Total Reward = 5.0, Total Steps = 8681\n",
      "Episode 942, Total Reward = 13.0, Total Steps = 8694\n",
      "Episode 943, Total Reward = 10.0, Total Steps = 8704\n",
      "Episode 944, Total Reward = 7.0, Total Steps = 8711\n",
      "Episode 945, Total Reward = 11.0, Total Steps = 8722\n",
      "Episode 946, Total Reward = 13.0, Total Steps = 8735\n",
      "Episode 947, Total Reward = 4.0, Total Steps = 8739\n",
      "Episode 948, Total Reward = 5.0, Total Steps = 8744\n",
      "Episode 949, Total Reward = 9.0, Total Steps = 8753\n",
      "Episode 950, Total Reward = 13.0, Total Steps = 8766\n",
      "Episode 951, Total Reward = 8.0, Total Steps = 8774\n",
      "Episode 952, Total Reward = 10.0, Total Steps = 8784\n",
      "Episode 953, Total Reward = 9.0, Total Steps = 8793\n",
      "Episode 954, Total Reward = 4.0, Total Steps = 8797\n",
      "Episode 955, Total Reward = 5.0, Total Steps = 8802\n",
      "Episode 956, Total Reward = 5.0, Total Steps = 8807\n",
      "Episode 957, Total Reward = 12.0, Total Steps = 8819\n",
      "Episode 958, Total Reward = 6.0, Total Steps = 8825\n",
      "Episode 959, Total Reward = 6.0, Total Steps = 8831\n",
      "Episode 960, Total Reward = 8.0, Total Steps = 8839\n",
      "Episode 961, Total Reward = 10.0, Total Steps = 8849\n",
      "Episode 962, Total Reward = 14.0, Total Steps = 8863\n",
      "Episode 963, Total Reward = 10.0, Total Steps = 8873\n",
      "Episode 964, Total Reward = 20.0, Total Steps = 8893\n",
      "Episode 965, Total Reward = 11.0, Total Steps = 8904\n",
      "Episode 966, Total Reward = 6.0, Total Steps = 8910\n",
      "Episode 967, Total Reward = 5.0, Total Steps = 8915\n",
      "Episode 968, Total Reward = 22.0, Total Steps = 8937\n",
      "Episode 969, Total Reward = 8.0, Total Steps = 8945\n",
      "Episode 970, Total Reward = 6.0, Total Steps = 8951\n",
      "Episode 971, Total Reward = 7.0, Total Steps = 8958\n",
      "Episode 972, Total Reward = 8.0, Total Steps = 8966\n",
      "Episode 973, Total Reward = 5.0, Total Steps = 8971\n",
      "Episode 974, Total Reward = 13.0, Total Steps = 8984\n",
      "Episode 975, Total Reward = 41.0, Total Steps = 9025\n",
      "Episode 976, Total Reward = 4.0, Total Steps = 9029\n",
      "Episode 977, Total Reward = 15.0, Total Steps = 9044\n",
      "Episode 978, Total Reward = 12.0, Total Steps = 9056\n",
      "Episode 979, Total Reward = 5.0, Total Steps = 9061\n",
      "Episode 980, Total Reward = 9.0, Total Steps = 9070\n",
      "Episode 981, Total Reward = 14.0, Total Steps = 9084\n",
      "Episode 982, Total Reward = 19.0, Total Steps = 9103\n",
      "Episode 983, Total Reward = 7.0, Total Steps = 9110\n",
      "Episode 984, Total Reward = 8.0, Total Steps = 9118\n",
      "Episode 985, Total Reward = 5.0, Total Steps = 9123\n",
      "Episode 986, Total Reward = 9.0, Total Steps = 9132\n",
      "Episode 987, Total Reward = 8.0, Total Steps = 9140\n",
      "Episode 988, Total Reward = 15.0, Total Steps = 9155\n",
      "Episode 989, Total Reward = 10.0, Total Steps = 9165\n",
      "Episode 990, Total Reward = 6.0, Total Steps = 9171\n",
      "Episode 991, Total Reward = 8.0, Total Steps = 9179\n",
      "Episode 992, Total Reward = 5.0, Total Steps = 9184\n",
      "Episode 993, Total Reward = 8.0, Total Steps = 9192\n",
      "Episode 994, Total Reward = 5.0, Total Steps = 9197\n",
      "Episode 995, Total Reward = 11.0, Total Steps = 9208\n",
      "Episode 996, Total Reward = 8.0, Total Steps = 9216\n",
      "Episode 997, Total Reward = 11.0, Total Steps = 9227\n",
      "Episode 998, Total Reward = 7.0, Total Steps = 9234\n",
      "Episode 999, Total Reward = 11.0, Total Steps = 9245\n",
      "Episode 1000, Total Reward = 25.0, Total Steps = 9270\n",
      "Episode 1001, Total Reward = 9.0, Total Steps = 9279\n",
      "Episode 1002, Total Reward = 6.0, Total Steps = 9285\n",
      "Episode 1003, Total Reward = 11.0, Total Steps = 9296\n",
      "Episode 1004, Total Reward = 9.0, Total Steps = 9305\n",
      "Episode 1005, Total Reward = 10.0, Total Steps = 9315\n",
      "Episode 1006, Total Reward = 17.0, Total Steps = 9332\n",
      "Episode 1007, Total Reward = 8.0, Total Steps = 9340\n",
      "Episode 1008, Total Reward = 8.0, Total Steps = 9348\n",
      "Episode 1009, Total Reward = 15.0, Total Steps = 9363\n",
      "Episode 1010, Total Reward = 9.0, Total Steps = 9372\n",
      "Episode 1011, Total Reward = 7.0, Total Steps = 9379\n",
      "Episode 1012, Total Reward = 6.0, Total Steps = 9385\n",
      "Episode 1013, Total Reward = 6.0, Total Steps = 9391\n",
      "Episode 1014, Total Reward = 31.0, Total Steps = 9422\n",
      "Episode 1015, Total Reward = 6.0, Total Steps = 9428\n",
      "Episode 1016, Total Reward = 10.0, Total Steps = 9438\n",
      "Episode 1017, Total Reward = 12.0, Total Steps = 9450\n",
      "Episode 1018, Total Reward = 5.0, Total Steps = 9455\n",
      "Episode 1019, Total Reward = 14.0, Total Steps = 9469\n",
      "Episode 1020, Total Reward = 6.0, Total Steps = 9475\n",
      "Episode 1021, Total Reward = 18.0, Total Steps = 9493\n",
      "Episode 1022, Total Reward = 51.0, Total Steps = 9544\n",
      "Episode 1023, Total Reward = 19.0, Total Steps = 9563\n",
      "Episode 1024, Total Reward = 4.0, Total Steps = 9567\n",
      "Episode 1025, Total Reward = 4.0, Total Steps = 9571\n",
      "Episode 1026, Total Reward = 4.0, Total Steps = 9575\n",
      "Episode 1027, Total Reward = 6.0, Total Steps = 9581\n",
      "Episode 1028, Total Reward = 7.0, Total Steps = 9588\n",
      "Episode 1029, Total Reward = 14.0, Total Steps = 9602\n",
      "Episode 1030, Total Reward = 6.0, Total Steps = 9608\n",
      "Episode 1031, Total Reward = 3.0, Total Steps = 9611\n",
      "Episode 1032, Total Reward = 12.0, Total Steps = 9623\n",
      "Episode 1033, Total Reward = 4.0, Total Steps = 9627\n",
      "Episode 1034, Total Reward = 8.0, Total Steps = 9635\n",
      "Episode 1035, Total Reward = 6.0, Total Steps = 9641\n",
      "Episode 1036, Total Reward = 15.0, Total Steps = 9656\n",
      "Episode 1037, Total Reward = 6.0, Total Steps = 9662\n",
      "Episode 1038, Total Reward = 8.0, Total Steps = 9670\n",
      "Episode 1039, Total Reward = 7.0, Total Steps = 9677\n",
      "Episode 1040, Total Reward = 5.0, Total Steps = 9682\n",
      "Episode 1041, Total Reward = 27.0, Total Steps = 9709\n",
      "Episode 1042, Total Reward = 13.0, Total Steps = 9722\n",
      "Episode 1043, Total Reward = 6.0, Total Steps = 9728\n",
      "Episode 1044, Total Reward = 9.0, Total Steps = 9737\n",
      "Episode 1045, Total Reward = 6.0, Total Steps = 9743\n",
      "Episode 1046, Total Reward = 13.0, Total Steps = 9756\n",
      "Episode 1047, Total Reward = 20.0, Total Steps = 9776\n",
      "Episode 1048, Total Reward = 6.0, Total Steps = 9782\n",
      "Episode 1049, Total Reward = 9.0, Total Steps = 9791\n",
      "Episode 1050, Total Reward = 6.0, Total Steps = 9797\n",
      "Episode 1051, Total Reward = 6.0, Total Steps = 9803\n",
      "Episode 1052, Total Reward = 8.0, Total Steps = 9811\n",
      "Episode 1053, Total Reward = 9.0, Total Steps = 9820\n",
      "Episode 1054, Total Reward = 8.0, Total Steps = 9828\n",
      "Episode 1055, Total Reward = 14.0, Total Steps = 9842\n",
      "Episode 1056, Total Reward = 34.0, Total Steps = 9876\n",
      "Episode 1057, Total Reward = 8.0, Total Steps = 9884\n",
      "Episode 1058, Total Reward = 8.0, Total Steps = 9892\n",
      "Episode 1059, Total Reward = 7.0, Total Steps = 9899\n",
      "Episode 1060, Total Reward = 12.0, Total Steps = 9911\n",
      "Episode 1061, Total Reward = 8.0, Total Steps = 9919\n",
      "Episode 1062, Total Reward = 6.0, Total Steps = 9925\n",
      "Episode 1063, Total Reward = 14.0, Total Steps = 9939\n",
      "Episode 1064, Total Reward = 7.0, Total Steps = 9946\n",
      "Episode 1065, Total Reward = 10.0, Total Steps = 9956\n",
      "Episode 1066, Total Reward = 16.0, Total Steps = 9972\n",
      "Episode 1067, Total Reward = 4.0, Total Steps = 9976\n",
      "Episode 1068, Total Reward = 13.0, Total Steps = 9989\n",
      "Episode 1069, Total Reward = 5.0, Total Steps = 9994\n",
      "Episode 1070, Total Reward = 9.0, Total Steps = 10003\n",
      "Episode 1071, Total Reward = 7.0, Total Steps = 10010\n",
      "Episode 1072, Total Reward = 14.0, Total Steps = 10024\n",
      "Episode 1073, Total Reward = 7.0, Total Steps = 10031\n",
      "Episode 1074, Total Reward = 5.0, Total Steps = 10036\n",
      "Episode 1075, Total Reward = 15.0, Total Steps = 10051\n",
      "Episode 1076, Total Reward = 11.0, Total Steps = 10062\n",
      "Episode 1077, Total Reward = 5.0, Total Steps = 10067\n",
      "Episode 1078, Total Reward = 12.0, Total Steps = 10079\n",
      "Episode 1079, Total Reward = 7.0, Total Steps = 10086\n",
      "Episode 1080, Total Reward = 6.0, Total Steps = 10092\n",
      "Episode 1081, Total Reward = 6.0, Total Steps = 10098\n",
      "Episode 1082, Total Reward = 13.0, Total Steps = 10111\n",
      "Episode 1083, Total Reward = 8.0, Total Steps = 10119\n",
      "Episode 1084, Total Reward = 4.0, Total Steps = 10123\n",
      "Episode 1085, Total Reward = 5.0, Total Steps = 10128\n",
      "Episode 1086, Total Reward = 10.0, Total Steps = 10138\n",
      "Episode 1087, Total Reward = 4.0, Total Steps = 10142\n",
      "Episode 1088, Total Reward = 9.0, Total Steps = 10151\n",
      "Episode 1089, Total Reward = 23.0, Total Steps = 10174\n",
      "Episode 1090, Total Reward = 7.0, Total Steps = 10181\n",
      "Episode 1091, Total Reward = 5.0, Total Steps = 10186\n",
      "Episode 1092, Total Reward = 11.0, Total Steps = 10197\n",
      "Episode 1093, Total Reward = 9.0, Total Steps = 10206\n",
      "Episode 1094, Total Reward = 4.0, Total Steps = 10210\n",
      "Episode 1095, Total Reward = 15.0, Total Steps = 10225\n",
      "Episode 1096, Total Reward = 16.0, Total Steps = 10241\n",
      "Episode 1097, Total Reward = 11.0, Total Steps = 10252\n",
      "Episode 1098, Total Reward = 17.0, Total Steps = 10269\n",
      "Episode 1099, Total Reward = 11.0, Total Steps = 10280\n",
      "Episode 1100, Total Reward = 19.0, Total Steps = 10299\n",
      "Episode 1101, Total Reward = 26.0, Total Steps = 10325\n",
      "Episode 1102, Total Reward = 6.0, Total Steps = 10331\n",
      "Episode 1103, Total Reward = 6.0, Total Steps = 10337\n",
      "Episode 1104, Total Reward = 22.0, Total Steps = 10359\n",
      "Episode 1105, Total Reward = 9.0, Total Steps = 10368\n",
      "Episode 1106, Total Reward = 8.0, Total Steps = 10376\n",
      "Episode 1107, Total Reward = 12.0, Total Steps = 10388\n",
      "Episode 1108, Total Reward = 13.0, Total Steps = 10401\n",
      "Episode 1109, Total Reward = 20.0, Total Steps = 10421\n",
      "Episode 1110, Total Reward = 13.0, Total Steps = 10434\n",
      "Episode 1111, Total Reward = 7.0, Total Steps = 10441\n",
      "Episode 1112, Total Reward = 6.0, Total Steps = 10447\n",
      "Episode 1113, Total Reward = 29.0, Total Steps = 10476\n",
      "Episode 1114, Total Reward = 4.0, Total Steps = 10480\n",
      "Episode 1115, Total Reward = 7.0, Total Steps = 10487\n",
      "Episode 1116, Total Reward = 12.0, Total Steps = 10499\n",
      "Episode 1117, Total Reward = 10.0, Total Steps = 10509\n",
      "Episode 1118, Total Reward = 9.0, Total Steps = 10518\n",
      "Episode 1119, Total Reward = 6.0, Total Steps = 10524\n",
      "Episode 1120, Total Reward = 8.0, Total Steps = 10532\n",
      "Episode 1121, Total Reward = 9.0, Total Steps = 10541\n",
      "Episode 1122, Total Reward = 8.0, Total Steps = 10549\n",
      "Episode 1123, Total Reward = 18.0, Total Steps = 10567\n",
      "Episode 1124, Total Reward = 14.0, Total Steps = 10581\n",
      "Episode 1125, Total Reward = 14.0, Total Steps = 10595\n",
      "Episode 1126, Total Reward = 24.0, Total Steps = 10619\n",
      "Episode 1127, Total Reward = 5.0, Total Steps = 10624\n",
      "Episode 1128, Total Reward = 7.0, Total Steps = 10631\n",
      "Episode 1129, Total Reward = 12.0, Total Steps = 10643\n",
      "Episode 1130, Total Reward = 7.0, Total Steps = 10650\n",
      "Episode 1131, Total Reward = 13.0, Total Steps = 10663\n",
      "Episode 1132, Total Reward = 18.0, Total Steps = 10681\n",
      "Episode 1133, Total Reward = 9.0, Total Steps = 10690\n",
      "Episode 1134, Total Reward = 15.0, Total Steps = 10705\n",
      "Episode 1135, Total Reward = 14.0, Total Steps = 10719\n",
      "Episode 1136, Total Reward = 12.0, Total Steps = 10731\n",
      "Episode 1137, Total Reward = 10.0, Total Steps = 10741\n",
      "Episode 1138, Total Reward = 10.0, Total Steps = 10751\n",
      "Episode 1139, Total Reward = 4.0, Total Steps = 10755\n",
      "Episode 1140, Total Reward = 4.0, Total Steps = 10759\n",
      "Episode 1141, Total Reward = 16.0, Total Steps = 10775\n",
      "Episode 1142, Total Reward = 9.0, Total Steps = 10784\n",
      "Episode 1143, Total Reward = 6.0, Total Steps = 10790\n",
      "Episode 1144, Total Reward = 10.0, Total Steps = 10800\n",
      "Episode 1145, Total Reward = 8.0, Total Steps = 10808\n",
      "Episode 1146, Total Reward = 6.0, Total Steps = 10814\n",
      "Episode 1147, Total Reward = 6.0, Total Steps = 10820\n",
      "Episode 1148, Total Reward = 27.0, Total Steps = 10847\n",
      "Episode 1149, Total Reward = 16.0, Total Steps = 10863\n",
      "Episode 1150, Total Reward = 10.0, Total Steps = 10873\n",
      "Episode 1151, Total Reward = 14.0, Total Steps = 10887\n",
      "Episode 1152, Total Reward = 29.0, Total Steps = 10916\n",
      "Episode 1153, Total Reward = 8.0, Total Steps = 10924\n",
      "Episode 1154, Total Reward = 12.0, Total Steps = 10936\n",
      "Episode 1155, Total Reward = 16.0, Total Steps = 10952\n",
      "Episode 1156, Total Reward = 13.0, Total Steps = 10965\n",
      "Episode 1157, Total Reward = 9.0, Total Steps = 10974\n",
      "Episode 1158, Total Reward = 28.0, Total Steps = 11002\n",
      "Episode 1159, Total Reward = 4.0, Total Steps = 11006\n",
      "Episode 1160, Total Reward = 8.0, Total Steps = 11014\n",
      "Episode 1161, Total Reward = 5.0, Total Steps = 11019\n",
      "Episode 1162, Total Reward = 15.0, Total Steps = 11034\n",
      "Episode 1163, Total Reward = 10.0, Total Steps = 11044\n",
      "Episode 1164, Total Reward = 7.0, Total Steps = 11051\n",
      "Episode 1165, Total Reward = 9.0, Total Steps = 11060\n",
      "Episode 1166, Total Reward = 9.0, Total Steps = 11069\n",
      "Episode 1167, Total Reward = 10.0, Total Steps = 11079\n",
      "Episode 1168, Total Reward = 10.0, Total Steps = 11089\n",
      "Episode 1169, Total Reward = 14.0, Total Steps = 11103\n",
      "Episode 1170, Total Reward = 22.0, Total Steps = 11125\n",
      "Episode 1171, Total Reward = 6.0, Total Steps = 11131\n",
      "Episode 1172, Total Reward = 12.0, Total Steps = 11143\n",
      "Episode 1173, Total Reward = 7.0, Total Steps = 11150\n",
      "Episode 1174, Total Reward = 10.0, Total Steps = 11160\n",
      "Episode 1175, Total Reward = 7.0, Total Steps = 11167\n",
      "Episode 1176, Total Reward = 24.0, Total Steps = 11191\n",
      "Episode 1177, Total Reward = 16.0, Total Steps = 11207\n",
      "Episode 1178, Total Reward = 21.0, Total Steps = 11228\n",
      "Episode 1179, Total Reward = 6.0, Total Steps = 11234\n",
      "Episode 1180, Total Reward = 12.0, Total Steps = 11246\n",
      "Episode 1181, Total Reward = 14.0, Total Steps = 11260\n",
      "Episode 1182, Total Reward = 8.0, Total Steps = 11268\n",
      "Episode 1183, Total Reward = 6.0, Total Steps = 11274\n",
      "Episode 1184, Total Reward = 16.0, Total Steps = 11290\n",
      "Episode 1185, Total Reward = 7.0, Total Steps = 11297\n",
      "Episode 1186, Total Reward = 14.0, Total Steps = 11311\n",
      "Episode 1187, Total Reward = 5.0, Total Steps = 11316\n",
      "Episode 1188, Total Reward = 10.0, Total Steps = 11326\n",
      "Episode 1189, Total Reward = 16.0, Total Steps = 11342\n",
      "Episode 1190, Total Reward = 16.0, Total Steps = 11358\n",
      "Episode 1191, Total Reward = 13.0, Total Steps = 11371\n",
      "Episode 1192, Total Reward = 22.0, Total Steps = 11393\n",
      "Episode 1193, Total Reward = 5.0, Total Steps = 11398\n",
      "Episode 1194, Total Reward = 8.0, Total Steps = 11406\n",
      "Episode 1195, Total Reward = 12.0, Total Steps = 11418\n",
      "Episode 1196, Total Reward = 25.0, Total Steps = 11443\n",
      "Episode 1197, Total Reward = 7.0, Total Steps = 11450\n",
      "Episode 1198, Total Reward = 6.0, Total Steps = 11456\n",
      "Episode 1199, Total Reward = 11.0, Total Steps = 11467\n",
      "Episode 1200, Total Reward = 5.0, Total Steps = 11472\n",
      "Episode 1201, Total Reward = 7.0, Total Steps = 11479\n",
      "Episode 1202, Total Reward = 10.0, Total Steps = 11489\n",
      "Episode 1203, Total Reward = 7.0, Total Steps = 11496\n",
      "Episode 1204, Total Reward = 16.0, Total Steps = 11512\n",
      "Episode 1205, Total Reward = 6.0, Total Steps = 11518\n",
      "Episode 1206, Total Reward = 18.0, Total Steps = 11536\n",
      "Episode 1207, Total Reward = 7.0, Total Steps = 11543\n",
      "Episode 1208, Total Reward = 11.0, Total Steps = 11554\n",
      "Episode 1209, Total Reward = 5.0, Total Steps = 11559\n",
      "Episode 1210, Total Reward = 12.0, Total Steps = 11571\n",
      "Episode 1211, Total Reward = 5.0, Total Steps = 11576\n",
      "Episode 1212, Total Reward = 6.0, Total Steps = 11582\n",
      "Episode 1213, Total Reward = 15.0, Total Steps = 11597\n",
      "Episode 1214, Total Reward = 7.0, Total Steps = 11604\n",
      "Episode 1215, Total Reward = 10.0, Total Steps = 11614\n",
      "Episode 1216, Total Reward = 14.0, Total Steps = 11628\n",
      "Episode 1217, Total Reward = 7.0, Total Steps = 11635\n",
      "Episode 1218, Total Reward = 3.0, Total Steps = 11638\n",
      "Episode 1219, Total Reward = 12.0, Total Steps = 11650\n",
      "Episode 1220, Total Reward = 10.0, Total Steps = 11660\n",
      "Episode 1221, Total Reward = 5.0, Total Steps = 11665\n",
      "Episode 1222, Total Reward = 7.0, Total Steps = 11672\n",
      "Episode 1223, Total Reward = 10.0, Total Steps = 11682\n",
      "Episode 1224, Total Reward = 9.0, Total Steps = 11691\n",
      "Episode 1225, Total Reward = 17.0, Total Steps = 11708\n",
      "Episode 1226, Total Reward = 13.0, Total Steps = 11721\n",
      "Episode 1227, Total Reward = 26.0, Total Steps = 11747\n",
      "Episode 1228, Total Reward = 12.0, Total Steps = 11759\n",
      "Episode 1229, Total Reward = 7.0, Total Steps = 11766\n",
      "Episode 1230, Total Reward = 5.0, Total Steps = 11771\n",
      "Episode 1231, Total Reward = 6.0, Total Steps = 11777\n",
      "Episode 1232, Total Reward = 4.0, Total Steps = 11781\n",
      "Episode 1233, Total Reward = 6.0, Total Steps = 11787\n",
      "Episode 1234, Total Reward = 19.0, Total Steps = 11806\n",
      "Episode 1235, Total Reward = 21.0, Total Steps = 11827\n",
      "Episode 1236, Total Reward = 51.0, Total Steps = 11878\n",
      "Episode 1237, Total Reward = 39.0, Total Steps = 11917\n",
      "Episode 1238, Total Reward = 10.0, Total Steps = 11927\n",
      "Episode 1239, Total Reward = 6.0, Total Steps = 11933\n",
      "Episode 1240, Total Reward = 4.0, Total Steps = 11937\n",
      "Episode 1241, Total Reward = 18.0, Total Steps = 11955\n",
      "Episode 1242, Total Reward = 8.0, Total Steps = 11963\n",
      "Episode 1243, Total Reward = 6.0, Total Steps = 11969\n",
      "Episode 1244, Total Reward = 5.0, Total Steps = 11974\n",
      "Episode 1245, Total Reward = 9.0, Total Steps = 11983\n",
      "Episode 1246, Total Reward = 9.0, Total Steps = 11992\n",
      "Episode 1247, Total Reward = 5.0, Total Steps = 11997\n",
      "Episode 1248, Total Reward = 11.0, Total Steps = 12008\n",
      "Episode 1249, Total Reward = 10.0, Total Steps = 12018\n",
      "Episode 1250, Total Reward = 8.0, Total Steps = 12026\n",
      "Episode 1251, Total Reward = 18.0, Total Steps = 12044\n",
      "Episode 1252, Total Reward = 7.0, Total Steps = 12051\n",
      "Episode 1253, Total Reward = 8.0, Total Steps = 12059\n",
      "Episode 1254, Total Reward = 12.0, Total Steps = 12071\n",
      "Episode 1255, Total Reward = 26.0, Total Steps = 12097\n",
      "Episode 1256, Total Reward = 7.0, Total Steps = 12104\n",
      "Episode 1257, Total Reward = 7.0, Total Steps = 12111\n",
      "Episode 1258, Total Reward = 7.0, Total Steps = 12118\n",
      "Episode 1259, Total Reward = 19.0, Total Steps = 12137\n",
      "Episode 1260, Total Reward = 14.0, Total Steps = 12151\n",
      "Episode 1261, Total Reward = 11.0, Total Steps = 12162\n",
      "Episode 1262, Total Reward = 14.0, Total Steps = 12176\n",
      "Episode 1263, Total Reward = 6.0, Total Steps = 12182\n",
      "Episode 1264, Total Reward = 7.0, Total Steps = 12189\n",
      "Episode 1265, Total Reward = 15.0, Total Steps = 12204\n",
      "Episode 1266, Total Reward = 17.0, Total Steps = 12221\n",
      "Episode 1267, Total Reward = 7.0, Total Steps = 12228\n",
      "Episode 1268, Total Reward = 12.0, Total Steps = 12240\n",
      "Episode 1269, Total Reward = 5.0, Total Steps = 12245\n",
      "Episode 1270, Total Reward = 4.0, Total Steps = 12249\n",
      "Episode 1271, Total Reward = 7.0, Total Steps = 12256\n",
      "Episode 1272, Total Reward = 13.0, Total Steps = 12269\n",
      "Episode 1273, Total Reward = 5.0, Total Steps = 12274\n",
      "Episode 1274, Total Reward = 17.0, Total Steps = 12291\n",
      "Episode 1275, Total Reward = 4.0, Total Steps = 12295\n",
      "Episode 1276, Total Reward = 11.0, Total Steps = 12306\n",
      "Episode 1277, Total Reward = 8.0, Total Steps = 12314\n",
      "Episode 1278, Total Reward = 42.0, Total Steps = 12356\n",
      "Episode 1279, Total Reward = 8.0, Total Steps = 12364\n",
      "Episode 1280, Total Reward = 6.0, Total Steps = 12370\n",
      "Episode 1281, Total Reward = 37.0, Total Steps = 12407\n",
      "Episode 1282, Total Reward = 41.0, Total Steps = 12448\n",
      "Episode 1283, Total Reward = 30.0, Total Steps = 12478\n",
      "Episode 1284, Total Reward = 12.0, Total Steps = 12490\n",
      "Episode 1285, Total Reward = 6.0, Total Steps = 12496\n",
      "Episode 1286, Total Reward = 7.0, Total Steps = 12503\n",
      "Episode 1287, Total Reward = 17.0, Total Steps = 12520\n",
      "Episode 1288, Total Reward = 12.0, Total Steps = 12532\n",
      "Episode 1289, Total Reward = 13.0, Total Steps = 12545\n",
      "Episode 1290, Total Reward = 7.0, Total Steps = 12552\n",
      "Episode 1291, Total Reward = 18.0, Total Steps = 12570\n",
      "Episode 1292, Total Reward = 6.0, Total Steps = 12576\n",
      "Episode 1293, Total Reward = 15.0, Total Steps = 12591\n",
      "Episode 1294, Total Reward = 5.0, Total Steps = 12596\n",
      "Episode 1295, Total Reward = 10.0, Total Steps = 12606\n",
      "Episode 1296, Total Reward = 9.0, Total Steps = 12615\n",
      "Episode 1297, Total Reward = 5.0, Total Steps = 12620\n",
      "Episode 1298, Total Reward = 7.0, Total Steps = 12627\n",
      "Episode 1299, Total Reward = 8.0, Total Steps = 12635\n",
      "Episode 1300, Total Reward = 19.0, Total Steps = 12654\n",
      "Episode 1301, Total Reward = 14.0, Total Steps = 12668\n",
      "Episode 1302, Total Reward = 31.0, Total Steps = 12699\n",
      "Episode 1303, Total Reward = 10.0, Total Steps = 12709\n",
      "Episode 1304, Total Reward = 20.0, Total Steps = 12729\n",
      "Episode 1305, Total Reward = 34.0, Total Steps = 12763\n",
      "Episode 1306, Total Reward = 5.0, Total Steps = 12768\n",
      "Episode 1307, Total Reward = 15.0, Total Steps = 12783\n",
      "Episode 1308, Total Reward = 22.0, Total Steps = 12805\n",
      "Episode 1309, Total Reward = 11.0, Total Steps = 12816\n",
      "Episode 1310, Total Reward = 13.0, Total Steps = 12829\n",
      "Episode 1311, Total Reward = 21.0, Total Steps = 12850\n",
      "Episode 1312, Total Reward = 4.0, Total Steps = 12854\n",
      "Episode 1313, Total Reward = 21.0, Total Steps = 12875\n",
      "Episode 1314, Total Reward = 6.0, Total Steps = 12881\n",
      "Episode 1315, Total Reward = 18.0, Total Steps = 12899\n",
      "Episode 1316, Total Reward = 4.0, Total Steps = 12903\n",
      "Episode 1317, Total Reward = 10.0, Total Steps = 12913\n",
      "Episode 1318, Total Reward = 5.0, Total Steps = 12918\n",
      "Episode 1319, Total Reward = 7.0, Total Steps = 12925\n",
      "Episode 1320, Total Reward = 29.0, Total Steps = 12954\n",
      "Episode 1321, Total Reward = 16.0, Total Steps = 12970\n",
      "Episode 1322, Total Reward = 8.0, Total Steps = 12978\n",
      "Episode 1323, Total Reward = 10.0, Total Steps = 12988\n",
      "Episode 1324, Total Reward = 13.0, Total Steps = 13001\n",
      "Episode 1325, Total Reward = 20.0, Total Steps = 13021\n",
      "Episode 1326, Total Reward = 19.0, Total Steps = 13040\n",
      "Episode 1327, Total Reward = 6.0, Total Steps = 13046\n",
      "Episode 1328, Total Reward = 17.0, Total Steps = 13063\n",
      "Episode 1329, Total Reward = 5.0, Total Steps = 13068\n",
      "Episode 1330, Total Reward = 8.0, Total Steps = 13076\n",
      "Episode 1331, Total Reward = 18.0, Total Steps = 13094\n",
      "Episode 1332, Total Reward = 10.0, Total Steps = 13104\n",
      "Episode 1333, Total Reward = 5.0, Total Steps = 13109\n",
      "Episode 1334, Total Reward = 19.0, Total Steps = 13128\n",
      "Episode 1335, Total Reward = 7.0, Total Steps = 13135\n",
      "Episode 1336, Total Reward = 7.0, Total Steps = 13142\n",
      "Episode 1337, Total Reward = 8.0, Total Steps = 13150\n",
      "Episode 1338, Total Reward = 16.0, Total Steps = 13166\n",
      "Episode 1339, Total Reward = 21.0, Total Steps = 13187\n",
      "Episode 1340, Total Reward = 7.0, Total Steps = 13194\n",
      "Episode 1341, Total Reward = 21.0, Total Steps = 13215\n",
      "Episode 1342, Total Reward = 6.0, Total Steps = 13221\n",
      "Episode 1343, Total Reward = 5.0, Total Steps = 13226\n",
      "Episode 1344, Total Reward = 15.0, Total Steps = 13241\n",
      "Episode 1345, Total Reward = 22.0, Total Steps = 13263\n",
      "Episode 1346, Total Reward = 18.0, Total Steps = 13281\n",
      "Episode 1347, Total Reward = 6.0, Total Steps = 13287\n",
      "Episode 1348, Total Reward = 7.0, Total Steps = 13294\n",
      "Episode 1349, Total Reward = 11.0, Total Steps = 13305\n",
      "Episode 1350, Total Reward = 5.0, Total Steps = 13310\n",
      "Episode 1351, Total Reward = 11.0, Total Steps = 13321\n",
      "Episode 1352, Total Reward = 7.0, Total Steps = 13328\n",
      "Episode 1353, Total Reward = 13.0, Total Steps = 13341\n",
      "Episode 1354, Total Reward = 12.0, Total Steps = 13353\n",
      "Episode 1355, Total Reward = 7.0, Total Steps = 13360\n",
      "Episode 1356, Total Reward = 12.0, Total Steps = 13372\n",
      "Episode 1357, Total Reward = 5.0, Total Steps = 13377\n",
      "Episode 1358, Total Reward = 33.0, Total Steps = 13410\n",
      "Episode 1359, Total Reward = 8.0, Total Steps = 13418\n",
      "Episode 1360, Total Reward = 5.0, Total Steps = 13423\n",
      "Episode 1361, Total Reward = 14.0, Total Steps = 13437\n",
      "Episode 1362, Total Reward = 15.0, Total Steps = 13452\n",
      "Episode 1363, Total Reward = 14.0, Total Steps = 13466\n",
      "Episode 1364, Total Reward = 11.0, Total Steps = 13477\n",
      "Episode 1365, Total Reward = 11.0, Total Steps = 13488\n",
      "Episode 1366, Total Reward = 6.0, Total Steps = 13494\n",
      "Episode 1367, Total Reward = 10.0, Total Steps = 13504\n",
      "Episode 1368, Total Reward = 9.0, Total Steps = 13513\n",
      "Episode 1369, Total Reward = 5.0, Total Steps = 13518\n",
      "Episode 1370, Total Reward = 19.0, Total Steps = 13537\n",
      "Episode 1371, Total Reward = 7.0, Total Steps = 13544\n",
      "Episode 1372, Total Reward = 12.0, Total Steps = 13556\n",
      "Episode 1373, Total Reward = 12.0, Total Steps = 13568\n",
      "Episode 1374, Total Reward = 23.0, Total Steps = 13591\n",
      "Episode 1375, Total Reward = 5.0, Total Steps = 13596\n",
      "Episode 1376, Total Reward = 14.0, Total Steps = 13610\n",
      "Episode 1377, Total Reward = 10.0, Total Steps = 13620\n",
      "Episode 1378, Total Reward = 6.0, Total Steps = 13626\n",
      "Episode 1379, Total Reward = 5.0, Total Steps = 13631\n",
      "Episode 1380, Total Reward = 7.0, Total Steps = 13638\n",
      "Episode 1381, Total Reward = 8.0, Total Steps = 13646\n",
      "Episode 1382, Total Reward = 30.0, Total Steps = 13676\n",
      "Episode 1383, Total Reward = 7.0, Total Steps = 13683\n",
      "Episode 1384, Total Reward = 16.0, Total Steps = 13699\n",
      "Episode 1385, Total Reward = 9.0, Total Steps = 13708\n",
      "Episode 1386, Total Reward = 6.0, Total Steps = 13714\n",
      "Episode 1387, Total Reward = 15.0, Total Steps = 13729\n",
      "Episode 1388, Total Reward = 25.0, Total Steps = 13754\n",
      "Episode 1389, Total Reward = 9.0, Total Steps = 13763\n",
      "Episode 1390, Total Reward = 4.0, Total Steps = 13767\n",
      "Episode 1391, Total Reward = 27.0, Total Steps = 13794\n",
      "Episode 1392, Total Reward = 28.0, Total Steps = 13822\n",
      "Episode 1393, Total Reward = 14.0, Total Steps = 13836\n",
      "Episode 1394, Total Reward = 11.0, Total Steps = 13847\n",
      "Episode 1395, Total Reward = 9.0, Total Steps = 13856\n",
      "Episode 1396, Total Reward = 19.0, Total Steps = 13875\n",
      "Episode 1397, Total Reward = 5.0, Total Steps = 13880\n",
      "Episode 1398, Total Reward = 7.0, Total Steps = 13887\n",
      "Episode 1399, Total Reward = 27.0, Total Steps = 13914\n",
      "Episode 1400, Total Reward = 16.0, Total Steps = 13930\n",
      "Episode 1401, Total Reward = 31.0, Total Steps = 13961\n",
      "Episode 1402, Total Reward = 19.0, Total Steps = 13980\n",
      "Episode 1403, Total Reward = 12.0, Total Steps = 13992\n",
      "Episode 1404, Total Reward = 22.0, Total Steps = 14014\n",
      "Episode 1405, Total Reward = 23.0, Total Steps = 14037\n",
      "Episode 1406, Total Reward = 22.0, Total Steps = 14059\n",
      "Episode 1407, Total Reward = 6.0, Total Steps = 14065\n",
      "Episode 1408, Total Reward = 7.0, Total Steps = 14072\n",
      "Episode 1409, Total Reward = 5.0, Total Steps = 14077\n",
      "Episode 1410, Total Reward = 9.0, Total Steps = 14086\n",
      "Episode 1411, Total Reward = 11.0, Total Steps = 14097\n",
      "Episode 1412, Total Reward = 6.0, Total Steps = 14103\n",
      "Episode 1413, Total Reward = 5.0, Total Steps = 14108\n",
      "Episode 1414, Total Reward = 6.0, Total Steps = 14114\n",
      "Episode 1415, Total Reward = 9.0, Total Steps = 14123\n",
      "Episode 1416, Total Reward = 8.0, Total Steps = 14131\n",
      "Episode 1417, Total Reward = 14.0, Total Steps = 14145\n",
      "Episode 1418, Total Reward = 12.0, Total Steps = 14157\n",
      "Episode 1419, Total Reward = 7.0, Total Steps = 14164\n",
      "Episode 1420, Total Reward = 8.0, Total Steps = 14172\n",
      "Episode 1421, Total Reward = 7.0, Total Steps = 14179\n",
      "Episode 1422, Total Reward = 8.0, Total Steps = 14187\n",
      "Episode 1423, Total Reward = 32.0, Total Steps = 14219\n",
      "Episode 1424, Total Reward = 14.0, Total Steps = 14233\n",
      "Episode 1425, Total Reward = 15.0, Total Steps = 14248\n",
      "Episode 1426, Total Reward = 19.0, Total Steps = 14267\n",
      "Episode 1427, Total Reward = 6.0, Total Steps = 14273\n",
      "Episode 1428, Total Reward = 8.0, Total Steps = 14281\n",
      "Episode 1429, Total Reward = 10.0, Total Steps = 14291\n",
      "Episode 1430, Total Reward = 22.0, Total Steps = 14313\n",
      "Episode 1431, Total Reward = 6.0, Total Steps = 14319\n",
      "Episode 1432, Total Reward = 13.0, Total Steps = 14332\n",
      "Episode 1433, Total Reward = 24.0, Total Steps = 14356\n",
      "Episode 1434, Total Reward = 29.0, Total Steps = 14385\n",
      "Episode 1435, Total Reward = 4.0, Total Steps = 14389\n",
      "Episode 1436, Total Reward = 14.0, Total Steps = 14403\n",
      "Episode 1437, Total Reward = 11.0, Total Steps = 14414\n",
      "Episode 1438, Total Reward = 7.0, Total Steps = 14421\n",
      "Episode 1439, Total Reward = 27.0, Total Steps = 14448\n",
      "Episode 1440, Total Reward = 12.0, Total Steps = 14460\n",
      "Episode 1441, Total Reward = 13.0, Total Steps = 14473\n",
      "Episode 1442, Total Reward = 15.0, Total Steps = 14488\n",
      "Episode 1443, Total Reward = 24.0, Total Steps = 14512\n",
      "Episode 1444, Total Reward = 6.0, Total Steps = 14518\n",
      "Episode 1445, Total Reward = 10.0, Total Steps = 14528\n",
      "Episode 1446, Total Reward = 23.0, Total Steps = 14551\n",
      "Episode 1447, Total Reward = 19.0, Total Steps = 14570\n",
      "Episode 1448, Total Reward = 16.0, Total Steps = 14586\n",
      "Episode 1449, Total Reward = 7.0, Total Steps = 14593\n",
      "Episode 1450, Total Reward = 8.0, Total Steps = 14601\n",
      "Episode 1451, Total Reward = 17.0, Total Steps = 14618\n",
      "Episode 1452, Total Reward = 15.0, Total Steps = 14633\n",
      "Episode 1453, Total Reward = 11.0, Total Steps = 14644\n",
      "Episode 1454, Total Reward = 16.0, Total Steps = 14660\n",
      "Episode 1455, Total Reward = 9.0, Total Steps = 14669\n",
      "Episode 1456, Total Reward = 5.0, Total Steps = 14674\n",
      "Episode 1457, Total Reward = 10.0, Total Steps = 14684\n",
      "Episode 1458, Total Reward = 6.0, Total Steps = 14690\n",
      "Episode 1459, Total Reward = 5.0, Total Steps = 14695\n",
      "Episode 1460, Total Reward = 16.0, Total Steps = 14711\n",
      "Episode 1461, Total Reward = 33.0, Total Steps = 14744\n",
      "Episode 1462, Total Reward = 4.0, Total Steps = 14748\n",
      "Episode 1463, Total Reward = 18.0, Total Steps = 14766\n",
      "Episode 1464, Total Reward = 12.0, Total Steps = 14778\n",
      "Episode 1465, Total Reward = 12.0, Total Steps = 14790\n",
      "Episode 1466, Total Reward = 4.0, Total Steps = 14794\n",
      "Episode 1467, Total Reward = 3.0, Total Steps = 14797\n",
      "Episode 1468, Total Reward = 18.0, Total Steps = 14815\n",
      "Episode 1469, Total Reward = 7.0, Total Steps = 14822\n",
      "Episode 1470, Total Reward = 9.0, Total Steps = 14831\n",
      "Episode 1471, Total Reward = 10.0, Total Steps = 14841\n",
      "Episode 1472, Total Reward = 7.0, Total Steps = 14848\n",
      "Episode 1473, Total Reward = 23.0, Total Steps = 14871\n",
      "Episode 1474, Total Reward = 5.0, Total Steps = 14876\n",
      "Episode 1475, Total Reward = 33.0, Total Steps = 14909\n",
      "Episode 1476, Total Reward = 12.0, Total Steps = 14921\n",
      "Episode 1477, Total Reward = 19.0, Total Steps = 14940\n",
      "Episode 1478, Total Reward = 8.0, Total Steps = 14948\n",
      "Episode 1479, Total Reward = 14.0, Total Steps = 14962\n",
      "Episode 1480, Total Reward = 31.0, Total Steps = 14993\n",
      "Episode 1481, Total Reward = 14.0, Total Steps = 15007\n",
      "Episode 1482, Total Reward = 12.0, Total Steps = 15019\n",
      "Episode 1483, Total Reward = 28.0, Total Steps = 15047\n",
      "Episode 1484, Total Reward = 9.0, Total Steps = 15056\n",
      "Episode 1485, Total Reward = 26.0, Total Steps = 15082\n",
      "Episode 1486, Total Reward = 7.0, Total Steps = 15089\n",
      "Episode 1487, Total Reward = 24.0, Total Steps = 15113\n",
      "Episode 1488, Total Reward = 6.0, Total Steps = 15119\n",
      "Episode 1489, Total Reward = 8.0, Total Steps = 15127\n",
      "Episode 1490, Total Reward = 34.0, Total Steps = 15161\n",
      "Episode 1491, Total Reward = 9.0, Total Steps = 15170\n",
      "Episode 1492, Total Reward = 7.0, Total Steps = 15177\n",
      "Episode 1493, Total Reward = 4.0, Total Steps = 15181\n",
      "Episode 1494, Total Reward = 20.0, Total Steps = 15201\n",
      "Episode 1495, Total Reward = 30.0, Total Steps = 15231\n",
      "Episode 1496, Total Reward = 4.0, Total Steps = 15235\n",
      "Episode 1497, Total Reward = 10.0, Total Steps = 15245\n",
      "Episode 1498, Total Reward = 24.0, Total Steps = 15269\n",
      "Episode 1499, Total Reward = 18.0, Total Steps = 15287\n",
      "Episode 1500, Total Reward = 10.0, Total Steps = 15297\n",
      "Episode 1501, Total Reward = 5.0, Total Steps = 15302\n",
      "Episode 1502, Total Reward = 10.0, Total Steps = 15312\n",
      "Episode 1503, Total Reward = 11.0, Total Steps = 15323\n",
      "Episode 1504, Total Reward = 5.0, Total Steps = 15328\n",
      "Episode 1505, Total Reward = 7.0, Total Steps = 15335\n",
      "Episode 1506, Total Reward = 10.0, Total Steps = 15345\n",
      "Episode 1507, Total Reward = 7.0, Total Steps = 15352\n",
      "Episode 1508, Total Reward = 17.0, Total Steps = 15369\n",
      "Episode 1509, Total Reward = 11.0, Total Steps = 15380\n",
      "Episode 1510, Total Reward = 17.0, Total Steps = 15397\n",
      "Episode 1511, Total Reward = 15.0, Total Steps = 15412\n",
      "Episode 1512, Total Reward = 11.0, Total Steps = 15423\n",
      "Episode 1513, Total Reward = 25.0, Total Steps = 15448\n",
      "Episode 1514, Total Reward = 12.0, Total Steps = 15460\n",
      "Episode 1515, Total Reward = 9.0, Total Steps = 15469\n",
      "Episode 1516, Total Reward = 19.0, Total Steps = 15488\n",
      "Episode 1517, Total Reward = 6.0, Total Steps = 15494\n",
      "Episode 1518, Total Reward = 5.0, Total Steps = 15499\n",
      "Episode 1519, Total Reward = 9.0, Total Steps = 15508\n",
      "Episode 1520, Total Reward = 10.0, Total Steps = 15518\n",
      "Episode 1521, Total Reward = 19.0, Total Steps = 15537\n",
      "Episode 1522, Total Reward = 35.0, Total Steps = 15572\n",
      "Episode 1523, Total Reward = 9.0, Total Steps = 15581\n",
      "Episode 1524, Total Reward = 24.0, Total Steps = 15605\n",
      "Episode 1525, Total Reward = 18.0, Total Steps = 15623\n",
      "Episode 1526, Total Reward = 13.0, Total Steps = 15636\n",
      "Episode 1527, Total Reward = 16.0, Total Steps = 15652\n",
      "Episode 1528, Total Reward = 21.0, Total Steps = 15673\n",
      "Episode 1529, Total Reward = 4.0, Total Steps = 15677\n",
      "Episode 1530, Total Reward = 13.0, Total Steps = 15690\n",
      "Episode 1531, Total Reward = 23.0, Total Steps = 15713\n",
      "Episode 1532, Total Reward = 26.0, Total Steps = 15739\n",
      "Episode 1533, Total Reward = 8.0, Total Steps = 15747\n",
      "Episode 1534, Total Reward = 13.0, Total Steps = 15760\n",
      "Episode 1535, Total Reward = 6.0, Total Steps = 15766\n",
      "Episode 1536, Total Reward = 6.0, Total Steps = 15772\n",
      "Episode 1537, Total Reward = 11.0, Total Steps = 15783\n",
      "Episode 1538, Total Reward = 9.0, Total Steps = 15792\n",
      "Episode 1539, Total Reward = 10.0, Total Steps = 15802\n",
      "Episode 1540, Total Reward = 10.0, Total Steps = 15812\n",
      "Episode 1541, Total Reward = 11.0, Total Steps = 15823\n",
      "Episode 1542, Total Reward = 19.0, Total Steps = 15842\n",
      "Episode 1543, Total Reward = 24.0, Total Steps = 15866\n",
      "Episode 1544, Total Reward = 7.0, Total Steps = 15873\n",
      "Episode 1545, Total Reward = 5.0, Total Steps = 15878\n",
      "Episode 1546, Total Reward = 15.0, Total Steps = 15893\n",
      "Episode 1547, Total Reward = 33.0, Total Steps = 15926\n",
      "Episode 1548, Total Reward = 12.0, Total Steps = 15938\n",
      "Episode 1549, Total Reward = 11.0, Total Steps = 15949\n",
      "Episode 1550, Total Reward = 12.0, Total Steps = 15961\n",
      "Episode 1551, Total Reward = 5.0, Total Steps = 15966\n",
      "Episode 1552, Total Reward = 14.0, Total Steps = 15980\n",
      "Episode 1553, Total Reward = 35.0, Total Steps = 16015\n",
      "Episode 1554, Total Reward = 5.0, Total Steps = 16020\n",
      "Episode 1555, Total Reward = 12.0, Total Steps = 16032\n",
      "Episode 1556, Total Reward = 24.0, Total Steps = 16056\n",
      "Episode 1557, Total Reward = 13.0, Total Steps = 16069\n",
      "Episode 1558, Total Reward = 17.0, Total Steps = 16086\n",
      "Episode 1559, Total Reward = 17.0, Total Steps = 16103\n",
      "Episode 1560, Total Reward = 8.0, Total Steps = 16111\n",
      "Episode 1561, Total Reward = 10.0, Total Steps = 16121\n",
      "Episode 1562, Total Reward = 5.0, Total Steps = 16126\n",
      "Episode 1563, Total Reward = 10.0, Total Steps = 16136\n",
      "Episode 1564, Total Reward = 7.0, Total Steps = 16143\n",
      "Episode 1565, Total Reward = 37.0, Total Steps = 16180\n",
      "Episode 1566, Total Reward = 10.0, Total Steps = 16190\n",
      "Episode 1567, Total Reward = 11.0, Total Steps = 16201\n",
      "Episode 1568, Total Reward = 5.0, Total Steps = 16206\n",
      "Episode 1569, Total Reward = 10.0, Total Steps = 16216\n",
      "Episode 1570, Total Reward = 7.0, Total Steps = 16223\n",
      "Episode 1571, Total Reward = 14.0, Total Steps = 16237\n",
      "Episode 1572, Total Reward = 4.0, Total Steps = 16241\n",
      "Episode 1573, Total Reward = 8.0, Total Steps = 16249\n",
      "Episode 1574, Total Reward = 10.0, Total Steps = 16259\n",
      "Episode 1575, Total Reward = 19.0, Total Steps = 16278\n",
      "Episode 1576, Total Reward = 21.0, Total Steps = 16299\n",
      "Episode 1577, Total Reward = 37.0, Total Steps = 16336\n",
      "Episode 1578, Total Reward = 14.0, Total Steps = 16350\n",
      "Episode 1579, Total Reward = 12.0, Total Steps = 16362\n",
      "Episode 1580, Total Reward = 10.0, Total Steps = 16372\n",
      "Episode 1581, Total Reward = 17.0, Total Steps = 16389\n",
      "Episode 1582, Total Reward = 7.0, Total Steps = 16396\n",
      "Episode 1583, Total Reward = 15.0, Total Steps = 16411\n",
      "Episode 1584, Total Reward = 13.0, Total Steps = 16424\n",
      "Episode 1585, Total Reward = 6.0, Total Steps = 16430\n",
      "Episode 1586, Total Reward = 10.0, Total Steps = 16440\n",
      "Episode 1587, Total Reward = 26.0, Total Steps = 16466\n",
      "Episode 1588, Total Reward = 5.0, Total Steps = 16471\n",
      "Episode 1589, Total Reward = 23.0, Total Steps = 16494\n",
      "Episode 1590, Total Reward = 5.0, Total Steps = 16499\n",
      "Episode 1591, Total Reward = 9.0, Total Steps = 16508\n",
      "Episode 1592, Total Reward = 14.0, Total Steps = 16522\n",
      "Episode 1593, Total Reward = 16.0, Total Steps = 16538\n",
      "Episode 1594, Total Reward = 8.0, Total Steps = 16546\n",
      "Episode 1595, Total Reward = 12.0, Total Steps = 16558\n",
      "Episode 1596, Total Reward = 16.0, Total Steps = 16574\n",
      "Episode 1597, Total Reward = 32.0, Total Steps = 16606\n",
      "Episode 1598, Total Reward = 5.0, Total Steps = 16611\n",
      "Episode 1599, Total Reward = 12.0, Total Steps = 16623\n",
      "Episode 1600, Total Reward = 9.0, Total Steps = 16632\n",
      "Episode 1601, Total Reward = 8.0, Total Steps = 16640\n",
      "Episode 1602, Total Reward = 16.0, Total Steps = 16656\n",
      "Episode 1603, Total Reward = 6.0, Total Steps = 16662\n",
      "Episode 1604, Total Reward = 5.0, Total Steps = 16667\n",
      "Episode 1605, Total Reward = 17.0, Total Steps = 16684\n",
      "Episode 1606, Total Reward = 10.0, Total Steps = 16694\n",
      "Episode 1607, Total Reward = 8.0, Total Steps = 16702\n",
      "Episode 1608, Total Reward = 23.0, Total Steps = 16725\n",
      "Episode 1609, Total Reward = 13.0, Total Steps = 16738\n",
      "Episode 1610, Total Reward = 8.0, Total Steps = 16746\n",
      "Episode 1611, Total Reward = 40.0, Total Steps = 16786\n",
      "Episode 1612, Total Reward = 6.0, Total Steps = 16792\n",
      "Episode 1613, Total Reward = 9.0, Total Steps = 16801\n",
      "Episode 1614, Total Reward = 15.0, Total Steps = 16816\n",
      "Episode 1615, Total Reward = 32.0, Total Steps = 16848\n",
      "Episode 1616, Total Reward = 35.0, Total Steps = 16883\n",
      "Episode 1617, Total Reward = 25.0, Total Steps = 16908\n",
      "Episode 1618, Total Reward = 16.0, Total Steps = 16924\n",
      "Episode 1619, Total Reward = 4.0, Total Steps = 16928\n",
      "Episode 1620, Total Reward = 7.0, Total Steps = 16935\n",
      "Episode 1621, Total Reward = 12.0, Total Steps = 16947\n",
      "Episode 1622, Total Reward = 8.0, Total Steps = 16955\n",
      "Episode 1623, Total Reward = 8.0, Total Steps = 16963\n",
      "Episode 1624, Total Reward = 10.0, Total Steps = 16973\n",
      "Episode 1625, Total Reward = 6.0, Total Steps = 16979\n",
      "Episode 1626, Total Reward = 5.0, Total Steps = 16984\n",
      "Episode 1627, Total Reward = 5.0, Total Steps = 16989\n",
      "Episode 1628, Total Reward = 5.0, Total Steps = 16994\n",
      "Episode 1629, Total Reward = 18.0, Total Steps = 17012\n",
      "Episode 1630, Total Reward = 8.0, Total Steps = 17020\n",
      "Episode 1631, Total Reward = 14.0, Total Steps = 17034\n",
      "Episode 1632, Total Reward = 10.0, Total Steps = 17044\n",
      "Episode 1633, Total Reward = 41.0, Total Steps = 17085\n",
      "Episode 1634, Total Reward = 10.0, Total Steps = 17095\n",
      "Episode 1635, Total Reward = 12.0, Total Steps = 17107\n",
      "Episode 1636, Total Reward = 11.0, Total Steps = 17118\n",
      "Episode 1637, Total Reward = 8.0, Total Steps = 17126\n",
      "Episode 1638, Total Reward = 18.0, Total Steps = 17144\n",
      "Episode 1639, Total Reward = 29.0, Total Steps = 17173\n",
      "Episode 1640, Total Reward = 13.0, Total Steps = 17186\n",
      "Episode 1641, Total Reward = 14.0, Total Steps = 17200\n",
      "Episode 1642, Total Reward = 11.0, Total Steps = 17211\n",
      "Episode 1643, Total Reward = 23.0, Total Steps = 17234\n",
      "Episode 1644, Total Reward = 8.0, Total Steps = 17242\n",
      "Episode 1645, Total Reward = 8.0, Total Steps = 17250\n",
      "Episode 1646, Total Reward = 15.0, Total Steps = 17265\n",
      "Episode 1647, Total Reward = 11.0, Total Steps = 17276\n",
      "Episode 1648, Total Reward = 16.0, Total Steps = 17292\n",
      "Episode 1649, Total Reward = 10.0, Total Steps = 17302\n",
      "Episode 1650, Total Reward = 11.0, Total Steps = 17313\n",
      "Episode 1651, Total Reward = 15.0, Total Steps = 17328\n",
      "Episode 1652, Total Reward = 6.0, Total Steps = 17334\n",
      "Episode 1653, Total Reward = 18.0, Total Steps = 17352\n",
      "Episode 1654, Total Reward = 8.0, Total Steps = 17360\n",
      "Episode 1655, Total Reward = 20.0, Total Steps = 17380\n",
      "Episode 1656, Total Reward = 6.0, Total Steps = 17386\n",
      "Episode 1657, Total Reward = 14.0, Total Steps = 17400\n",
      "Episode 1658, Total Reward = 4.0, Total Steps = 17404\n",
      "Episode 1659, Total Reward = 8.0, Total Steps = 17412\n",
      "Episode 1660, Total Reward = 4.0, Total Steps = 17416\n",
      "Episode 1661, Total Reward = 9.0, Total Steps = 17425\n",
      "Episode 1662, Total Reward = 18.0, Total Steps = 17443\n",
      "Episode 1663, Total Reward = 6.0, Total Steps = 17449\n",
      "Episode 1664, Total Reward = 19.0, Total Steps = 17468\n",
      "Episode 1665, Total Reward = 18.0, Total Steps = 17486\n",
      "Episode 1666, Total Reward = 19.0, Total Steps = 17505\n",
      "Episode 1667, Total Reward = 7.0, Total Steps = 17512\n",
      "Episode 1668, Total Reward = 19.0, Total Steps = 17531\n",
      "Episode 1669, Total Reward = 8.0, Total Steps = 17539\n",
      "Episode 1670, Total Reward = 12.0, Total Steps = 17551\n",
      "Episode 1671, Total Reward = 17.0, Total Steps = 17568\n",
      "Episode 1672, Total Reward = 9.0, Total Steps = 17577\n",
      "Episode 1673, Total Reward = 13.0, Total Steps = 17590\n",
      "Episode 1674, Total Reward = 10.0, Total Steps = 17600\n",
      "Episode 1675, Total Reward = 12.0, Total Steps = 17612\n",
      "Episode 1676, Total Reward = 14.0, Total Steps = 17626\n",
      "Episode 1677, Total Reward = 10.0, Total Steps = 17636\n",
      "Episode 1678, Total Reward = 16.0, Total Steps = 17652\n",
      "Episode 1679, Total Reward = 12.0, Total Steps = 17664\n",
      "Episode 1680, Total Reward = 6.0, Total Steps = 17670\n",
      "Episode 1681, Total Reward = 35.0, Total Steps = 17705\n",
      "Episode 1682, Total Reward = 9.0, Total Steps = 17714\n",
      "Episode 1683, Total Reward = 24.0, Total Steps = 17738\n",
      "Episode 1684, Total Reward = 13.0, Total Steps = 17751\n",
      "Episode 1685, Total Reward = 10.0, Total Steps = 17761\n",
      "Episode 1686, Total Reward = 5.0, Total Steps = 17766\n",
      "Episode 1687, Total Reward = 5.0, Total Steps = 17771\n",
      "Episode 1688, Total Reward = 9.0, Total Steps = 17780\n",
      "Episode 1689, Total Reward = 10.0, Total Steps = 17790\n",
      "Episode 1690, Total Reward = 21.0, Total Steps = 17811\n",
      "Episode 1691, Total Reward = 4.0, Total Steps = 17815\n",
      "Episode 1692, Total Reward = 16.0, Total Steps = 17831\n",
      "Episode 1693, Total Reward = 7.0, Total Steps = 17838\n",
      "Episode 1694, Total Reward = 5.0, Total Steps = 17843\n",
      "Episode 1695, Total Reward = 4.0, Total Steps = 17847\n",
      "Episode 1696, Total Reward = 9.0, Total Steps = 17856\n",
      "Episode 1697, Total Reward = 17.0, Total Steps = 17873\n",
      "Episode 1698, Total Reward = 8.0, Total Steps = 17881\n",
      "Episode 1699, Total Reward = 3.0, Total Steps = 17884\n",
      "Episode 1700, Total Reward = 5.0, Total Steps = 17889\n",
      "Episode 1701, Total Reward = 13.0, Total Steps = 17902\n",
      "Episode 1702, Total Reward = 22.0, Total Steps = 17924\n",
      "Episode 1703, Total Reward = 41.0, Total Steps = 17965\n",
      "Episode 1704, Total Reward = 5.0, Total Steps = 17970\n",
      "Episode 1705, Total Reward = 16.0, Total Steps = 17986\n",
      "Episode 1706, Total Reward = 16.0, Total Steps = 18002\n",
      "Episode 1707, Total Reward = 11.0, Total Steps = 18013\n",
      "Episode 1708, Total Reward = 13.0, Total Steps = 18026\n",
      "Episode 1709, Total Reward = 6.0, Total Steps = 18032\n",
      "Episode 1710, Total Reward = 10.0, Total Steps = 18042\n",
      "Episode 1711, Total Reward = 6.0, Total Steps = 18048\n",
      "Episode 1712, Total Reward = 8.0, Total Steps = 18056\n",
      "Episode 1713, Total Reward = 37.0, Total Steps = 18093\n",
      "Episode 1714, Total Reward = 31.0, Total Steps = 18124\n",
      "Episode 1715, Total Reward = 5.0, Total Steps = 18129\n",
      "Episode 1716, Total Reward = 31.0, Total Steps = 18160\n",
      "Episode 1717, Total Reward = 6.0, Total Steps = 18166\n",
      "Episode 1718, Total Reward = 11.0, Total Steps = 18177\n",
      "Episode 1719, Total Reward = 41.0, Total Steps = 18218\n",
      "Episode 1720, Total Reward = 10.0, Total Steps = 18228\n",
      "Episode 1721, Total Reward = 20.0, Total Steps = 18248\n",
      "Episode 1722, Total Reward = 6.0, Total Steps = 18254\n",
      "Episode 1723, Total Reward = 12.0, Total Steps = 18266\n",
      "Episode 1724, Total Reward = 8.0, Total Steps = 18274\n",
      "Episode 1725, Total Reward = 16.0, Total Steps = 18290\n",
      "Episode 1726, Total Reward = 12.0, Total Steps = 18302\n",
      "Episode 1727, Total Reward = 15.0, Total Steps = 18317\n",
      "Episode 1728, Total Reward = 29.0, Total Steps = 18346\n",
      "Episode 1729, Total Reward = 6.0, Total Steps = 18352\n",
      "Episode 1730, Total Reward = 6.0, Total Steps = 18358\n",
      "Episode 1731, Total Reward = 13.0, Total Steps = 18371\n",
      "Episode 1732, Total Reward = 11.0, Total Steps = 18382\n",
      "Episode 1733, Total Reward = 16.0, Total Steps = 18398\n",
      "Episode 1734, Total Reward = 52.0, Total Steps = 18450\n",
      "Episode 1735, Total Reward = 10.0, Total Steps = 18460\n",
      "Episode 1736, Total Reward = 12.0, Total Steps = 18472\n",
      "Episode 1737, Total Reward = 41.0, Total Steps = 18513\n",
      "Episode 1738, Total Reward = 31.0, Total Steps = 18544\n",
      "Episode 1739, Total Reward = 7.0, Total Steps = 18551\n",
      "Episode 1740, Total Reward = 24.0, Total Steps = 18575\n",
      "Episode 1741, Total Reward = 11.0, Total Steps = 18586\n",
      "Episode 1742, Total Reward = 13.0, Total Steps = 18599\n",
      "Episode 1743, Total Reward = 5.0, Total Steps = 18604\n",
      "Episode 1744, Total Reward = 19.0, Total Steps = 18623\n",
      "Episode 1745, Total Reward = 10.0, Total Steps = 18633\n",
      "Episode 1746, Total Reward = 11.0, Total Steps = 18644\n",
      "Episode 1747, Total Reward = 5.0, Total Steps = 18649\n",
      "Episode 1748, Total Reward = 27.0, Total Steps = 18676\n",
      "Episode 1749, Total Reward = 38.0, Total Steps = 18714\n",
      "Episode 1750, Total Reward = 5.0, Total Steps = 18719\n",
      "Episode 1751, Total Reward = 7.0, Total Steps = 18726\n",
      "Episode 1752, Total Reward = 7.0, Total Steps = 18733\n",
      "Episode 1753, Total Reward = 29.0, Total Steps = 18762\n",
      "Episode 1754, Total Reward = 8.0, Total Steps = 18770\n",
      "Episode 1755, Total Reward = 11.0, Total Steps = 18781\n",
      "Episode 1756, Total Reward = 23.0, Total Steps = 18804\n",
      "Episode 1757, Total Reward = 25.0, Total Steps = 18829\n",
      "Episode 1758, Total Reward = 10.0, Total Steps = 18839\n",
      "Episode 1759, Total Reward = 13.0, Total Steps = 18852\n",
      "Episode 1760, Total Reward = 8.0, Total Steps = 18860\n",
      "Episode 1761, Total Reward = 16.0, Total Steps = 18876\n",
      "Episode 1762, Total Reward = 7.0, Total Steps = 18883\n",
      "Episode 1763, Total Reward = 5.0, Total Steps = 18888\n",
      "Episode 1764, Total Reward = 10.0, Total Steps = 18898\n",
      "Episode 1765, Total Reward = 23.0, Total Steps = 18921\n",
      "Episode 1766, Total Reward = 15.0, Total Steps = 18936\n",
      "Episode 1767, Total Reward = 40.0, Total Steps = 18976\n",
      "Episode 1768, Total Reward = 22.0, Total Steps = 18998\n",
      "Episode 1769, Total Reward = 26.0, Total Steps = 19024\n",
      "Episode 1770, Total Reward = 12.0, Total Steps = 19036\n",
      "Episode 1771, Total Reward = 11.0, Total Steps = 19047\n",
      "Episode 1772, Total Reward = 8.0, Total Steps = 19055\n",
      "Episode 1773, Total Reward = 9.0, Total Steps = 19064\n",
      "Episode 1774, Total Reward = 32.0, Total Steps = 19096\n",
      "Episode 1775, Total Reward = 26.0, Total Steps = 19122\n",
      "Episode 1776, Total Reward = 7.0, Total Steps = 19129\n",
      "Episode 1777, Total Reward = 9.0, Total Steps = 19138\n",
      "Episode 1778, Total Reward = 7.0, Total Steps = 19145\n",
      "Episode 1779, Total Reward = 9.0, Total Steps = 19154\n",
      "Episode 1780, Total Reward = 26.0, Total Steps = 19180\n",
      "Episode 1781, Total Reward = 16.0, Total Steps = 19196\n",
      "Episode 1782, Total Reward = 11.0, Total Steps = 19207\n",
      "Episode 1783, Total Reward = 7.0, Total Steps = 19214\n",
      "Episode 1784, Total Reward = 10.0, Total Steps = 19224\n",
      "Episode 1785, Total Reward = 5.0, Total Steps = 19229\n",
      "Episode 1786, Total Reward = 4.0, Total Steps = 19233\n",
      "Episode 1787, Total Reward = 23.0, Total Steps = 19256\n",
      "Episode 1788, Total Reward = 19.0, Total Steps = 19275\n",
      "Episode 1789, Total Reward = 15.0, Total Steps = 19290\n",
      "Episode 1790, Total Reward = 14.0, Total Steps = 19304\n",
      "Episode 1791, Total Reward = 15.0, Total Steps = 19319\n",
      "Episode 1792, Total Reward = 18.0, Total Steps = 19337\n",
      "Episode 1793, Total Reward = 13.0, Total Steps = 19350\n",
      "Episode 1794, Total Reward = 6.0, Total Steps = 19356\n",
      "Episode 1795, Total Reward = 14.0, Total Steps = 19370\n",
      "Episode 1796, Total Reward = 12.0, Total Steps = 19382\n",
      "Episode 1797, Total Reward = 20.0, Total Steps = 19402\n",
      "Episode 1798, Total Reward = 7.0, Total Steps = 19409\n",
      "Episode 1799, Total Reward = 26.0, Total Steps = 19435\n",
      "Episode 1800, Total Reward = 21.0, Total Steps = 19456\n",
      "Episode 1801, Total Reward = 25.0, Total Steps = 19481\n",
      "Episode 1802, Total Reward = 7.0, Total Steps = 19488\n",
      "Episode 1803, Total Reward = 24.0, Total Steps = 19512\n",
      "Episode 1804, Total Reward = 24.0, Total Steps = 19536\n",
      "Episode 1805, Total Reward = 7.0, Total Steps = 19543\n",
      "Episode 1806, Total Reward = 17.0, Total Steps = 19560\n",
      "Episode 1807, Total Reward = 15.0, Total Steps = 19575\n",
      "Episode 1808, Total Reward = 12.0, Total Steps = 19587\n",
      "Episode 1809, Total Reward = 19.0, Total Steps = 19606\n",
      "Episode 1810, Total Reward = 27.0, Total Steps = 19633\n",
      "Episode 1811, Total Reward = 5.0, Total Steps = 19638\n",
      "Episode 1812, Total Reward = 12.0, Total Steps = 19650\n",
      "Episode 1813, Total Reward = 12.0, Total Steps = 19662\n",
      "Episode 1814, Total Reward = 26.0, Total Steps = 19688\n",
      "Episode 1815, Total Reward = 20.0, Total Steps = 19708\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 149\u001b[0m\n\u001b[0;32m    147\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPOAgent(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    148\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInvertedPendulum-v4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 149\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[32], line 108\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[1;34m(self, env, episodes, batch_size)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m    107\u001b[0m     action, log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m--> 108\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcritic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    109\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# Accumulate data in the lists\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[32], line 28\u001b[0m, in \u001b[0;36mCritic.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\torch\\fx\\traceback.py:68\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\traceback.py:232\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 232\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\traceback.py:395\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\traceback.py:434\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    430\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    431\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals,\n\u001b[0;32m    432\u001b[0m         end_lineno\u001b[38;5;241m=\u001b[39mend_lineno, colno\u001b[38;5;241m=\u001b[39mcolno, end_colno\u001b[38;5;241m=\u001b[39mend_colno))\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 434\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mc:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.fc(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        return mu, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, actor_lr=1e-4, critic_lr=1e-3, gamma=0.99, epsilon=0.2, k_epochs=10, c1=0.5, c2=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs  # Number of optimization epochs per batch\n",
    "        self.c1 = c1  # Value function coefficient\n",
    "        self.c2 = c2  # Entropy coefficient\n",
    "        self.actor = Actor(input_dim, action_dim)\n",
    "        self.critic = Critic(input_dim)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        mu, std = self.actor(state)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * 0.95 * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        # Convert lists or arrays to tensors outside the loop\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.k_epochs):  # Multiple optimization epochs\n",
    "            # Optimization steps for both actor and critic\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "\n",
    "            mu, std = self.actor(states)\n",
    "            dist = torch.distributions.Normal(mu, std)\n",
    "            log_probs_new = dist.log_prob(actions).sum(dim=-1)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(log_probs_new - log_probs_old)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - self.c2 * entropy  # Include entropy bonus\n",
    "            critic_loss = self.mse_loss(self.critic(states), returns) * self.c1  # Apply value loss coefficient directly\n",
    "\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "    def train(self, env, episodes=250000, batch_size=2048):\n",
    "        all_rewards = []\n",
    "        step_counter = 0\n",
    "        \n",
    "        # Initialize empty lists to collect data until batch size is reached\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()[0]\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                value = self.critic(torch.from_numpy(state).float()).item()\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                # Accumulate data in the lists\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "                step_counter += 1\n",
    "\n",
    "                if step_counter % batch_size == 0:\n",
    "                    # Update policy with the accumulated data once the batch size is reached\n",
    "                    next_value = self.critic(torch.from_numpy(state).float()).item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    # Clear the accumulated data for the next batch\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {episode + 1}, Total Reward = {episode_rewards}, Total Steps = {step_counter}\")\n",
    "\n",
    "            # Check termination condition (if needed)\n",
    "            if step_counter >= 1000000:\n",
    "                return all_rewards\n",
    "\n",
    "        return all_rewards\n",
    "    \n",
    "    \n",
    "ppo = PPOAgent(4, 1)\n",
    "env = gym.make('InvertedPendulum-v4')\n",
    "rewards = ppo.train(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "\n",
    "class AgentNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(AgentNet, self).__init__()\n",
    "        self.affine = nn.Linear(num_inputs, 128)\n",
    "        self.action_head = nn.Linear(128, num_outputs)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.affine(x))\n",
    "        action_probs = torch.softmax(self.action_head(x), dim=-1)\n",
    "        state_values = self.value_head(x)\n",
    "        return action_probs, state_values\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, config, policy_params):\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        self.model = AgentNet(env.observation_space.shape[0], env.action_space.n)\n",
    "        self.model.load_state_dict(policy_params)\n",
    "\n",
    "    def step(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs, state_value = self.model(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        next_state, reward, done, _ = self.env.step(action.item())\n",
    "        return action.item(), log_prob, state_value, next_state, reward, done\n",
    "\n",
    "    def update_policy(self, states, actions, old_log_probs, rewards, values, optimizer):\n",
    "        actions = torch.tensor(actions)\n",
    "        old_log_probs = torch.stack(old_log_probs)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        values = torch.cat(values)\n",
    "        masks = torch.tensor([1.0] * len(rewards))\n",
    "\n",
    "        # Adding last value for advantage calculation\n",
    "        _, last_value = self.model(torch.from_numpy(states[-1]).float().unsqueeze(0))\n",
    "        values = torch.cat([values, last_value.detach()])\n",
    "\n",
    "        advantages = self.compute_advantages(rewards, masks, values)\n",
    "\n",
    "        # Convert advantages to tensor and standardize\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Convert rewards to returns\n",
    "        returns = advantages + values[:-1]\n",
    "\n",
    "        # Optimization loop\n",
    "        for _ in range(self.config['epochs']):\n",
    "            idx = torch.randperm(len(states))\n",
    "            for batch_indices in idx.split(self.config['batch_size']):\n",
    "                sampled_states = torch.tensor(states)[batch_indices]\n",
    "                sampled_actions = actions[batch_indices]\n",
    "                sampled_old_log_probs = old_log_probs[batch_indices]\n",
    "                sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                new_probs, new_values = self.model(sampled_states)\n",
    "                new_dist = Categorical(new_probs)\n",
    "                new_log_probs = new_dist.log_prob(sampled_actions)\n",
    "\n",
    "                # Calculating the ratio (pi_theta / pi_theta_old):\n",
    "                ratio = torch.exp(new_log_probs - sampled_old_log_probs)\n",
    "\n",
    "                # Clipped surrogate loss\n",
    "                surr1 = ratio * sampled_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * sampled_advantages\n",
    "                loss = -torch.min(surr1, surr2).mean()  # Focus only on the clipping part\n",
    "\n",
    "                # take gradient step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m complete.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 58\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Aggregate data from all workers\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(return_dict)\n\u001b[1;32m---> 58\u001b[0m aggregated_data \u001b[38;5;241m=\u001b[39m {k: [] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mreturn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m aggregated_data\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\multiprocessing\\managers.py:837\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[1;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[0;32m    835\u001b[0m     dispatch(conn, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecref\u001b[39m\u001b[38;5;124m'\u001b[39m, (token\u001b[38;5;241m.\u001b[39mid,))\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proxy\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m convert_to_error(kind, result)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def worker(worker_id, policy_params, config, return_dict):\n",
    "    \"\"\"Worker process to collect data from the environment.\"\"\"\n",
    "    np.random.seed(worker_id)\n",
    "    torch.manual_seed(worker_id)\n",
    "    env = gym.make(config['env_name'])\n",
    "    agent = PPOAgent(env, config, policy_params)\n",
    "\n",
    "    state = env.reset()\n",
    "    rewards, log_probs, states, actions, values = [], [], [], [], []\n",
    "    for _ in range(config['horizon']):\n",
    "        action, log_prob, value, next_state, reward, done = agent.step(state)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        state = next_state if not done else env.reset()\n",
    "\n",
    "    return_dict[worker_id] = {\n",
    "        'states': states,\n",
    "        'actions': actions,\n",
    "        'log_probs': log_probs,\n",
    "        'values': values,\n",
    "        'rewards': rewards\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    config = {\n",
    "        'env_name': 'InvertedPendulum-v4',\n",
    "        'horizon': 2048,\n",
    "        'learning_rate': 3e-4,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 10,\n",
    "        'gamma': 0.99,\n",
    "        'gae_lambda': 0.95,\n",
    "        'num_workers': 4\n",
    "    }\n",
    "\n",
    "    env = gym.make(config['env_name'])\n",
    "    model = AgentNet(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "\n",
    "    for iteration in range(10):  # run for 10 iterations\n",
    "        processes = []\n",
    "        for i in range(config['num_workers']):\n",
    "            p = mp.Process(target=worker, args=(i, model.state_dict(), config, return_dict))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        \n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "        # Aggregate data from all workers\n",
    "        print(return_dict)\n",
    "        aggregated_data = {k: [] for k in return_dict[0].keys()}\n",
    "        for i in range(config['num_workers']):\n",
    "            for key in aggregated_data.keys():\n",
    "                aggregated_data[key].extend(return_dict[i][key])\n",
    "        \n",
    "        # Convert lists to tensors and perform PPO update\n",
    "        states = torch.FloatTensor(aggregated_data['states'])\n",
    "        actions = torch.LongTensor(aggregated_data['actions'])\n",
    "        old_log_probs = torch.stack(aggregated_data['log_probs'])\n",
    "        rewards = torch.FloatTensor(aggregated_data['rewards'])\n",
    "        values = torch.stack(aggregated_data['values'])\n",
    "\n",
    "        # Example PPO update, assuming `update_policy` is implemented\n",
    "        agent = PPOAgent(env, config, model.state_dict())\n",
    "        agent.update_policy(states, actions, old_log_probs, rewards, values, optimizer)\n",
    "\n",
    "        print(f'Iteration {iteration + 1} complete.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shani\\miniconda3\\envs\\gym\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.00537837,  0.00386772, -0.00311708,  0.01416213]),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('InvertedPendulum-v4')\n",
    "env.reset()\n",
    "env.step([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
