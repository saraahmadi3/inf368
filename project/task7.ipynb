{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "env = gym.make('InvertedPendulum-v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ignore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Actor-Critic Network for Continuous Action Spaces\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))  # Learnable log standard deviation\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mean)  # Standard deviation\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        return dist, self.critic(x)\n",
    "\n",
    "# PPO Agent with Clipping Method\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, clip_epsilon=0.2):\n",
    "        self.model = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = 0.99\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        dist, _ = self.model(state_tensor)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "    def train(self, states, actions, rewards, next_states, dones, old_log_probs):\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        actions_tensor = torch.FloatTensor(actions).unsqueeze(1)\n",
    "        next_states_tensor = torch.FloatTensor(next_states)\n",
    "        rewards_tensor = torch.FloatTensor(rewards)\n",
    "        dones_tensor = torch.FloatTensor(dones)\n",
    "        old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "\n",
    "        _, values = self.model(states_tensor)\n",
    "        dists, next_values = self.model(next_states_tensor)\n",
    "\n",
    "        new_log_probs = dists.log_prob(actions_tensor).sum(axis=1, keepdim=True)\n",
    "\n",
    "        advantages = rewards_tensor + self.gamma * next_values.squeeze() * (1 - dones_tensor) - values.squeeze()\n",
    "\n",
    "        ratios = (new_log_probs - old_log_probs_tensor).exp()\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "        surrogate_loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
    "\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        loss = surrogate_loss + critic_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# Training Function\n",
    "def train_agent(agent, env, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        states, actions, rewards, next_states, dones, old_log_probs = [], [], [], [], [], []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, done, _,_ = env.step([action])  # Action needs to be a list\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            old_log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            \n",
    "\n",
    "        agent.train(states, actions, rewards, next_states, dones, old_log_probs)\n",
    "        print(f'Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4')\n",
    "# agent = PPOAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "# train_agent(agent, env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent, env, episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.select_action(state)  # Use the trained policy to select actions\n",
    "            state, reward, done, _ , _= env.step([action])  # Action should be in the correct format\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f'Evaluation Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "    \n",
    "    average_reward = sum(total_rewards) / len(total_rewards)\n",
    "    print(f'Average Reward over {episodes} episodes: {average_reward}')\n",
    "    return average_reward\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4', render_mode='human')\n",
    "# evaluate_policy(agent, env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('InvertedPendulum-v4', render_mode='human')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.actor(x)\n",
    "        std = self.log_std.exp().expand_as(mean)\n",
    "        dist = torch.distributions.Normal(mean, std)\n",
    "        return dist, self.critic(x)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, state_dim, action_dim, horizon=2048, lr=3e-4, clip_epsilon=0.2, gamma=0.99, gae_lambda=0.95, epochs=10, minibatch_size=64):\n",
    "        self.model = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        dist, _ = self.model(state_tensor)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob.item()\n",
    "\n",
    "    def train(self, states, actions, rewards, next_states, dones, old_log_probs):\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        actions_tensor = torch.FloatTensor(actions).unsqueeze(1)\n",
    "        next_states_tensor = torch.FloatTensor(next_states)\n",
    "        rewards_tensor = torch.FloatTensor(rewards)\n",
    "        dones_tensor = torch.FloatTensor(dones)\n",
    "        old_log_probs_tensor = torch.FloatTensor(old_log_probs)\n",
    "\n",
    "        _, values = self.model(states_tensor)\n",
    "        dists, next_values = self.model(next_states_tensor)\n",
    "\n",
    "        new_log_probs = dists.log_prob(actions_tensor).sum(axis=1, keepdim=True)\n",
    "\n",
    "        advantages = rewards_tensor + self.gamma * next_values.squeeze() * (1 - dones_tensor) - values.squeeze()\n",
    "\n",
    "        ratios = (new_log_probs - old_log_probs_tensor).exp()\n",
    "        clipped_ratios = torch.clamp(ratios, 1 - self.clip_epsilon, 1 + self.clip_epsilon)\n",
    "        surrogate_loss = -torch.min(ratios * advantages, clipped_ratios * advantages).mean()\n",
    "\n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        loss = surrogate_loss + critic_loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "def train_agent(agent, env, episodes=1000):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        states, actions, rewards, next_states, dones, old_log_probs = [], [], [], [], [], []\n",
    "\n",
    "        while not done:\n",
    "            action, log_prob = agent.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step([action])\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            dones.append(done)\n",
    "            old_log_probs.append(log_prob)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "        \n",
    "        agent.train(states, actions, rewards, next_states, dones, old_log_probs)\n",
    "        print(f'Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4')\n",
    "# agent = PPOAgent(state_dim=env.observation_space.shape[0], action_dim=env.action_space.shape[0])\n",
    "# train_agent(agent, env, episodes=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(agent, env, episodes=10):\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _ = agent.select_action(state)  # Use the trained policy to select actions\n",
    "            state, reward, done, _ , _= env.step([action])  # Action should be in the correct format\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f'Evaluation Episode {episode + 1}: Total Reward: {total_reward}')\n",
    "    \n",
    "    average_reward = sum(total_rewards) / len(total_rewards)\n",
    "    print(f'Average Reward over {episodes} episodes: {average_reward}')\n",
    "    return average_reward\n",
    "\n",
    "# env = gym.make('InvertedPendulum-v4', render_mode='human')\n",
    "# evaluate_policy(agent, env, episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.distributions import Normal\n",
    "# import numpy as np\n",
    "# import os\n",
    "# # from torch.utils.tensorboard import SummaryWriter\n",
    "# from functools import reduce\n",
    "# from operator import mul\n",
    "\n",
    "# glob_i = 0\n",
    "# glob_error = 0\n",
    "\n",
    "# class PPOAgent():\n",
    "#     def __init__(self, TRAIN, env=None, traj=3, net_size=64, net_std=1,\n",
    "#                 lr=1e-4, bs=100, y=0.99, ep=10, W=None,\n",
    "#                 c1=0.5, c2=0.01):\n",
    "#         self.TRAIN  = TRAIN\n",
    "#         self.LR     = lr\n",
    "#         self.GAMMA  = y\n",
    "#         self.BATCHSIZE  = bs\n",
    "#         self.EPOCHS = ep\n",
    "#         self.NTRAJ = traj\n",
    "#         self.env = env\n",
    "#         self.C1 = c1\n",
    "#         self.C2 = c2\n",
    "\n",
    "#         self.WRITER = W\n",
    "\n",
    "#         self.input_size = reduce(mul, env.observation_space.shape, 1)\n",
    "#         self.out_size   = env.action_space.shape[0]\n",
    "\n",
    "#         self.model = AgentNet(self.input_size, self.out_size, h=net_size, std=net_std)\n",
    "#         self.opt = optim.Adam(self.model.parameters(), lr=self.LR)\n",
    "\n",
    "#         self.value_criterion = nn.MSELoss()\n",
    "\n",
    "#         self.trajectories = []\n",
    "#         self.states   = []\n",
    "#         self.rewards  = []\n",
    "#         self.actions  = []\n",
    "#         self.values   = []\n",
    "#         self.logAprob = []\n",
    "\n",
    "#     def __call__(self, state):\n",
    "#         # Each time an action is required, we save the\n",
    "#         # state value for computing advantages later\n",
    "#         state = torch.FloatTensor(state).reshape(1, self.input_size)\n",
    "#         normal, _ = self.model(state) \n",
    "#         action = normal.sample().reshape(1, self.out_size)\n",
    "\n",
    "#         self.values.append(normal.mean)\n",
    "#         self.actions.append(action)\n",
    "#         self.logAprob.append(normal.log_prob(action).squeeze(0))\n",
    "\n",
    "#         return action.numpy().reshape(self.out_size)\n",
    "\n",
    "#     def observe(self, s, r, s1, done, NEPISODE):\n",
    "#         if not self.TRAIN: return\n",
    "\n",
    "#         s  = torch.FloatTensor(s).reshape(1, self.input_size)\n",
    "#         s1 = torch.FloatTensor(s1).reshape(1, self.input_size)\n",
    "\n",
    "#         self.states.append(s)\n",
    "\n",
    "#         # For rewards we only maintain the value.\n",
    "#         self.rewards.append(r)\n",
    "\n",
    "#         if done:\n",
    "#             # Compute advantages using critic network\n",
    "#             with torch.no_grad():\n",
    "#                 s1_tensor = s1\n",
    "#                 _, next_value = self.model(s1_tensor)\n",
    "#             next_value = next_value.detach().squeeze().numpy()\n",
    "#             advantages = self._compute_advantages(next_value)\n",
    "\n",
    "#             # Update trajectories with advantages\n",
    "#             for traj, adv in zip(self.trajectories, advantages):\n",
    "#                 traj['Adv'] = torch.FloatTensor(adv)\n",
    "\n",
    "#             # Update Condition\n",
    "#             if NEPISODE != 0 and (NEPISODE % self.NTRAJ) == 0: \n",
    "#                 self.update() \n",
    "\n",
    "#             self.states   = []\n",
    "#             self.actions  = []\n",
    "#             self.logAprob = []\n",
    "#             self.values   = []\n",
    "#             self.rewards  = []\n",
    "\n",
    "#     def _compute_advantages(self, next_value):\n",
    "#         advantages = []\n",
    "#         for traj in self.trajectories:\n",
    "#             rewards = traj['r']\n",
    "#             values = traj['V'].numpy()\n",
    "#             deltas = [r + self.GAMMA * next_v - v for r, next_v, v in zip(rewards, [next_value] + values[:-1], values)]\n",
    "#             advantages.extend(self._discount_cumsum(deltas, self.GAMMA * self.C2))\n",
    "#         return advantages\n",
    "\n",
    "#     def _discount_cumsum(self, x, discount):\n",
    "#         \"\"\"\n",
    "#         Compute discounted cumulative sums of vectors.\n",
    "\n",
    "#         Parameters:\n",
    "#             x (list): Input list of numbers.\n",
    "#             discount (float): Discount factor.\n",
    "\n",
    "#         Returns:\n",
    "#             list: Discounted cumulative sums.\n",
    "#         \"\"\"\n",
    "#         discounted = [x[-1]]\n",
    "#         for v in reversed(x[:-1]):\n",
    "#             discounted.append(v + discount * discounted[-1])\n",
    "#         return list(reversed(discounted))\n",
    "\n",
    "#     def update(self):\n",
    "#         EPS = 0.2\n",
    "\n",
    "#         # Compute advantages\n",
    "#         S = torch.cat([x['S'] for x in self.trajectories], 0)\n",
    "#         A = torch.cat([x['A'] for x in self.trajectories], 0)\n",
    "#         Adv = torch.cat([x['Adv'] for x in self.trajectories], 0)\n",
    "#         Log_old = torch.cat([x['LogP'] for x in self.trajectories], 0)\n",
    "#         G = torch.cat([x['G'] for x in self.trajectories], 0)\n",
    "#         V = torch.cat([x['V'] for x in self.trajectories], 0)\n",
    "\n",
    "#         bufsize = S.size(0)\n",
    "\n",
    "#         for ep in range(self.EPOCHS*(bufsize//self.BATCHSIZE+1)):\n",
    "#             ids = np.random.randint(0, bufsize,\n",
    "#                     min(self.BATCHSIZE, bufsize))\n",
    "\n",
    "#             bS, bA, bAdv = S[ids,:], A[ids,:], Adv[ids,:]\n",
    "\n",
    "#             normal, Vnew = self.model(bS)\n",
    "#             logAprob_old = Log_old[ids,:]\n",
    "#             logAprob = normal.log_prob(bA)\n",
    "\n",
    "#             # L_CLIP\n",
    "#             ratio = (logAprob - logAprob_old).exp().squeeze(0)\n",
    "#             m1 = ratio * bAdv\n",
    "#             m2 = torch.clamp(ratio, 1.0 - EPS, 1.0 + EPS) * bAdv\n",
    "#             L_CLIP = torch.min(m1, m2).mean()\n",
    "\n",
    "#             # L_VF\n",
    "#             L_VF = (G[ids,:] - Vnew).pow(2).mean()\n",
    "\n",
    "#             # Entropy\n",
    "#             E = normal.entropy().mean()\n",
    "\n",
    "#             # Total Loss\n",
    "#             L = -L_CLIP + self.C1 * L_VF - self.C2 * E\n",
    "\n",
    "#             # Apply Gradients\n",
    "#             self.opt.zero_grad()\n",
    "#             L.backward()\n",
    "#             self.opt.step()\n",
    "\n",
    "#         # Update Graphs\n",
    "#         # self.WRITER.add_scalar(\"L_VF\", L_VF, glob_i)\n",
    "#         # global glob_i\n",
    "#         # glob_i += 1\n",
    "#         self.trajectories = []\n",
    "\n",
    "#     def load(self, path):\n",
    "#         try:\n",
    "#             self.model.load_state_dict(torch.load(path))\n",
    "#             self.model.eval()\n",
    "#         except FileNotFoundError:\n",
    "#             print(f'Error: {path} not found.')\n",
    "#             exit()\n",
    "\n",
    "#     def save(self, path):\n",
    "#         torch.save(self.model.state_dict(), path)\n",
    "\n",
    "# class AgentNet(nn.Module):\n",
    "#     def __init__(self, inp, out, h=32, std=0):\n",
    "#         super(AgentNet, self).__init__()\n",
    "\n",
    "#         self.actor = nn.Sequential(\n",
    "#             nn.Linear(inp, h),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h, h//2),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h//2, out),\n",
    "#         )\n",
    "#         self.log_std = nn.Parameter(torch.ones(1, out) * std)\n",
    "\n",
    "#         self.critic = nn.Sequential(\n",
    "#             nn.Linear(inp, h),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h, h//2),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Linear(h//2, 1)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         actor_output = self.actor(x)\n",
    "#         critic_output = self.critic(x)\n",
    "#         std = self.log_std.exp().expand_as(actor_output)\n",
    "#         dist = Normal(actor_output, std)\n",
    "#         return dist, critic_output\n",
    "\n",
    "# # main.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # import mujoco_py\n",
    "# import gym\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# # from torch.utils.tensorboard import SummaryWriter\n",
    "# # from ppo_agent import PPOAgent\n",
    "# import sys, os, json\n",
    "# from glob import glob\n",
    "# import argparse\n",
    "\n",
    "# # Environemnt Params\n",
    "# MODELS_PATH        = 'models'\n",
    "# DEFAULT_EPISODES   = 2000\n",
    "# DEFAULT_MAX_STEPS  = 2000\n",
    "# DEFAULT_CHECKPOINT = 5\n",
    "\n",
    "# ## These may be updated by arguments\n",
    "# EnvName = 'InvertedPendulum-v4'\n",
    "# TRAIN = False\n",
    "# RENDER = True\n",
    "\n",
    "# # writer = SummaryWriter(max_queue=2)\n",
    "\n",
    "# def main():\n",
    "#     global TRAIN, RENDER, EnvName\n",
    "\n",
    "#     # Training Parameters\n",
    "#     params_path = os.path.join(MODELS_PATH, f'{EnvName}.json')\n",
    "#     with open(params_path, 'r') as f:\n",
    "#         params = json.load(f)\n",
    "\n",
    "#     EPOCHS        = params[\"EPOCHS\"] if \"EPOCHS\" in params else 200\n",
    "#     LR            = params[\"LR\"] if \"LR\" in params else 1e-3\n",
    "#     C2            = params[\"C2\"] if \"C2\" in params else 0\n",
    "#     GAMMA         = params[\"GAMMA\"] if \"GAMMA\" in params else 0.99\n",
    "#     STD           = params[\"STD\"] if \"STD\" in params else 1\n",
    "#     NETSIZE       = params[\"NETSIZE\"] if \"NETSIZE\" in params else 64\n",
    "#     BATCHSIZE     = params[\"BATCHSIZE\"] if \"BATCHSIZE\" in params else 500\n",
    "#     TRAJECTORIES  = params[\"TRAJECTORIES\"] if \"TRAJECTORIES\" in params else 10\n",
    "#     MAX_STEPS     = params[\"MAX_STEPS\"] if \"MAX_STEPS\" in params else 2000\n",
    "#     CHECKPOINT    = params[\"CHECKPOINT\"] if \"CHECKPOINT\" in params else 5\n",
    "#     EPISODES      = params[\"EPISODES\"] if \"EPISODES\" in params else 2000\n",
    "\n",
    "#     print(\"Environment:  \", EnvName)\n",
    "#     print(\"Train:        \", TRAIN)\n",
    "#     print(\"EPOCHS:       \", EPOCHS)\n",
    "#     print(\"LR:           \", LR)\n",
    "#     print(\"C2:           \", C2)\n",
    "#     print(\"GAMMA:        \", GAMMA)\n",
    "#     print(\"STD:          \", STD)\n",
    "#     print(\"NETSIZE:      \", NETSIZE)\n",
    "#     print(\"BATCHSIZE:    \", BATCHSIZE)\n",
    "#     print(\"TRAJECTORIES: \", TRAJECTORIES)\n",
    "#     print(\"MAX_STEPS:    \", MAX_STEPS)\n",
    "#     print(\"CHECKPOINT:   \", CHECKPOINT)\n",
    "#     print(\"EPISODES:     \", EPISODES)\n",
    "\n",
    "#     # save model after collecting N trajectories \n",
    "#     # (which corresponds to when the update is calculated)\n",
    "#     SAVE_STEP = CHECKPOINT * TRAJECTORIES\n",
    "#     save_model_name = os.path.join(MODELS_PATH, EnvName + \".pth\")\n",
    "\n",
    "#     total = 0\n",
    "\n",
    "#     env = gym.make(EnvName, render_mode=\"human\" if RENDER else None)\n",
    "#     agent = PPOAgent(\n",
    "#             TRAIN, env=env,\n",
    "#             lr=LR, c2=C2,\n",
    "#             net_size=NETSIZE,\n",
    "#             net_std=STD,\n",
    "#             y=GAMMA,\n",
    "#             traj=TRAJECTORIES,\n",
    "#             bs=BATCHSIZE,\n",
    "#             ep=EPOCHS\n",
    "#     )\n",
    "#     if not TRAIN: agent.load(save_model_name)\n",
    "\n",
    "#     for i in range(EPISODES):\n",
    "#         state, _ = env.reset()\n",
    "\n",
    "#         for t in range(MAX_STEPS+1):\n",
    "#             # RL Step\n",
    "#             action = agent(state)\n",
    "#             new_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "#             # Impose done=True if last-step\n",
    "#             if t == MAX_STEPS: done = True\n",
    "\n",
    "#             agent.observe(state, reward, new_state, done, i)\n",
    "\n",
    "#             total += reward\n",
    "#             state  = new_state\n",
    "\n",
    "#             if done: break\n",
    "\n",
    "#         # Print Performance\n",
    "#         print(f\"[{i}] Steps: {t}\\tReward: {total}\")\n",
    "#         # writer.add_scalar('Reward', total, i)\n",
    "#         total = 0\n",
    "\n",
    "#         if TRAIN and (i % SAVE_STEP) == SAVE_STEP -1:\n",
    "#             agent.save(save_model_name)\n",
    "#             print(\"Model Checkpoint saved\")\n",
    "\n",
    "#     env.close()\n",
    "\n",
    "\n",
    "# # envs_names = glob(f'{MODELS_PATH}/*.json')\n",
    "# # envs_names = [x.split('/')[-1].split('.')[0] for x in envs_names]\n",
    "\n",
    "# # parser = argparse.ArgumentParser(description=\"Train PPO models and run Gym environments\")\n",
    "# # parser.add_argument('env', type=str, metavar=\"environment\", help=\", \".join(envs_names),\n",
    "# #                     choices=envs_names, default=\"MountainCarContinuous-v0\")\n",
    "# # parser.add_argument('--train', action='store_true')\n",
    "# # args = parser.parse_args()\n",
    "\n",
    "# # EnvName = EnvName\n",
    "# # TRAIN   = TRAIN\n",
    "# # RENDER  = not(TRAIN)\n",
    "\n",
    "# # main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, epochs=200, lr=1e-3, gamma=0.99, c1=0.5, c2=0.01, batch_size=500):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            for _ in range(self.epochs):\n",
    "                self.update(states, actions, rewards, log_probs, values)\n",
    "\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Ensure the state tensor is of type float\n",
    "            dist, value = self.agent(state_tensor)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "            \n",
    "            states.append(state_tensor)  # Append the state tensor\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "            values.append(value)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset()[0]\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            print(states.shape, self.agent)\n",
    "            dist, new_values = self.agent(states)\n",
    "            new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "            ratio = (new_log_probs - log_probs).exp()\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.c2, 1.0 + self.c2) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean()\n",
    "            critic_loss = (values - returns).pow(2).mean()\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = self.compute_returns(rewards) - values.squeeze(1)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# ppo = PPO(\"InvertedPendulum-v4\")\n",
    "# ppo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Updating\n",
      "Epoch 2\n",
      "Updating\n",
      "Epoch 3\n",
      "Updating\n",
      "Epoch 4\n",
      "Updating\n",
      "Epoch 5\n",
      "Updating\n",
      "Epoch 6\n",
      "Updating\n",
      "Epoch 7\n",
      "Updating\n",
      "Epoch 8\n",
      "Updating\n",
      "Epoch 9\n",
      "Updating\n",
      "Epoch 10\n",
      "Updating\n",
      "Epoch 11\n",
      "Updating\n",
      "Epoch 12\n",
      "Updating\n",
      "Epoch 13\n",
      "Updating\n",
      "Epoch 14\n",
      "Updating\n",
      "Epoch 15\n",
      "Updating\n",
      "Epoch 16\n",
      "Updating\n",
      "Epoch 17\n",
      "Updating\n",
      "Epoch 18\n",
      "Updating\n",
      "Epoch 19\n",
      "Updating\n",
      "Epoch 20\n",
      "Updating\n",
      "Epoch 21\n",
      "Updating\n",
      "Epoch 22\n",
      "Updating\n",
      "Epoch 23\n",
      "Updating\n",
      "Epoch 24\n",
      "Updating\n",
      "Epoch 25\n",
      "Updating\n",
      "Epoch 26\n",
      "Updating\n",
      "Epoch 27\n",
      "Updating\n",
      "Epoch 28\n",
      "Updating\n",
      "Epoch 29\n",
      "Updating\n",
      "Epoch 30\n",
      "Updating\n",
      "Epoch 31\n",
      "Updating\n",
      "Epoch 32\n",
      "Updating\n",
      "Epoch 33\n",
      "Updating\n",
      "Epoch 34\n",
      "Updating\n",
      "Epoch 35\n",
      "Updating\n",
      "Epoch 36\n",
      "Updating\n",
      "Epoch 37\n",
      "Updating\n",
      "Epoch 38\n",
      "Updating\n",
      "Epoch 39\n",
      "Updating\n",
      "Epoch 40\n",
      "Updating\n",
      "Epoch 41\n",
      "Updating\n",
      "Epoch 42\n",
      "Updating\n",
      "Epoch 43\n",
      "Updating\n",
      "Epoch 44\n",
      "Updating\n",
      "Epoch 45\n",
      "Updating\n",
      "Epoch 46\n",
      "Updating\n",
      "Epoch 47\n",
      "Updating\n",
      "Epoch 48\n",
      "Updating\n",
      "Epoch 49\n",
      "Updating\n",
      "Epoch 50\n",
      "Updating\n",
      "Epoch 51\n",
      "Updating\n",
      "Epoch 52\n",
      "Updating\n",
      "Epoch 53\n",
      "Updating\n",
      "Epoch 54\n",
      "Updating\n",
      "Epoch 55\n",
      "Updating\n",
      "Epoch 56\n",
      "Updating\n",
      "Epoch 57\n",
      "Updating\n",
      "Epoch 58\n",
      "Updating\n",
      "Epoch 59\n",
      "Updating\n",
      "Epoch 60\n",
      "Updating\n",
      "Epoch 61\n",
      "Updating\n",
      "Epoch 62\n",
      "Updating\n",
      "Epoch 63\n",
      "Updating\n",
      "Epoch 64\n",
      "Updating\n",
      "Epoch 65\n",
      "Updating\n",
      "Epoch 66\n",
      "Updating\n",
      "Epoch 67\n",
      "Updating\n",
      "Epoch 68\n",
      "Updating\n",
      "Epoch 69\n",
      "Updating\n",
      "Epoch 70\n",
      "Updating\n",
      "Epoch 71\n",
      "Updating\n",
      "Epoch 72\n",
      "Updating\n",
      "Epoch 73\n",
      "Updating\n",
      "Epoch 74\n",
      "Updating\n",
      "Epoch 75\n",
      "Updating\n",
      "Epoch 76\n",
      "Updating\n",
      "Epoch 77\n",
      "Updating\n",
      "Epoch 78\n",
      "Updating\n",
      "Epoch 79\n",
      "Updating\n",
      "Epoch 80\n",
      "Updating\n",
      "Epoch 81\n",
      "Updating\n",
      "Epoch 82\n",
      "Updating\n",
      "Epoch 83\n",
      "Updating\n",
      "Epoch 84\n",
      "Updating\n",
      "Epoch 85\n",
      "Updating\n",
      "Epoch 86\n",
      "Updating\n",
      "Epoch 87\n",
      "Updating\n",
      "Epoch 88\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 146\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m average_reward\n\u001b[0;32m    145\u001b[0m ppo \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvertedPendulum-v4\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 146\u001b[0m \u001b[43mppo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 52\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 52\u001b[0m     states, actions, rewards, log_probs, values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;66;03m# Detach tensors before passing them to the update method\u001b[39;00m\n\u001b[0;32m     55\u001b[0m     states_detached \u001b[38;5;241m=\u001b[39m states\u001b[38;5;241m.\u001b[39mdetach()\n",
      "Cell \u001b[1;32mIn[40], line 82\u001b[0m, in \u001b[0;36mPPO.generate_trajectories\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m actions\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m     81\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n\u001b[1;32m---> 82\u001b[0m log_probs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     83\u001b[0m values\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[0;32m     85\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#this one sorta, but doesnt use correct batch and stuyff\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, epochs=1000, lr=3e-4, gamma=0.99, c1=0.5, c2=0.01, batch_size=64):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.epochs = epochs\n",
    "        self.gamma = gamma\n",
    "        self.c1 = c1\n",
    "        self.c2 = c2\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'Epoch {epoch + 1}')\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            # Detach tensors before passing them to the update method\n",
    "            states_detached = states.detach()\n",
    "            actions_detached = actions.detach()\n",
    "            rewards_detached = rewards.detach()\n",
    "            log_probs_detached = log_probs.detach()\n",
    "            values_detached = values.detach()\n",
    "            print('Updating')\n",
    "            self.update(states_detached, actions_detached, rewards_detached, log_probs_detached, values_detached)\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        state = self.env.reset()[0]\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)  # Ensure the state tensor is of type float\n",
    "            dist, value = self.agent(state_tensor)\n",
    "            action = dist.sample()\n",
    "\n",
    "            next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "            \n",
    "            states.append(state_tensor)  # Append the state tensor\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "            values.append(value)\n",
    "\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state = self.env.reset()[0]\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        self.optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "        dist, new_values = self.agent(states)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        ratio = (new_log_probs - log_probs).exp()\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.c2, 1.0 + self.c2) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (values - returns).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = actor_loss + self.c1 * critic_loss - self.c2 * entropy\n",
    "\n",
    "        loss.backward(retain_graph=True)  # Retain computational graph\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = self.compute_returns(rewards) - values.squeeze(1)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    def evaluate(self, env, num_episodes=10):\n",
    "        total_rewards = 0\n",
    "        for _ in range(num_episodes):\n",
    "            state = env.reset()[0]\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, _ = self.agent(state_tensor)\n",
    "                action = dist.mean  # Using mean action for evaluation\n",
    "                next_state, reward, done, _, _ = env.step(action.detach().numpy()[0])\n",
    "                state = next_state\n",
    "                episode_reward += reward\n",
    "\n",
    "            total_rewards += episode_reward\n",
    "\n",
    "        average_reward = total_rewards / num_episodes\n",
    "        return average_reward\n",
    "\n",
    "ppo = PPO(\"InvertedPendulum-v4\")\n",
    "ppo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Create a new environment for evaluation\n",
    "# eval_env = gym.make('InvertedPendulum-v4', render_mode='human')\n",
    "\n",
    "# average_reward = ppo.evaluate(eval_env)\n",
    "# print(f\"Average reward over 10 episodes: {average_reward}\")\n",
    "\n",
    "# # Don't forget to close the evaluation environment when done\n",
    "# eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, horizon=2048, lr=3e-4, epochs=10, minibatch_size=64, gamma=0.99, gae_lambda=0.95):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def train(self, total_timesteps=1000):\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < total_timesteps:\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            # Update timesteps_so_far\n",
    "            print(states.shape)\n",
    "            timesteps_so_far += states.shape[0]\n",
    "\n",
    "            self.update(states, actions, rewards, log_probs, values)\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < self.horizon:\n",
    "            state = self.env.reset()[0]\n",
    "\n",
    "            for _ in range(self.minibatch_size):\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, value = self.agent(state_tensor)\n",
    "                action = dist.sample()\n",
    "\n",
    "                next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "                \n",
    "                states.append(state_tensor)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "                values.append(value)\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    state = self.env.reset()[0]\n",
    "\n",
    "            timesteps_so_far += self.minibatch_size\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        dist, new_values = self.agent(states)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        ratio = (new_log_probs - log_probs).exp()\n",
    "        print(ratio.shape, advantages.shape)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.gae_lambda, 1.0 + self.gae_lambda) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (values - returns).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = actor_loss + critic_loss - 0.01 * entropy\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = self.compute_returns(rewards) - values.squeeze(1)\n",
    "        return (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "# ppo = PPO(\"InvertedPendulum-v4\")\n",
    "# ppo.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class PPOAgent(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_size=64, std=0.1):\n",
    "        super(PPOAgent, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, action_dim),\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.ones(1, action_dim) * std)\n",
    "\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        actor_output = self.actor(x)\n",
    "        critic_output = self.critic(x)\n",
    "        std = self.log_std.exp().expand_as(actor_output)\n",
    "        dist = Normal(actor_output, std)\n",
    "        return dist, critic_output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env_name, horizon=2048, lr=3e-4, epochs=10, minibatch_size=64, gamma=0.99, gae_lambda=0.95):\n",
    "        self.env = gym.make(env_name)\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.agent = PPOAgent(self.state_dim, self.action_dim)\n",
    "        self.optimizer = optim.Adam(self.agent.parameters(), lr=lr)\n",
    "        self.horizon = horizon\n",
    "        self.epochs = epochs\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "    def train(self, total_timesteps=1000000):\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < total_timesteps:\n",
    "            states, actions, rewards, log_probs, values = self.generate_trajectories()\n",
    "\n",
    "            # Update timesteps_so_far\n",
    "            timesteps_so_far += states.shape[0]\n",
    "\n",
    "            self.update(states, actions, rewards, log_probs, values)\n",
    "\n",
    "    def generate_trajectories(self):\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "\n",
    "        timesteps_so_far = 0\n",
    "        while timesteps_so_far < self.horizon:\n",
    "            state = self.env.reset()[0]\n",
    "\n",
    "            for _ in range(self.minibatch_size):\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                dist, value = self.agent(state_tensor)\n",
    "                action = dist.sample()\n",
    "\n",
    "                next_state, reward, done, _, _ = self.env.step(action.numpy()[0])\n",
    "                \n",
    "                states.append(state_tensor)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(dist.log_prob(action).unsqueeze(0))\n",
    "                values.append(value)\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    state = self.env.reset()[0]\n",
    "\n",
    "            timesteps_so_far += self.minibatch_size\n",
    "\n",
    "        return torch.cat(states), torch.cat(actions), torch.tensor(rewards), torch.cat(log_probs), torch.cat(values)\n",
    "\n",
    "    def update(self, states, actions, rewards, log_probs, values):\n",
    "        returns = self.compute_returns(rewards)\n",
    "        advantages = self.compute_advantages(rewards, values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        dist, new_values = self.agent(states)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "\n",
    "        # Expand ratio to match the shape of advantages\n",
    "        ratio = (new_log_probs - log_probs).exp().unsqueeze(1)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.gae_lambda, 1.0 + self.gae_lambda) * advantages\n",
    "\n",
    "        actor_loss = -torch.min(surr1, surr2).mean()\n",
    "        critic_loss = (values - returns).pow(2).mean()\n",
    "        entropy = dist.entropy().mean()\n",
    "\n",
    "        loss = actor_loss + critic_loss - 0.01 * entropy\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def compute_returns(self, rewards):\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return torch.tensor(returns)\n",
    "\n",
    "    def compute_advantages(self, rewards, values):\n",
    "        advantages = []\n",
    "        G = 0\n",
    "        for r, v in zip(reversed(rewards), reversed(values)):\n",
    "            G = r + self.gamma * G\n",
    "            advantages.insert(0, G - v.item())\n",
    "        advantages = torch.tensor(advantages)\n",
    "        \n",
    "        # Split advantages into minibatches\n",
    "        minibatch_advantages = []\n",
    "        minibatch_size = len(advantages) // self.minibatch_size\n",
    "        for i in range(self.minibatch_size):\n",
    "            start_idx = i * minibatch_size\n",
    "            end_idx = (i + 1) * minibatch_size\n",
    "            minibatch_advantages.append(advantages[start_idx:end_idx])\n",
    "        \n",
    "        # Stack minibatch advantages along a new dimension\n",
    "        minibatch_advantages = torch.stack(minibatch_advantages, dim=1)\n",
    "        \n",
    "        # Calculate mean and standard deviation batch-wise\n",
    "        advantages_mean = minibatch_advantages.mean(dim=1)\n",
    "        advantages_std = minibatch_advantages.std(dim=1) + 1e-8\n",
    "        \n",
    "        # Normalize advantages using batch-wise mean and std\n",
    "        normalized_advantages = (minibatch_advantages - advantages_mean.unsqueeze(1)) / advantages_std.unsqueeze(1)\n",
    "        \n",
    "        return normalized_advantages\n",
    "\n",
    "# ppo = PPO(\"InvertedPendulum-v4\")\n",
    "# ppo.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# joakim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu = self.fc(x)\n",
    "        std = self.log_std.exp().expand_as(mu)\n",
    "        return mu, std\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, input_dim, action_dim, actor_lr=1e-4, critic_lr=1e-3, gamma=0.99, epsilon=0.2, k_epochs=10, c1=0.5, c2=0.01):\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.k_epochs = k_epochs  # Number of optimization epochs per batch\n",
    "        self.c1 = c1  # Value function coefficient\n",
    "        self.c2 = c2  # Entropy coefficient\n",
    "        self.actor = Actor(input_dim, action_dim)\n",
    "        self.critic = Critic(input_dim)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.from_numpy(state).float()\n",
    "        mu, std = self.actor(state)\n",
    "        distribution = torch.distributions.Normal(mu, std)\n",
    "        action = distribution.sample()\n",
    "        log_prob = distribution.log_prob(action).sum(dim=-1)\n",
    "        return action.detach().numpy(), log_prob\n",
    "\n",
    "    def compute_gae(self, rewards, masks, values, next_value):\n",
    "        gae = 0\n",
    "        returns = []\n",
    "        values = values + [next_value]\n",
    "        for step in reversed(range(len(rewards))):\n",
    "            delta = rewards[step] + self.gamma * values[step + 1] * masks[step] - values[step]\n",
    "            gae = delta + self.gamma * 0.95 * masks[step] * gae\n",
    "            returns.insert(0, gae + values[step])\n",
    "        return returns\n",
    "\n",
    "    def update(self, trajectory):\n",
    "        states, actions, log_probs_old, returns, advantages = trajectory\n",
    "\n",
    "        # Convert lists or arrays to tensors outside the loop\n",
    "        log_probs_old = torch.stack(log_probs_old).detach()\n",
    "        states = torch.stack(states).detach()\n",
    "        actions = torch.tensor(actions).detach()\n",
    "        returns = torch.tensor(returns).unsqueeze(-1).detach()\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-10)\n",
    "\n",
    "        for _ in range(self.k_epochs):  # Multiple optimization epochs\n",
    "            # Optimization steps for both actor and critic\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "\n",
    "            mu, std = self.actor(states)\n",
    "            dist = torch.distributions.Normal(mu, std)\n",
    "            log_probs_new = dist.log_prob(actions).sum(dim=-1)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(log_probs_new - log_probs_old)\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.epsilon, 1 + self.epsilon) * advantages\n",
    "\n",
    "            actor_loss = -torch.min(surr1, surr2).mean() - self.c2 * entropy  # Include entropy bonus\n",
    "            critic_loss = self.mse_loss(self.critic(states), returns) * self.c1  # Apply value loss coefficient directly\n",
    "\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "    def train(self, env, episodes=250000, batch_size=2048):\n",
    "        all_rewards = []\n",
    "        step_counter = 0\n",
    "        \n",
    "        # Initialize empty lists to collect data until batch size is reached\n",
    "        states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()[0]\n",
    "            episode_rewards = 0\n",
    "\n",
    "            for _ in range(1000):\n",
    "                action, log_prob = self.select_action(state)\n",
    "                value = self.critic(torch.from_numpy(state).float()).item()\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "                # Accumulate data in the lists\n",
    "                states.append(torch.from_numpy(state).float())\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(log_prob)\n",
    "                values.append(value)\n",
    "                masks.append(1 - done)\n",
    "\n",
    "                state = next_state\n",
    "                episode_rewards += reward\n",
    "                step_counter += 1\n",
    "\n",
    "                if step_counter % batch_size == 0:\n",
    "                    # Update policy with the accumulated data once the batch size is reached\n",
    "                    next_value = self.critic(torch.from_numpy(state).float()).item()\n",
    "                    returns = self.compute_gae(rewards, masks, values, next_value)\n",
    "                    advantages = [ret - val for ret, val in zip(returns, values)]\n",
    "                    trajectory = (states, actions, log_probs, returns, advantages)\n",
    "                    self.update(trajectory)\n",
    "\n",
    "                    # Clear the accumulated data for the next batch\n",
    "                    states, actions, rewards, log_probs, values, masks = [], [], [], [], [], []\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            all_rewards.append(episode_rewards)\n",
    "            print(f\"Episode {episode + 1}, Total Reward = {episode_rewards}, Total Steps = {step_counter}\")\n",
    "\n",
    "            # Check termination condition (if needed)\n",
    "            if step_counter >= 1000000:\n",
    "                return all_rewards\n",
    "\n",
    "        return all_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chat 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "from collections import deque\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "\n",
    "class AgentNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(AgentNet, self).__init__()\n",
    "        self.affine = nn.Linear(num_inputs, 128)\n",
    "        self.action_head = nn.Linear(128, num_outputs)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.affine(x))\n",
    "        action_probs = torch.softmax(self.action_head(x), dim=-1)\n",
    "        state_values = self.value_head(x)\n",
    "        return action_probs, state_values\n",
    "\n",
    "class PPOAgent:\n",
    "    def __init__(self, env, config, policy_params):\n",
    "        self.env = env\n",
    "        self.config = config\n",
    "        self.model = AgentNet(env.observation_space.shape[0], env.action_space.n)\n",
    "        self.model.load_state_dict(policy_params)\n",
    "\n",
    "    def step(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        probs, state_value = self.model(state)\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        log_prob = m.log_prob(action)\n",
    "        next_state, reward, done, _ = self.env.step(action.item())\n",
    "        return action.item(), log_prob, state_value, next_state, reward, done\n",
    "\n",
    "    def update_policy(self, states, actions, old_log_probs, rewards, values, optimizer):\n",
    "        actions = torch.tensor(actions)\n",
    "        old_log_probs = torch.stack(old_log_probs)\n",
    "        rewards = torch.tensor(rewards)\n",
    "        values = torch.cat(values)\n",
    "        masks = torch.tensor([1.0] * len(rewards))\n",
    "\n",
    "        # Adding last value for advantage calculation\n",
    "        _, last_value = self.model(torch.from_numpy(states[-1]).float().unsqueeze(0))\n",
    "        values = torch.cat([values, last_value.detach()])\n",
    "\n",
    "        advantages = self.compute_advantages(rewards, masks, values)\n",
    "\n",
    "        # Convert advantages to tensor and standardize\n",
    "        advantages = torch.tensor(advantages)\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "        # Convert rewards to returns\n",
    "        returns = advantages + values[:-1]\n",
    "\n",
    "        # Optimization loop\n",
    "        for _ in range(self.config['epochs']):\n",
    "            idx = torch.randperm(len(states))\n",
    "            for batch_indices in idx.split(self.config['batch_size']):\n",
    "                sampled_states = torch.tensor(states)[batch_indices]\n",
    "                sampled_actions = actions[batch_indices]\n",
    "                sampled_old_log_probs = old_log_probs[batch_indices]\n",
    "                sampled_advantages = advantages[batch_indices]\n",
    "\n",
    "                # Forward pass\n",
    "                new_probs, new_values = self.model(sampled_states)\n",
    "                new_dist = Categorical(new_probs)\n",
    "                new_log_probs = new_dist.log_prob(sampled_actions)\n",
    "\n",
    "                # Calculating the ratio (pi_theta / pi_theta_old):\n",
    "                ratio = torch.exp(new_log_probs - sampled_old_log_probs)\n",
    "\n",
    "                # Clipped surrogate loss\n",
    "                surr1 = ratio * sampled_advantages\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.epsilon, 1.0 + self.epsilon) * sampled_advantages\n",
    "                loss = -torch.min(surr1, surr2).mean()  # Focus only on the clipping part\n",
    "\n",
    "                # take gradient step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 77\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m complete.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 77\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[38], line 58\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Aggregate data from all workers\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(return_dict)\n\u001b[1;32m---> 58\u001b[0m aggregated_data \u001b[38;5;241m=\u001b[39m {k: [] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[43mreturn_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()}\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_workers\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m aggregated_data\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32m<string>:2\u001b[0m, in \u001b[0;36m__getitem__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\multiprocessing\\managers.py:837\u001b[0m, in \u001b[0;36mBaseProxy._callmethod\u001b[1;34m(self, methodname, args, kwds)\u001b[0m\n\u001b[0;32m    835\u001b[0m     dispatch(conn, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdecref\u001b[39m\u001b[38;5;124m'\u001b[39m, (token\u001b[38;5;241m.\u001b[39mid,))\n\u001b[0;32m    836\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m proxy\n\u001b[1;32m--> 837\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m convert_to_error(kind, result)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "def worker(worker_id, policy_params, config, return_dict):\n",
    "    \"\"\"Worker process to collect data from the environment.\"\"\"\n",
    "    np.random.seed(worker_id)\n",
    "    torch.manual_seed(worker_id)\n",
    "    env = gym.make(config['env_name'])\n",
    "    agent = PPOAgent(env, config, policy_params)\n",
    "\n",
    "    state = env.reset()\n",
    "    rewards, log_probs, states, actions, values = [], [], [], [], []\n",
    "    for _ in range(config['horizon']):\n",
    "        action, log_prob, value, next_state, reward, done = agent.step(state)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(reward)\n",
    "        state = next_state if not done else env.reset()\n",
    "\n",
    "    return_dict[worker_id] = {\n",
    "        'states': states,\n",
    "        'actions': actions,\n",
    "        'log_probs': log_probs,\n",
    "        'values': values,\n",
    "        'rewards': rewards\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    config = {\n",
    "        'env_name': 'InvertedPendulum-v4',\n",
    "        'horizon': 2048,\n",
    "        'learning_rate': 3e-4,\n",
    "        'batch_size': 64,\n",
    "        'epochs': 10,\n",
    "        'gamma': 0.99,\n",
    "        'gae_lambda': 0.95,\n",
    "        'num_workers': 4\n",
    "    }\n",
    "\n",
    "    env = gym.make(config['env_name'])\n",
    "    model = AgentNet(env.observation_space.shape[0], env.action_space.shape[0])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
    "\n",
    "    manager = mp.Manager()\n",
    "    return_dict = manager.dict()\n",
    "\n",
    "    for iteration in range(10):  # run for 10 iterations\n",
    "        processes = []\n",
    "        for i in range(config['num_workers']):\n",
    "            p = mp.Process(target=worker, args=(i, model.state_dict(), config, return_dict))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "        \n",
    "        for p in processes:\n",
    "            p.join()\n",
    "\n",
    "        # Aggregate data from all workers\n",
    "        print(return_dict)\n",
    "        aggregated_data = {k: [] for k in return_dict[0].keys()}\n",
    "        for i in range(config['num_workers']):\n",
    "            for key in aggregated_data.keys():\n",
    "                aggregated_data[key].extend(return_dict[i][key])\n",
    "        \n",
    "        # Convert lists to tensors and perform PPO update\n",
    "        states = torch.FloatTensor(aggregated_data['states'])\n",
    "        actions = torch.LongTensor(aggregated_data['actions'])\n",
    "        old_log_probs = torch.stack(aggregated_data['log_probs'])\n",
    "        rewards = torch.FloatTensor(aggregated_data['rewards'])\n",
    "        values = torch.stack(aggregated_data['values'])\n",
    "\n",
    "        # Example PPO update, assuming `update_policy` is implemented\n",
    "        agent = PPOAgent(env, config, model.state_dict())\n",
    "        agent.update_policy(states, actions, old_log_probs, rewards, values, optimizer)\n",
    "\n",
    "        print(f'Iteration {iteration + 1} complete.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
