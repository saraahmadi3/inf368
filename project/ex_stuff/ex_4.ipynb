{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL?\n",
    "from chat:\n",
    "\n",
    "Certainly! The Kullback-Leibler (KL) divergence is a measure used in information theory and statistics to quantify how one probability distribution, $q(x)$, diverges from another, $p(x)$.\n",
    "\n",
    "The formula you provided, $\\text{KL}(p; q) = \\sum p(x) \\log \\left(\\frac{p(x)}{q(x)}\\right)$, calculates this divergence by comparing the probability of each outcome $x$ under the distributions $p$ and $q$, weighted by the probability of $x$ under $p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) explain why KL(p,q) is asymmetric in p,q?\n",
    "\n",
    "from chat:\n",
    "\n",
    "Regarding the asymmetry of KL divergence in $p$ and $q$: this comes from the fact that the KL divergence measures how much information is lost when $q$ is used to approximate $p$, and this loss of information is not the same when we switch the roles of $p$ and $q$. In other words, $\\text{KL}(p; q)$ is generally not equal to $\\text{KL}(q; p)$.\n",
    "\n",
    "Let me illustrate this with an example:\n",
    "\n",
    "Suppose $p$ is the true distribution of some phenomenon, and $q$ is our approximation of $p$. If $\\text{KL}(p; q)$ is large, it means that using $q$ to approximate $p$ results in a significant loss of information; $q$ diverges substantially from $p$. However, if $\\text{KL}(q; p)$ is small, it means that using $p$ to approximate $q$ results in little loss of information; $p$ is a good approximation of $q$.\n",
    "\n",
    "This difference in loss of information between $p$ and $q$ is why KL divergence is asymmetric.\n",
    "\n",
    "In summary, KL divergence quantifies the dissimilarity between two distributions by measuring how much information is lost when one distribution is used to approximate the other, and it's asymmetric because the loss of information is not the same when we switch the roles of the distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note to self* just find a video to understand it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dom_1 = [0,1,2,3,4]\n",
    "dom_2 = [0.0,0.2,0.4,0.6,0.8]\n",
    "\n",
    "p_1 = [0.2,0.2,0.2,0.2,0.2]\n",
    "p_2 = [0.1,0.2,0.4,0.2,0.1]\n",
    "p_3 = [0.1,0.2,0.4,0.2,0.1]\n",
    "p_4 = [0.1,0.2,0.2,0.4,0.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(p, q):\n",
    "    return sum(p[i] * math.log(p[i]/q[i]) for i in range(len(p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) calc KL(p_1,p_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(p_1, p_1) = 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'KL(p_1, p_1) = {KL(p_1, p_1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does it make sense????????\n",
    "\n",
    "TODO: answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) calc KL(p_1,p_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(p_1, p_2) = 0.13862943611198905\n"
     ]
    }
   ],
   "source": [
    "print(f'KL(p_1, p_2) = {KL(p_1, p_2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does it make sense????????\n",
    "\n",
    "TODO: answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) calc KL(p_1,p_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL(p_1, p_3) = 0.13862943611198905\n"
     ]
    }
   ],
   "source": [
    "print(f'KL(p_1, p_3) = {KL(p_1, p_3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does it make sense????????\n",
    "\n",
    "TODO: answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from task:\n",
    "\n",
    "*If we are learning a distribution $q$ and we are trying to approximate or approach another distribution $p$, we can use the KL divergence to evaluate how close we are.*\n",
    "\n",
    "eg --> aim is to minimize KL value!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf368",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
