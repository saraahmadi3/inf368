{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import cookiedisaster\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)\n",
    "\n",
    "SEED=2\n",
    "np.random.seed(SEED)\n",
    "\n",
    "env = gym.make('cookiedisaster-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\spaces\\box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1000: Training Started\n",
      "Episode 0, Total Reward: -0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saraa\\AppData\\Local\\Temp\\ipykernel_21804\\3030047161.py:133: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action = torch.tensor(action)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100, Total Reward: 0\n",
      "Episode 200, Total Reward: 0\n",
      "Episode 300, Total Reward: -2.5\n",
      "Episode 400, Total Reward: -2.5\n",
      "Episode 500, Total Reward: -2.5\n",
      "Episode 600, Total Reward: -2.5\n",
      "Episode 700, Total Reward: -1.0\n",
      "Episode 800, Total Reward: -2.5\n",
      "Episode 900, Total Reward: -0.5\n",
      "Average Reward over 10 episodes: 0.0\n",
      "Iteration 1: Average Reward = 0.0\n",
      "New best model found!\n",
      "Desired threshold reward of -2.5 achieved.\n",
      "Best average reward achieved: 0.0\n"
     ]
    }
   ],
   "source": [
    "env.reset(seed=SEED) \n",
    "# env.render()\n",
    "DUMMY_STATE=env.step(0)\n",
    "MAX_TIME=5*25 # 5 cookies elapse time\n",
    "PUNISHMENT=-1 # may have to be adjusted\n",
    "\n",
    "def normalize(value, min_value, max_value, scale_min=-1, scale_max=1):\n",
    "    return ((value - min_value) / (max_value - min_value)) * (scale_max - scale_min) + scale_min\n",
    "\n",
    "def preprocess_state(state):\n",
    "    # Assuming state is a dictionary like:\n",
    "    # {'robot': {'pos': x, 'vel': y}, 'cookie': {'pos': z, 'time': w}}\n",
    "    robot_pos = normalize(state[0]['agent']['pos'], 0, 10)\n",
    "    robot_vel = normalize(state[0]['agent']['vel'], -7, 7)\n",
    "    cookie_pos = normalize(state[0]['cookie']['pos'], 0, 10)\n",
    "    cookie_time = normalize(state[0]['cookie']['time'], 0, 5)\n",
    "    distance = robot_pos - cookie_pos\n",
    "    direction = 1 if distance > 0 else -1\n",
    "    \n",
    "    # Return the normalized state as a numpy array\n",
    "    return np.array([robot_pos, robot_vel, cookie_pos, cookie_time,distance, direction])\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "\n",
    "input_dim = 6  # From preprocess_state function\n",
    "output_dim = env.action_space.n\n",
    "\n",
    "actor = Actor(input_dim, output_dim)\n",
    "critic = Critic(input_dim)\n",
    "\n",
    "\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=0.001)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=0.001)\n",
    "\n",
    "num_episodes = 1000\n",
    "gamma = 0.99  # Discount factor for future rewards\n",
    "\n",
    "episode_rewards = []  # Track rewards per episode\n",
    "action_counts = np.zeros(output_dim)\n",
    "def train_actor_critic(env, num_episodes=10, gamma=0.99, actor_lr=0.001, critic_lr=0.001):\n",
    "    # Environment and model parameters\n",
    "    input_dim = 6  # From preprocess_state function\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    # Initialize models\n",
    "    actor = Actor(input_dim, output_dim)\n",
    "    critic = Critic(input_dim)\n",
    "\n",
    "    # Initialize optimizers\n",
    "    actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "    critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    # Training loop\n",
    "    \n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        count = 0\n",
    "        epsilon_start = 1.0\n",
    "        epsilon_min = 0.01\n",
    "        epsilon_decay = 0.995\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = actor(state_tensor)\n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            epsilon = max(epsilon_min, epsilon_start * epsilon_decay**episode)\n",
    "            if np.random.rand() < epsilon:\n",
    "                action = env.action_space.sample()  # Random action\n",
    "            else:\n",
    "                action = distribution.sample()\n",
    "            \n",
    "            action_counts[action] += 1\n",
    "            next_state= env.step(action.item())\n",
    "            reward = next_state[1]\n",
    "            done = next_state[2]\n",
    "            \n",
    "            if next_state[1]>0:\n",
    "                break\n",
    "            # missed 5 cookies (MAX_TIME), minus points and break\n",
    "            if count>MAX_TIME:\n",
    "                # env._cumulative_reward+=PUNISHMENT\n",
    "                # total_reward+=PUNISHMENT\n",
    "                break\n",
    "            count+=1\n",
    "            # if count>100:\n",
    "            #     print('count',count)\n",
    "            #     break\n",
    "            \n",
    "            next_state = preprocess_state(next_state)\n",
    "            \n",
    "            total_reward += reward\n",
    "            \n",
    "            # Critic update\n",
    "            value = critic(state_tensor)\n",
    "            next_value = critic(torch.FloatTensor(next_state).unsqueeze(0))\n",
    "            td_error = reward + gamma * next_value * (1 - int(done)) - value\n",
    "            critic_loss = td_error.pow(2)\n",
    "            \n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            # Actor update\n",
    "            action = torch.tensor(action)\n",
    "            actor_loss = -distribution.log_prob(action) * td_error.detach()\n",
    "            \n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        episode_rewards.append(total_reward)\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "    \n",
    "    return actor, critic, episode_rewards, action_counts, episode_rewards\n",
    "\n",
    "def evaluate_policy(env, actor, num_episodes=100):\n",
    "    total_rewards = 0.0\n",
    "    rewards = []    \n",
    "    for _ in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = preprocess_state(state)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        count = 0\n",
    "\n",
    "        while not done:\n",
    "            if count > MAX_TIME:  # Break if max time exceeded without positive reward\n",
    "                break\n",
    "            \n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action_probs = actor(state_tensor)\n",
    "            distribution = torch.distributions.Categorical(action_probs)\n",
    "            action = distribution.sample()\n",
    "            \n",
    "            next_state= env.step(action.item())\n",
    "            reward = next_state[1]\n",
    "            rewards.append(reward)\n",
    "            done = next_state[2]\n",
    "            \n",
    "            if next_state[1]>0:\n",
    "                break\n",
    "            \n",
    "            if count>MAX_TIME:\n",
    "                \n",
    "                break\n",
    "            count+=1\n",
    "          \n",
    "            \n",
    "            next_state = preprocess_state(next_state)\n",
    "            \n",
    "            \n",
    "            total_reward += reward\n",
    "\n",
    "       \n",
    "\n",
    "    average_reward = total_rewards / num_episodes\n",
    "    print(f\"Average Reward over {num_episodes} episodes: {average_reward}\")\n",
    "    return average_reward, rewards\n",
    "\n",
    "def find_optimal_policy(env, initial_actor_lr=0.001, initial_critic_lr=0.001, gamma=0.99, threshold_reward=None, max_iterations=10, eval_episodes=10):\n",
    "    actor_lr = initial_actor_lr\n",
    "    critic_lr = initial_critic_lr\n",
    "    \n",
    "    best_average_reward = float('-inf')\n",
    "    best_actor = None\n",
    "\n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"Iteration {iteration + 1}/{max_iterations}: Training Started\")\n",
    "        actor, critic, episode_rewards, action_counts, episode_rewards= train_actor_critic(env, num_episodes=num_episodes, gamma=gamma, actor_lr=actor_lr, critic_lr=critic_lr)\n",
    "        average_reward, rewards = evaluate_policy(env, actor, num_episodes=eval_episodes)\n",
    "        print(f\"Iteration {iteration + 1}: Average Reward = {average_reward}\")\n",
    "        \n",
    "        # Check if the current model is the best one\n",
    "        if average_reward > best_average_reward:\n",
    "            best_average_reward = average_reward\n",
    "            best_actor = actor\n",
    "            print(\"New best model found!\")\n",
    "            \n",
    "            # Optional: Adjust learning rates based on performance, implement your strategy here\n",
    "            # actor_lr *= learning_rate_decay\n",
    "            # critic_lr *= learning_rate_decay\n",
    "            \n",
    "        # If a threshold is defined and met, stop training\n",
    "        if threshold_reward is not None and average_reward >= threshold_reward:\n",
    "            print(f\"Desired threshold reward of {threshold_reward} achieved.\")\n",
    "            break\n",
    "        \n",
    "        # Optional: Implement additional logic to adjust training parameters or terminate early\n",
    "    \n",
    "    return best_actor, best_average_reward, rewards\n",
    "\n",
    "best_actor, best_reward, rewards = find_optimal_policy(env, initial_actor_lr=0.001, initial_critic_lr=0.001, threshold_reward=-2.5, max_iterations=num_episodes)\n",
    "print(f\"Best average reward achieved: {best_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_actor_critic(env, actor, episodes=num_episodes):\n",
    "    total_rewards = []\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        state = preprocess_state(state)  # Adjust based on your state preprocessing\n",
    "        done = False\n",
    "      \n",
    "        count = 0\n",
    "        \n",
    "        \n",
    "      \n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = actor(state_tensor)\n",
    "        action = torch.argmax(q_values).item()\n",
    "        next_state= env.step(action)\n",
    "        reward = next_state[1]\n",
    "        done = next_state[2]    \n",
    "        # next_state = preprocess_state(next_state)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "    \n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average reward over {episodes} episodes: {avg_reward}\")\n",
    "    return avg_reward, total_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\envs\\registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment cookiedisaster-v1 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\spaces\\box.py:240: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:131: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "c:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcookiedisaster-v1\u001b[39m\u001b[38;5;124m'\u001b[39m,render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m avg_act, act_rewards\u001b[38;5;241m=\u001b[39m\u001b[43mtest_actor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_actor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m, in \u001b[0;36mtest_actor_critic\u001b[1;34m(env, actor, episodes)\u001b[0m\n\u001b[0;32m     15\u001b[0m q_values \u001b[38;5;241m=\u001b[39m actor(state_tensor)\n\u001b[0;32m     16\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(q_values)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 17\u001b[0m next_state\u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m reward \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     19\u001b[0m done \u001b[38;5;241m=\u001b[39m next_state[\u001b[38;5;241m2\u001b[39m]    \n",
      "File \u001b[1;32mc:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saraa\\anaconda3\\envs\\inf368\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saraa\\OneDrive - University of Bergen\\24V\\INF368\\inf368\\project\\cookiedisaster\\envs\\cookiedisaster.py:211\u001b[0m, in \u001b[0;36mCookieDisasterEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    208\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_info()\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_render_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cummlative_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mreward\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_time\n",
      "File \u001b[1;32mc:\\Users\\saraa\\OneDrive - University of Bergen\\24V\\INF368\\inf368\\project\\cookiedisaster\\envs\\cookiedisaster.py:306\u001b[0m, in \u001b[0;36mCookieDisasterEnv._render_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow\u001b[38;5;241m.\u001b[39mblit(canvas, (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m    305\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mflip()\n\u001b[1;32m--> 306\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrender_fps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mtranspose(np\u001b[38;5;241m.\u001b[39marray(pygame\u001b[38;5;241m.\u001b[39msurfarray\u001b[38;5;241m.\u001b[39mpixels3d(canvas)), axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('cookiedisaster-v1',render_mode='human')\n",
    "avg_act, act_rewards=test_actor_critic(env, best_actor, episodes=500)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf368",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
