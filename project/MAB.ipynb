{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bandit as b\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from BaseAgent import AbstractAgent\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandits: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three medicine where the effectiveness varies randomly over time. \n",
    "\n",
    "non-stationary MAB scenario? bc the reward distributions are changing over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandit_final_env =b.Bandits_final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent implements a variant of the Upper Confidence Bound (UCB) algorithm, a popular method in multi-armed bandit problems. It uses a sliding window mechanism to adapt to potentially non-stationary environments (where the properties of the environment can change over time).\n",
    "\n",
    "1. Action Selection with UCB: This part calculates the Upper Confidence Bound for each action (or \"arm\") based on recent rewards. It balances exploration (trying new actions) and exploitation (using actions known to yield high rewards). Actions with fewer trials or high uncertainty get a \"bonus\" to their estimated value, encouraging exploration.\n",
    "2. Sliding Window: This mechanism ensures that the agent's decisions are based on recent experiences by maintaining a fixed-size list of the most recent rewards for each action. This helps adapt to changes in the environment's dynamics.\n",
    "3. Learning from Interaction: After performing an action and observing a reward, the agent updates its knowledge (the sliding windows of rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 4130.602790310084\n"
     ]
    }
   ],
   "source": [
    "class SlidingWindowUCB:\n",
    "    def __init__(self, k, window_size=50, c=1):\n",
    "        self.k = k\n",
    "        self.window_size = window_size\n",
    "        self.c = c\n",
    "        self.windows = {i: [] for i in range(k)}\n",
    "        self.t = 0\n",
    "    \n",
    "    def select_action(self):\n",
    "        self.t += 1\n",
    "        ucb_values = []\n",
    "        for arm in range(self.k):\n",
    "            rewards = self.windows[arm]\n",
    "            if len(rewards) < 1:\n",
    "                ucb_values.append(float('inf'))\n",
    "            else:\n",
    "                mean_reward = np.mean(rewards)\n",
    "                bonus = self.c * np.sqrt(np.log(self.t) / len(rewards))\n",
    "                ucb_values.append(mean_reward + bonus)\n",
    "        return np.argmax(ucb_values)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        rewards = self.windows[action]\n",
    "        if len(rewards) >= self.window_size:\n",
    "            rewards.pop(0)\n",
    "        rewards.append(reward)\n",
    "\n",
    "\n",
    "agent = SlidingWindowUCB(k=3, window_size=50, c=1)\n",
    "\n",
    "total_reward = 0\n",
    "for episode in range(1000):\n",
    "    obs, reward, terminated, truncated, info = bandit_final_env.reset()\n",
    "    while not terminated:\n",
    "        action = agent.select_action()\n",
    "        _, reward, terminated, _, _ = bandit_final_env.step(action)\n",
    "        agent.update(action, reward)\n",
    "        total_reward += reward\n",
    "\n",
    "print(\"Total Reward:\", total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decay epsilon greedy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regret(mab, cumulative_reward, t, print_=False):\n",
    "    if mab.state is not None:\n",
    "        optimal_reward = mab.means[mab.state][mab.get_optimal_action()] * t\n",
    "    else:\n",
    "        optimal_reward = mab.means[mab.get_optimal_action()] * t\n",
    "    #if print_:\n",
    "       # print(\"optimal reward at time step\", t, \":\", optimal_reward)\n",
    "        #print(\"cumulative reward at time step\", t, \":\", cumulative_reward)\n",
    "    return optimal_reward - cumulative_reward\n",
    "\n",
    " \n",
    "def update_expected_reward(expected_reward,action_count,current_action,reward):\n",
    "    expected_reward[current_action]+=(1/action_count[current_action]) * (reward-expected_reward[current_action])\n",
    "\n",
    "def get_confidence_bounds(t,action_count):\n",
    "    return np.sqrt(2*np.log(t)/action_count)\n",
    "## here i am calculating the \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decaying_epsilon_greedy(mab,T,epsilon, alpha,print_=False):\n",
    "    mab.reset()\n",
    "\n",
    "    # initialise the expected reward and action count and cumulative reward\n",
    "    expected_reward = np.ones(mab.k)*100\n",
    "    action_count = np.zeros(mab.k)\n",
    "    cumulative_rewards = 0\n",
    "    \n",
    "    for _ in range(T):\n",
    "        if np.random.uniform(0,1) < epsilon:\n",
    "            current_action = np.random.randint(mab.k)\n",
    "        else:\n",
    "            current_action = np.argmax(expected_reward)\n",
    "            \n",
    "        current_state = mab.step(current_action)        # maybe not state...\n",
    "        cumulative_rewards += current_state[1]\n",
    "        action_count[current_action] += 1\n",
    "        update_expected_reward(expected_reward,action_count,current_action,current_state[1])\n",
    "        epsilon*=alpha\n",
    "    \n",
    "    if print_:\n",
    "        for i in range(mab.k):\n",
    "            print(\"Expected reward action\", i, \":\", expected_reward[i], \"Action count:\", action_count[i])\n",
    "\n",
    "    regret = calculate_regret(mab, cumulative_rewards, T,print_)   \n",
    "\n",
    "    if print_:\n",
    "        print('-------------------------------------\\n\\n')\n",
    "\n",
    "    return np.argmax(expected_reward), regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, -167.20138791448608)\n"
     ]
    }
   ],
   "source": [
    "print(decaying_epsilon_greedy(bandit_final_env,1000,0.5,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(mab,T,epsilon,print_=False):\n",
    "    mab.reset()\n",
    "\n",
    "    # initialise the expected reward and action count and cumulative reward\n",
    "    expected_reward = np.ones(mab.k)*1000\n",
    "    action_count = np.zeros(mab.k)\n",
    "    cumulative_rewards = 0\n",
    "    regret2 = np.zeros(T)\n",
    "    \n",
    "    for t in range(T):\n",
    "        if np.random.uniform(0,1) < epsilon:\n",
    "            current_action = np.random.randint(mab.k)\n",
    "        else:\n",
    "            current_action = np.argmax(expected_reward)\n",
    "            \n",
    "        current_state = mab.step(current_action)        # maybe not state...\n",
    "        cumulative_rewards += current_state[1]\n",
    "        action_count[current_action] += 1\n",
    "        update_expected_reward(expected_reward,action_count,current_action,current_state[1])\n",
    "        regret2[t] = calculate_regret(mab, cumulative_rewards, t,print_)\n",
    "    if print_:\n",
    "        for i in range(mab.k):\n",
    "            print(\"Expected reward action\", i, \":\", expected_reward[i], \"Action count:\", action_count[i])\n",
    "            \n",
    "        \n",
    "    regret = calculate_regret(mab, cumulative_rewards, T,print_)   \n",
    "\n",
    "    if print_:\n",
    "        print(\"regert\", regret2)\n",
    "        print('-------------------------------------\\n\\n')\n",
    "\n",
    "    return np.argmax(expected_reward), regret2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'b1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-------------------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39margmax(expected_reward), regret\n\u001b[1;32m---> 36\u001b[0m ucb(\u001b[43mb1\u001b[49m,\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m0.5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'b1' is not defined"
     ]
    }
   ],
   "source": [
    "def ucb(mab,T,c,print_=False):\n",
    "    mab.reset()\n",
    "\n",
    "    # initialise the expected reward and action count and cumulative reward\n",
    "    expected_reward = np.zeros(mab.k)\n",
    "    action_count = np.zeros(mab.k)\n",
    "    cumulative_rewards =0\n",
    "\n",
    "    # initialisation, pull each arm once and use the rewards to initialise the expected reward and action count\n",
    "    # OBS! not a smart thing to do in real hospital setting, with trying drugs on patients ...\n",
    "    # (needed to avoid dividing by zero in the first iteration)\n",
    "    for i in range(mab.k):\n",
    "        expected_reward[i] = mab.step(i)[1]\n",
    "        action_count[i] += 1\n",
    "    \n",
    "    for t in range(mab.k+1,T+1):\n",
    "        confidence_bounds = get_confidence_bounds(t,action_count)\n",
    "        best_action = np.argmax(expected_reward + c*confidence_bounds)\n",
    "\n",
    "        current_state = mab.step(best_action)        # maybe not state...\n",
    "        cumulative_rewards += current_state[1]\n",
    "        action_count[best_action] += 1\n",
    "        update_expected_reward(expected_reward,action_count,best_action,current_state[1])\n",
    "    \n",
    "    if print_:\n",
    "        for i in range(mab.k):\n",
    "            print(\"Expected reward action\", i, \":\", expected_reward[i], \"Action count:\", action_count[i])\n",
    "        \n",
    "    regret = calculate_regret(mab, cumulative_rewards, T,print_)   \n",
    "\n",
    "    if print_:\n",
    "        print('-------------------------------------\\n\\n')\n",
    "\n",
    "    return np.argmax(expected_reward), regret\n",
    "\n",
    "ucb(bandit_final_env,1000,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(mab, T, epsilon, alpha, c, print_=False):\n",
    "    eps_greed=epsilon_greedy(mab,T,epsilon,print_)\n",
    "    \n",
    "    dec_greed=decaying_epsilon_greedy(mab,T,epsilon,alpha,print_)\n",
    "    ucb_res=ucb(mab,T,c,print_)\n",
    "    if print_:\n",
    "        print(f'epsilon greedy:          medicine: {eps_greed[0]}  regret: {eps_greed[1]}')\n",
    "        print(f'decaying epsiol greedy:  medicine: {dec_greed[0]}  regret: {dec_greed[1]}')\n",
    "        print(f'ucb:                     medicine: {ucb_res[0]}  regret: {ucb_res[1]}')\n",
    "        print('-------------------------------------\\n')\n",
    "    return eps_greed,dec_greed,ucb_res\n",
    "\n",
    "run_experiments(bandit_final_env,1000,0.5,0.99,0.5,True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
