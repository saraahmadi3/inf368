{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import skyscraper\n",
    "import numpy as np\n",
    "from collections import defaultdict \n",
    "\n",
    "import random\n",
    "import time\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "env = gymnasium.make('skyscraper/GridWorld-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a new directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Define the states and actions with rewards\n",
    "states = [\"home\", \"uni\", \"end\"]\n",
    "actions = {\n",
    "    \"home\": [(\"uni\", \"travel\", 0)],\n",
    "    \"uni\": [(\"end\", \"take exam\", \"r=1 or 0\")]\n",
    "}\n",
    "\n",
    "# Add nodes and edges with labels for each state and action\n",
    "for state in states:\n",
    "    G.add_node(state)\n",
    "\n",
    "for source_state, action_list in actions.items():\n",
    "    for target_state, action, reward in action_list:\n",
    "        G.add_edge(source_state, target_state, label=f\"{action}, {reward}\")\n",
    "\n",
    "# Position nodes using the spring layout\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Draw the graph\n",
    "nx.draw(G, pos, with_labels=True, node_size=2000, node_color=\"lightblue\", font_size=10, font_weight=\"bold\")\n",
    "\n",
    "# Draw edge labels\n",
    "edge_labels = nx.get_edge_attributes(G, 'label')\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# powered_flight_data = open('powered_flight.txt', 'r')\n",
    "# powered_flight_data = powered_flight_data.read().split('\\n')\n",
    "# len(powered_flight_data)\n",
    "# powered_flight_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First need to fix index problem, right know they are 1-indexed need to become 0-indexed bc env is 0-indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fix_index(powered_flight_data):\n",
    "#     fixed_data = []\n",
    "#     for line in powered_flight_data:\n",
    "#         data = line.strip().split('\\t')\n",
    "#         print(data)\n",
    "#         if not data or len(data) != 6:\n",
    "#              continue\n",
    "        \n",
    "#         try:\n",
    "#             print(\"here\")\n",
    "#             adjusted_data = [int(value) - 1 if value else None for value in data]\n",
    "#             # print(\"adjusted1: \",adjusted_data)\n",
    "#             adjusted_data = [value for value in adjusted_data if value is not None]\n",
    "#             # print(\"adjusted2: \",adjusted_data)\n",
    "#             fixed_data.append(adjusted_data)\n",
    "#         except ValueError as e:\n",
    "#             print(f\"Error processing line: {line} - {e}\")\n",
    "#     return fixed_data\n",
    "# fixed_powered_flight_data = fix_index(powered_flight_data)\n",
    "# fixed_powered_flight_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(fixed_powered_flight_data))\n",
    "# print(len(powered_flight_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a={1:1,2:3,4:5}\n",
    "\n",
    "3 in a.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('powered_flight.txt', dtype=int)\n",
    "data[:,:3] -= 1\n",
    "data[:,4:6] -= 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transtition function, reward function and model M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_function = {}  # Transition function: (state, action) -> next state\n",
    "\n",
    "for i, j, a, r, i_prime, j_prime in data:\n",
    "    \n",
    "    current_state = (i, j)\n",
    "    next_state = (i_prime, j_prime)\n",
    "    action = a\n",
    "    \n",
    "    transition_function[(current_state, action)] = next_state\n",
    "transition_function_original = transition_function.copy()\n",
    "transition_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_function = {}  # Reward function: (state, action, next state) -> reward\n",
    "\n",
    "for i, j, a, r, i_prime, j_prime in data:\n",
    "    current_state = (i, j)\n",
    "    next_state = (i_prime, j_prime)\n",
    "    action = a\n",
    "    \n",
    "    reward_function[(current_state, action)] = r\n",
    "    \n",
    "reward_function_original = reward_function.copy()\n",
    "reward_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_M(current_state, action, transition_function, reward_function):\n",
    "   \n",
    "    next_state = transition_function.get((current_state, action), None)  # Get the next state from T\n",
    "    # print(\"next_state: \",next_state)\n",
    "    reward = reward_function.get((current_state, action), 0)  # Get the reward from R\n",
    "    # print(\"reward: \",reward)\n",
    "    \n",
    "    return next_state, reward\n",
    "print(model_M((12, 17), 0, transition_function, reward_function))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dyna arcitecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = [0, 1]  \n",
    "alpha = 0.5\n",
    "gamma = 0.99\n",
    "epsilon = 0.3\n",
    "n_planning_steps = 1000\n",
    "# punishment=0.1\n",
    "T = 5000  # Total number of real interactions\n",
    "state = env.reset()[0]['agent']['pos']\n",
    "state = tuple(state)\n",
    "\n",
    "\n",
    "# Initialize Q-values\n",
    "# Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "Q=np.zeros((env.height,env.width,env.action_space.n))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose action based on ε-greedy policy\n",
    "def choose_action(state, Q, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        # print(\"random\")\n",
    "        return env.action_space.sample()  \n",
    "    else:\n",
    "        \n",
    "        return np.argmax(Q[state])  \n",
    "\n",
    "# Direct learning from real experience\n",
    "# def q_learning_update(Q, state, action, reward, next_state, alpha, gamma):\n",
    "    \n",
    "#     best_next_action = np.argmax(Q[next_state])\n",
    "    \n",
    "#     td_target = reward + gamma * Q[next_state][best_next_action]\n",
    "#     td_error = td_target - Q[state][action]\n",
    "#     Q[state][action] += alpha * td_error\n",
    "    \n",
    "# def q_learning(Q, env, alpha, gamma, epsilon, num_episodes):\n",
    "#     for episode in range(num_episodes):\n",
    "#         state = env.reset()[0]['agent']['pos']\n",
    "#         state = tuple(state)\n",
    "#         done = False\n",
    "        \n",
    "#         while not done:\n",
    "#             action = choose_action(state, Q, epsilon)\n",
    "#             next_state, reward, done, _, _ = env.step(action)\n",
    "#             next_state = tuple(map(int, next_state['agent']['pos']))\n",
    "            \n",
    "#             q_learning_update(Q, state, action, reward, next_state, alpha, gamma)\n",
    "            \n",
    "#             state = next_state\n",
    "    \n",
    "#     return Q\n",
    "\n",
    "def q_learning(state, action, reward, next_state, value, discound,lr):\n",
    "\n",
    "    # value of previous state-action pair\n",
    "    i,j = state\n",
    "    prev_value = value[i,j,action]\n",
    "\n",
    "    # maximum Q-value at current state\n",
    "    if next_state is None or np.isnan(next_state).any():\n",
    "        max_value = 0\n",
    "    else:\n",
    "        n_i,n_j = next_state\n",
    "        max_value = np.max(value[n_i,n_j, :])\n",
    "\n",
    "    # reward prediction error\n",
    "    delta = reward + discound * max_value - prev_value\n",
    "\n",
    "    # update value of previous state-action pair\n",
    "    value[i,j,action] = prev_value + lr * delta\n",
    "\n",
    "    return value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_optimal_policy(Q):\n",
    "   return np.argmax(Q,axis=2)\n",
    "        # print(f\"Optimal action for state {state} is {optimal_action}\")\n",
    "\n",
    "print_optimal_policy(Q)\n",
    "\n",
    "\n",
    "# Main loop for Dyna-Q\n",
    "\n",
    "for k in range(T):\n",
    "    # epsilon = epsilon * 0.99\n",
    "    # print(k)\n",
    "    if k % 100 == 0:\n",
    "        print(f\"Real interaction {k}/{T}\")\n",
    "    \n",
    "    state =env.reset()[0]['agent']['pos']\n",
    "    state = tuple(state)\n",
    "    start_state=state\n",
    "    # print(\"start state\", start_state)\n",
    "    # print(\"state: \",state)  \n",
    "    done = False\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        \n",
    "        action = choose_action(state, Q, epsilon)\n",
    "        \n",
    "        next_state, reward, done, err, info = env.step(action) \n",
    "        \n",
    "        next_state = tuple(map(int,next_state['agent']['pos']))\n",
    "        #print(\"next state\", next_state)\n",
    "        \n",
    "        q_learning(state, action, reward, next_state, Q, gamma, alpha)\n",
    "        transition_function[(state, action)] = next_state\n",
    "        reward_function[(state, action)] = reward\n",
    "        \n",
    "        crashed  = (start_state==next_state and state != start_state)\n",
    "        \n",
    "        state = next_state\n",
    "        #print(\"state\", state)\n",
    "        \n",
    "        if done or crashed:\n",
    "            done = True\n",
    "        # print(\"state\", state)\n",
    "    \n",
    "    \n",
    "    \n",
    "# print('after')\n",
    "    for m in range(n_planning_steps):\n",
    "        # print(\"m: \",m)\n",
    "        sampled_state, sampled_action = random.choice(list(transition_function.keys()))\n",
    "        #sampled_action = random.choice(action_space)\n",
    "        if (sampled_state, sampled_action) in transition_function:\n",
    "            # simulated_next_state, simulated_reward = model_M(sampled_state, sampled_action, transition_function, reward_function)\n",
    "            simulated_next_state = transition_function[(sampled_state, sampled_action)]\n",
    "            simulated_reward = reward_function[(sampled_state, sampled_action)]\n",
    "            q_learning(sampled_state, sampled_action, simulated_reward, simulated_next_state,Q, gamma,alpha)\n",
    "            # print('hi')\n",
    "\n",
    "# env.close()\n",
    "policy=print_optimal_policy(Q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_function==reward_function_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming other parts of your code remain unchanged\n",
    "def run_with_policy(env, policy_matrix, num_steps=1000):\n",
    "    observation, _ = env.reset()\n",
    "    # Convert observation to initial position tuple\n",
    "    current_pos = tuple(map(int, observation[\"agent\"][\"pos\"]))\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Steps: {step} to {step + 99} of {num_steps}\")\n",
    "\n",
    "        # env.render()\n",
    "\n",
    "        # Directly use current_pos to get the best action from the policy matrix\n",
    "        action = policy_matrix[current_pos]\n",
    "        print(\"action: \",action)\n",
    "        #print(f\"Action: {action}\")\n",
    "\n",
    "        # Take the action in the environment\n",
    "        observation, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        # Update current position based on the observation\n",
    "        current_pos = tuple(map(int, observation[\"agent\"][\"pos\"]))\n",
    "        \n",
    "        print(f\"Current Position: {current_pos}\")\n",
    "\n",
    "        if done:\n",
    "            #print(\"Reached the goal or crashed, resetting.\")\n",
    "            observation, _ = env.reset()\n",
    "            current_pos = tuple(map(int, observation[\"agent\"][\"pos\"]))\n",
    "\n",
    "    # env.close()\n",
    "\n",
    "#env = gymnasium.make('skyscraper/GridWorld-v0',render_mode=\"human\")\n",
    "#print(\"Run with policy\")\n",
    "#run_with_policy(env, policy)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "env.reset()\n",
    "env_map = env.MAP  # Make sure this is the correct way to access the map\n",
    "\n",
    "# Visualize the environment\n",
    "plt.imshow(env_map, cmap='copper', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def follow_policy(env, policy):\n",
    "    total_reward = 0\n",
    "    trajectory = []\n",
    "    done = False\n",
    "    state= env.reset()[0]['agent']['pos']\n",
    "    state = tuple([int(state[0]),int(state[1])])\n",
    "    trajectory.append(state)\n",
    "    \n",
    "    while not done:\n",
    "        print(\"state: \",state)\n",
    "        action = policy[state[0], state[1]]  # Adjust based on your state definition\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = next_state['agent']['pos']  # Corrected line\n",
    "        total_reward += reward\n",
    "        state = tuple([int(next_state[0]),int(next_state[1])])\n",
    "        trajectory.append(state)\n",
    "        \n",
    "        if done:\n",
    "            break  # Exit the loop if the episode is done\n",
    "    \n",
    "    return total_reward, trajectory\n",
    "\n",
    "# Convert Q-values to policy\n",
    "# policy = np.argmax(Q, axis=2)  # Make sure this aligns with how your Q-table is structured\n",
    "\n",
    "# # Follow the policy\n",
    "# control_total_reward, control_trajectory = follow_policy(env, policy)\n",
    "# print(\"Total Reward:\", control_total_reward)\n",
    "# print(\"Trajectory:\", control_trajectory)\n",
    "\n",
    "# plt.imshow(env_map, cmap='copper', interpolation='nearest')  # Use 'viridis' or another colormap as needed\n",
    "# plt.plot([x[1] for x in control_trajectory], [x[0] for x in control_trajectory], color='purple', markersize=5, linewidth=2)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy1 =runq(env,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward, trajectory = follow_policy(env, policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(env_map, cmap='viridis', interpolation='nearest')  # Use 'viridis' or another colormap as needed\n",
    "plt.colorbar()\n",
    "plt.plot([x[1] for x in trajectory], [x[0] for x in trajectory], color='black', markersize=5, linewidth=2)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Other parts of your original code omitted for brevity\n",
    "# # ...action_space = [0, 1]  \n",
    "# # alpha = 0.5\n",
    "# # gamma = 0.99\n",
    "# # epsilon = 0.3\n",
    "# # #n_planning_steps = 1000\n",
    "# # # punishment=0.1\n",
    "# # T = 1000  # Total number of real interactions\n",
    "# # state = env.reset()[0]['agent']['pos']\n",
    "# # state = tuple(state)\n",
    "\n",
    "\n",
    "# # Initialize Q-values\n",
    "# # Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "# Q=np.zeros((env.height,env.width,env.action_space.n))\n",
    "\n",
    "\n",
    "# def print_optimal_policy(Q):\n",
    "#    return np.argmax(Q,axis=2)\n",
    "#         # print(f\"Optimal action for state {state} is {optimal_action}\")\n",
    "\n",
    "# print_optimal_policy(Q)\n",
    "\n",
    "# def run_dyna_Q(env,n_planning_steps):\n",
    "#     env.reset()\n",
    "    \n",
    "#     Q = np.zeros((env.height, env.width, env.action_space.n))\n",
    "#     # Initialize your Q-table each time to start learning from scratch\n",
    "\n",
    "#     # Copy the Dyna-Q learning loop here\n",
    "#     for k in range(T):\n",
    "#     # epsilon = epsilon * 0.99\n",
    "#         print(k)\n",
    "#         if k % 100 == 0:\n",
    "#             print(f\"Real interaction {k}/{T}\")\n",
    "        \n",
    "#         state =env.reset()[0]['agent']['pos']\n",
    "#         state = tuple(state)\n",
    "#         start_state=state\n",
    "    \n",
    "#         done = False\n",
    "        \n",
    "#         done = False\n",
    "#         while not done:\n",
    "            \n",
    "            \n",
    "#             action = choose_action(state, Q, epsilon)\n",
    "            \n",
    "#             next_state, reward, done, err, info = env.step(action) \n",
    "            \n",
    "#             next_state = tuple(map(int,next_state['agent']['pos']))\n",
    "#             #print(\"next state\", next_state)\n",
    "            \n",
    "#             q_learning(state, action, reward, next_state, Q, gamma, alpha)\n",
    "#             transition_function[(state, action)] = next_state\n",
    "#             reward_function[(state, action)] = reward\n",
    "            \n",
    "#             crashed  = (start_state==next_state and state != start_state)\n",
    "            \n",
    "#             state = next_state\n",
    "#             #print(\"state\", state)\n",
    "            \n",
    "#             if done or crashed:\n",
    "#                 done = True\n",
    "#         # print(\"state\", state)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # print('after')\n",
    "#         for m in range(n_planning_steps):\n",
    "#             print(\"m: \",m)\n",
    "#             sampled_state, sampled_action = random.choice(list(transition_function.keys()))\n",
    "#             #sampled_action = random.choice(action_space)\n",
    "#             if (sampled_state, sampled_action) in transition_function:\n",
    "#                 # simulated_next_state, simulated_reward = model_M(sampled_state, sampled_action, transition_function, reward_function)\n",
    "#                 simulated_next_state = transition_function[(sampled_state, sampled_action)]\n",
    "#                 simulated_reward = reward_function[(sampled_state, sampled_action)]\n",
    "#                 q_learning(sampled_state, sampled_action, simulated_reward, simulated_next_state,Q, gamma,alpha)\n",
    "#                 # print('hi')\n",
    "                    \n",
    "\n",
    "        \n",
    "#         # After learning, evaluate the policy\n",
    "#     policy = print_optimal_policy(Q)\n",
    "    \n",
    "#     return policy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy1 = run_dyna_Q(env,0)\n",
    "# policy2 = run_dyna_Q(env,5)\n",
    "# policy3 = run_dyna_Q(env,10)\n",
    "# policy4 = run_dyna_Q(env,20)\n",
    "# policy5 = run_dyna_Q(env,50)\n",
    "# policy6 = run_dyna_Q(env,100)\n",
    "# policy7 = run_dyna_Q(env,200)\n",
    "# policy8 = run_dyna_Q(env,500)\n",
    "# policy9 = run_dyna_Q(env,1000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policies = [policy1, policy2, policy3, policy4, policy5, policy6, policy7, policy8, policy9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dyna_Q(env,n_planning_steps,transition_function,reward_function):\n",
    "    # Main loop for Dyna-Q\n",
    "\n",
    "    for k in range(T):\n",
    "        # epsilon = epsilon * 0.99\n",
    "        # print(k)\n",
    "        if k % 100 == 0:\n",
    "            print(f\"Real interaction {k}/{T}\")\n",
    "        \n",
    "        state =env.reset()[0]['agent']['pos']\n",
    "        state = tuple(state)\n",
    "        start_state=state\n",
    "        # print(\"start state\", start_state)\n",
    "        # print(\"state: \",state)  \n",
    "        done = False\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            \n",
    "            \n",
    "            action = choose_action(state, Q, epsilon)\n",
    "            \n",
    "            next_state, reward, done, err, info = env.step(action) \n",
    "            \n",
    "            next_state = tuple(map(int,next_state['agent']['pos']))\n",
    "            #print(\"next state\", next_state)\n",
    "            \n",
    "            q_learning(state, action, reward, next_state, Q, gamma, alpha)\n",
    "            transition_function[(state, action)] = next_state\n",
    "            reward_function[(state, action)] = reward\n",
    "            \n",
    "            crashed  = (start_state==next_state and state != start_state)\n",
    "            \n",
    "            state = next_state\n",
    "            #print(\"state\", state)\n",
    "            \n",
    "            if done or crashed:\n",
    "                done = True\n",
    "            # print(\"state\", state)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # print('after')\n",
    "        for m in range(n_planning_steps):\n",
    "            # print(\"m: \",m)\n",
    "            sampled_state, sampled_action = random.choice(list(transition_function.keys()))\n",
    "            #sampled_action = random.choice(action_space)\n",
    "            if (sampled_state, sampled_action) in transition_function:\n",
    "                # simulated_next_state, simulated_reward = model_M(sampled_state, sampled_action, transition_function, reward_function)\n",
    "                simulated_next_state = transition_function[(sampled_state, sampled_action)]\n",
    "                simulated_reward = reward_function[(sampled_state, sampled_action)]\n",
    "                q_learning(sampled_state, sampled_action, simulated_reward, simulated_next_state,Q, gamma,alpha)\n",
    "                # print('hi')\n",
    "\n",
    "    # env.close()\n",
    "    return np.argmax(Q,axis=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj = []\n",
    "rewards=[]\n",
    "n_num =  [5, 10, 20, 50, 100]\n",
    "for n in n_num:\n",
    "    transition_function = transition_function_original.copy()\n",
    "    reward_function = reward_function_original.copy()\n",
    "    print(f\"Running Dyna-Q with {n} planning steps\")\n",
    "    policy = run_dyna_Q(env,n,transition_function,reward_function)\n",
    "    print(f\"Following policy with {n} planning steps\")\n",
    "    reward, trajectory = follow_policy(env, policy)\n",
    "    print(f\"Total Reward: {reward}\")\n",
    "    print(f\"Trajectory: {trajectory}\")\n",
    "    traj.append(trajectory)\n",
    "    rewards.append(reward)\n",
    "    print('-' * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axs = plt.subplots(len(traj), 1, figsize=(10, 10))\n",
    "\n",
    "# Plot each SARSA n trajectory with total reward\n",
    "for i, (trajectory, reward) in enumerate(zip(traj, rewards)):\n",
    "    axs[i].imshow(env_map, cmap='viridis', interpolation='nearest')  # Use 'viridis' or another colormap as needed\n",
    "    axs[i].plot([x[1] for x in trajectory], [x[0] for x in trajectory], color='black', markersize=5, linewidth=2)\n",
    "    axs[i].set_title(f\"Trajectory of Planning step: {n_num[i]}\", fontsize=15)\n",
    "    axs[i].set_xlabel(\"Columns\", fontsize=12)\n",
    "    axs[i].set_ylabel(\"Rows\", fontsize=12)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Function to follow a given policy and return total reward and trajectory\n",
    "# def follow_policy(env, policy):\n",
    "#     total_reward = 0\n",
    "#     trajectory = []\n",
    "#     done = False\n",
    "#     state, info = env.reset()\n",
    "#     state = tuple(state['agent']['pos'])\n",
    "#     trajectory.append(state)\n",
    "    \n",
    "#     while not done:\n",
    "#         action = policy[state[0], state[1]]  # Adjust based on your state definition\n",
    "#         next_state, reward, done, _, _ = env.step(action)\n",
    "#         next_state = tuple(map(int, next_state['agent']['pos']))\n",
    "#         total_reward += reward\n",
    "#         state = next_state\n",
    "#         trajectory.append(state)\n",
    "        \n",
    "#         if done:\n",
    "#             break  # Exit the loop if the episode is done\n",
    "    \n",
    "#     return total_reward, trajectory\n",
    "\n",
    "# # Convert Q-values to policy\n",
    "# policy = np.argmax(Q, axis=2)  # Make sure this aligns with how your Q-table is structured\n",
    "\n",
    "\n",
    "# policies = [policy1, policy2, policy3, policy4, policy5, policy6, policy7, policy8, policy9]\n",
    "\n",
    "# # Correcting the axis in np.argmax\n",
    "# for index, Q in enumerate(policies, start=1):\n",
    "#     policy = np.argmax(Q, axis=1)  # Adjust the axis to 1 for a 2D array\n",
    "#     total_reward, trajectory = follow_policy(env, policy)\n",
    "#     print(f\"Policy {index} Total Reward:\", total_reward)\n",
    "#     print(f\"Policy {index} Trajectory:\", trajectory)\n",
    "    \n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.imshow(env_map, cmap='copper', interpolation='nearest')\n",
    "#     plt.plot([x[1] for x in trajectory], [x[0] for x in trajectory], marker='o', color='purple', markersize=5, linewidth=2)\n",
    "#     plt.title(f\"Trajectory for Policy {index}\")\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def follow_policy(env, policy):\n",
    "#     total_reward = 0\n",
    "#     trajectory = []\n",
    "#     done = False\n",
    "#     state, info = env.reset()\n",
    "#     state = tuple(state['agent']['pos'])\n",
    "#     trajectory.append(state)\n",
    "    \n",
    "#     while not done:\n",
    "#         action = policy[state[0], state[1]]  # Adjust based on your state definition\n",
    "#         next_state, reward, done, _, _ = env.step(action)\n",
    "#         next_state = tuple(map(int, next_state['agent']['pos']))\n",
    "#         total_reward += reward\n",
    "#         state = next_state\n",
    "#         trajectory.append(state)\n",
    "        \n",
    "#         if done:\n",
    "#             break  # Exit the loop if the episode is done\n",
    "    \n",
    "#     return total_reward, trajectory\n",
    "\n",
    "# # Convert Q-values to policy\n",
    "# policy = np.argmax(Q, axis=2)  # Make sure this aligns with how your Q-table is structured\n",
    "\n",
    "# # Follow the policy\n",
    "# control_total_reward, control_trajectory = follow_policy(env, policy9)\n",
    "# print(\"Total Reward:\", control_total_reward)\n",
    "# print(\"Trajectory:\", control_trajectory)\n",
    "\n",
    "# plt.imshow(env_map, cmap='copper', interpolation='nearest')  # Use 'viridis' or another colormap as needed\n",
    "# plt.plot([x[1] for x in control_trajectory], [x[0] for x in control_trajectory], color='purple', markersize=5, linewidth=2)\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assume follow_policy is defined as before\n",
    "# # Assume policies is a list of policy arrays as shown above\n",
    "\n",
    "# # Container for trajectories\n",
    "# trajectories = []\n",
    "\n",
    "# # Follow each policy and store the trajectory\n",
    "# for policy in policies:\n",
    "#     _, trajectory = follow_policy(env, policy)\n",
    "#     trajectories.append(trajectory)\n",
    "\n",
    "# # Plot the environment\n",
    "# plt.imshow(env_map, cmap='copper', interpolation='nearest')  # Assuming env_map is your environment map\n",
    "\n",
    "# # Plot each trajectory\n",
    "# colors = plt.cm.viridis(np.linspace(0, 1, len(trajectories)))  # Generate distinct colors\n",
    "# for traj, color in zip(trajectories, colors):\n",
    "#     plt.plot([x[1] for x in traj], [x[0] for x in traj], markersize=5, linewidth=2, color=color)\n",
    "\n",
    "# # Customizing the plot\n",
    "# plt.title('Trajectories of Different Policies')\n",
    "# plt.xlabel('X-axis')\n",
    "# plt.ylabel('Y-axis')\n",
    "# plt.legend(['0', '5', '10', '20', '50', '100', '200', '500', '1000'], title=\"Planning Steps\", loc='upper left')\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming you have already computed the optimal policies for each planning step\n",
    "# # policies = [policy1, policy2, ..., policy9]\n",
    "\n",
    "# # Create a figure for plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot trajectories for each policy\n",
    "# for i, policy in enumerate(policies):\n",
    "    \n",
    "#     total_reward, trajectory = follow_policy(env, policy)\n",
    "#     print(f\"Total Reward for Policy {i + 1}: {total_reward}\")\n",
    "#     plt.plot([x[1] for x in trajectory], [x[0] for x in trajectory], label=f\"Policy {i + 1}\")\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.title(\"Trajectories for Different Policies (Dyna-Q)\")\n",
    "# plt.xlabel(\"X-coordinate\")\n",
    "# plt.ylabel(\"Y-coordinate\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policies = [policy1, policy2]\n",
    "\n",
    "# # Assuming follow_policy is correctly defined elsewhere and works as expected\n",
    "# def evaluate_policies(policies):\n",
    "#     results = {}\n",
    "#     for index, policy in enumerate(policies):\n",
    "#         print(f\"Evaluating policy {index + 1} of {len(policies)}\")\n",
    "#         total_reward, trajectory = follow_policy(env, policy)\n",
    "        \n",
    "#         # Store the results\n",
    "#         results[index] = {\n",
    "#             'total_reward': total_reward,\n",
    "#             'trajectory': trajectory\n",
    "#             # You should only store the Q-table if it's still in scope and relevant\n",
    "#         }\n",
    "#     return results\n",
    "\n",
    "# # Now call the function and pass the list of policies\n",
    "# evaluation_results = evaluate_policies(policies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- power flight.txt in the format (i, j, a, r, i′.j′), where\n",
    "(i, j) is the current state, a the action, r the reward, and (i′, j′) the next state. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Then compare the results\n",
    "# for n_planning_steps, data in results.items():\n",
    "#     print(f\"Planning Steps: {n_planning_steps}, Total Reward: {data['total_reward']}, Trajectory Length: {len(data['trajectory'])}\")\n",
    "#     # Add any other comparisons or visualizations here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Assuming you have 'results' containing optimal policies and total rewards\n",
    "# # results = {n_planning_steps: {'total_reward': ..., 'trajectory': ...}, ...}\n",
    "\n",
    "# # Create a figure for plotting\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# # Plot trajectories for each policy\n",
    "# for n_planning_steps, data in results.items():\n",
    "#     trajectory = results['trajectory']\n",
    "#     plt.plot(trajectory[:, 0], trajectory[:, 1], label=f\"Steps={n_planning_steps}\")\n",
    "\n",
    "# # Customize the plot\n",
    "# plt.title(\"Trajectories for Different Planning Steps (Dyna-Q)\")\n",
    "# plt.xlabel(\"X-coordinate\")\n",
    "# plt.ylabel(\"Y-coordinate\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import gymnasium\n",
    "# import numpy as np\n",
    "# import random\n",
    "# import matplotlib.pyplot as plt\n",
    "# import skyscraper\n",
    "# # Load data and adjust indexing to match Python's 0-based indexing\n",
    "# data = np.loadtxt('powered_flight.txt', dtype=int)\n",
    "# data[:,:3] -= 1\n",
    "# data[:,4:6] -= 1\n",
    "# action_space = [0, 1]  \n",
    "# ls = 0.4  #originally 0.5\n",
    "# discount = 0.99\n",
    "# epsilon = 0.3\n",
    "# # Initialize environment\n",
    "# env = gymnasium.make('skyscraper/GridWorld-v0')\n",
    "\n",
    "# # Define the transition and reward functions based on the loaded data\n",
    "# transition_function = {}\n",
    "# reward_function = {}\n",
    "# for i, j, a, r, i_prime, j_prime in data:\n",
    "#     current_state, next_state = (i, j), (i_prime, j_prime)\n",
    "#     transition_function[(current_state, a)] = next_state\n",
    "#     reward_function[(current_state, a, next_state)] = r\n",
    "\n",
    "# # Define the model function for the environment\n",
    "# def model_M(current_state, action):\n",
    "#     next_state = transition_function.get((current_state, action), current_state)  # Default to current state if not found\n",
    "#     reward = reward_function.get((current_state, action, next_state), 0)  # Default reward is 0\n",
    "#     return next_state, reward\n",
    "\n",
    "# # Dyna-Q algorithm function\n",
    "\n",
    "# def run_dyna_q(env, n_planning_steps, alpha=0.1, gamma=0.99, epsilon=0.1, episodes=100):\n",
    "#     Q = np.zeros((env.height, env.width, 2))  # Initialize Q-values\n",
    "#     for episode in range(episodes):\n",
    "#         # Reset the environment for a new episode\n",
    "#         state = tuple(env.reset()[0]['agent']['pos'])\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             action = choose_action(state, Q, epsilon)  # Choose an action based on the current policy\n",
    "#             next_state, reward, done, _, _ = env.step(action)  # Take the action in the environment\n",
    "#             next_state = tuple(map(int,next_state['agent']['pos']))  # Get the next state\n",
    "#             # Update Q-values using the Q-learning algorithm\n",
    "#             Q = q_learning(state, action, reward, next_state, Q, gamma, alpha)\n",
    "#             state = next_state  # Move to the next state\n",
    "\n",
    "#             # Planning: randomly sample previous experiences to update Q-values\n",
    "#             for _ in range(n_planning_steps):\n",
    "#                 sampled_state, sampled_action = random.choice(list(transition_function.keys()))\n",
    "#                 simulated_next_state, simulated_reward = model_M(sampled_state, sampled_action)\n",
    "#                 Q = q_learning(sampled_state, sampled_action, simulated_reward, simulated_next_state, Q, gamma, alpha)\n",
    "#     return np.argmax(Q, axis=2)  # Return the derived policy\n",
    "\n",
    "# # Implementation for q_learning and choose_action remains the same\n",
    "\n",
    "# # Define Q-learning function\n",
    "# def q_learning(state, action, reward, next_state, value, discount, lr):\n",
    "#     i,j = state\n",
    "#     prev_value = value[i,j,action]\n",
    "\n",
    "#     if next_state is None or np.isnan(next_state).any():\n",
    "#         max_value = 0\n",
    "#     else:\n",
    "#         n_i,n_j = next_state\n",
    "#         max_value = np.max(value[n_i,n_j, :])\n",
    "\n",
    "#     delta = reward + discount * max_value - prev_value\n",
    "\n",
    "#     value[i,j,action] = prev_value + lr * delta\n",
    "\n",
    "#     return value\n",
    "\n",
    "# # Define action choice function\n",
    "# def choose_action(state, Q, epsilon):\n",
    "#     # Implementation of ε-greedy policy\n",
    "#     # (This needs to be provided based on your Dyna-Q setup)\n",
    "#     if random.random() < epsilon:\n",
    "        \n",
    "#         return env.action_space.sample()  \n",
    "#     else:\n",
    "        \n",
    "#         return np.argmax(Q[state]) \n",
    "    \n",
    "# # Run Dyna-Q for different planning steps and collect policies\n",
    "# planning_steps_list = [0, 5, 10, 20, 50, 100]\n",
    "# policies = {}\n",
    "# for n_planning_steps in planning_steps_list:\n",
    "#     policies[n_planning_steps] = run_dyna_q(env, n_planning_steps)\n",
    "\n",
    "# # Evaluate and compare the policies\n",
    "# # (Evaluation function needs to be defined based on your environment and requirements)\n",
    "\n",
    "# # Example of evaluating and plotting could go here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
